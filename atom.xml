<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yuthon&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.yuthon.com/"/>
  <updated>2016-11-01T14:17:18.000Z</updated>
  <id>http://www.yuthon.com/</id>
  
  <author>
    <name>Yusu Pan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Solution for matplotlib importing error on macOS</title>
    <link href="http://www.yuthon.com/2016/11/01/Solution-for-matplotlib-importing-error-on-Mac-OS-X/"/>
    <id>http://www.yuthon.com/2016/11/01/Solution-for-matplotlib-importing-error-on-Mac-OS-X/</id>
    <published>2016-11-01T13:57:58.000Z</published>
    <updated>2016-11-01T14:17:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天在做 CS231n 的 Assignment #2 的时候遇到了导入 matplotlib.pyplot 的问题, 特此记录.</p>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>打开 IPython Notebook 之后, 执行以下命令:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div></pre></td></tr></table></figure>
<p>出现错误:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ValueError: unknown locale: UTF<span class="number">-8</span></div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><p>把下面这些加到<code>~/.zshrc</code>或者是<code>~/.bash_profile</code>里面:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export LC_ALL=en_US.UTF-8</div><div class="line">export LANG=en_US.UTF-8</div></pre></td></tr></table></figure>
<p>同时, 如果用 iTerm 的话, 还需要在 Preference -&gt; Profiles -&gt; Terminal -&gt; Environment 中, 取消选择 <code>Set locale variables automatically</code>.</p>
<p><strong>然而</strong>又出现了下列报错:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ImportError: cannot <span class="keyword">import</span> name _thread</div></pre></td></tr></table></figure>
<p>这个问题已经在最新的<code>six</code>和<code>dateutil</code>库中解决了, 然而 macOS 本身却还在使用旧版本的库. 解决方法如下:</p>
<ul>
<li><p>执行以下命令安装最新版本的<code>six</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo pip install six -U</div></pre></td></tr></table></figure>
</li>
<li><p>开 python 看看是否还在使用旧版本的库:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> six</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>six.__file__</div><div class="line">/System/Library/Frameworks/Python.framework/Versions/<span class="number">2.7</span>/Extras/lib/python/six.pyc</div></pre></td></tr></table></figure>
<p>显然确实是这样</p>
</li>
<li><p>删除旧版本的库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ rm -rf /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six.*</div></pre></td></tr></table></figure>
<p>这样就可以了, 之后 python 会使用我们之前新装的版本的<code>six</code>库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> six</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>six.__file__</div><div class="line"><span class="string">'/Library/Python/2.7/site-packages/six.pyc'</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p>之后再执行<code>import matplotlib.pyplot as plt</code>之后就没问题了.</p>
<p>说到底还是 macOS 的锅…</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天在做 CS231n 的 Assignment #2 的时候遇到了导入 matplotlib.pyplot 的问题, 特此记录.&lt;/p&gt;
&lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;打开 IPython Notebook 之后, 执行以下命令:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;出现错误:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;ValueError: unknown locale: UTF&lt;span class=&quot;number&quot;&gt;-8&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Python" scheme="http://www.yuthon.com/tags/Python/"/>
    
      <category term="Matplotlib" scheme="http://www.yuthon.com/tags/Matplotlib/"/>
    
      <category term="macOS" scheme="http://www.yuthon.com/tags/macOS/"/>
    
  </entry>
  
  <entry>
    <title>Notes for SLIC Superpixels Compared to State-of-the-Art Superpixel Methods</title>
    <link href="http://www.yuthon.com/2016/11/01/Notes-for-SLIC-Superpixels-Compared-to-State-of-the-Art-Superpixel-Methods/"/>
    <id>http://www.yuthon.com/2016/11/01/Notes-for-SLIC-Superpixels-Compared-to-State-of-the-Art-Superpixel-Methods/</id>
    <published>2016-11-01T10:21:04.000Z</published>
    <updated>2016-11-01T12:09:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章介绍了当前 State-of-the-Art 的5种<strong>超像素 (Superpixel)</strong> 的算法, 并主要从其对于图像边缘信息的拟合程度 (their ability to adhere to image boundaries), 速度, 内存利用效率, 以及它们对于图像分割性能的影响 (their impact on segmentation performance) 来综合评价.</p>
<p>同时, 本文还提出了一种 <strong>SLIC (simple linear iterative clustering)</strong> 的算法, 用的是 k-means clustering 的方法.</p>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><strong>超像素算法</strong>主要是将一幅图像中, 具有相似颜色, 纹理或者亮度等信息的相邻的像素点聚集起来, 组成一些具有一定视觉意义的像素块, 为后续处理做准备. 这种算法用少量的超像素来代替原来的图像像素, 能够减少图像的冗余度, 为计算图像特征做了很好的铺垫, 并且显著地降低了随后的图像处理步骤的复杂度. 超像素算法作为一个<strong>预处理</strong>的步骤, 现在已经成为了很多计算机视觉算法中的重要一环, 比如说图像分割 (image segmentation), 深度估计(depth estimation), 姿态预估 (body model estimation) 和目标定位 (object localization) 这些领域.</p>
<p><img src="/images/Images_segmented_using_SLIC_into_superpixels.png" alt="Images_segmented_using_SLIC_into_superpixels"></p>
<p>生成超像素的方法有很多, 各自有各自的优点和缺陷. 在此主要考虑以下方面来评测对比这些算法:</p>
<ul>
<li>超像素必须<u>遵循图像的边界</u> (Superpixels should adhere well to image boundaries.)</li>
<li>如果超像素是作为一个预处理步骤, 来减少计算复杂度的, 那么就需要考虑<u>计算速度, 内存效率以及是否方便使用</u>等因素 (When used to reduce computational complexity as a pre- processing step, superpixels should be fast to compute, memory efficient, and simple to use.)</li>
<li>同时, 如果超像素算法是为了用来给之后的分割做准备的, 那么就要考虑它<u>是否既能加快速度, 又能提升分割的效果</u>. (When used for segmentation purposes, superpixels should both increase the speed and improve the quality of the results.)</li>
</ul>
<h2 id="Existing-Superpixel-Methods"><a href="#Existing-Superpixel-Methods" class="headerlink" title="Existing Superpixel Methods"></a>Existing Superpixel Methods</h2><h3 id="Graph-Based-Algorithms"><a href="#Graph-Based-Algorithms" class="headerlink" title="Graph-Based Algorithms"></a>Graph-Based Algorithms</h3><p>这类算法主要就是把整个图像看成一张图, 各个像素是节点, 相邻像素之间的相似度作为边上的权值. 超像素就是根据最小化损失函数来构建的.</p>
<p>相关的方法有:</p>
<ul>
<li><strong><a href="https://www.cs.sfu.ca/research/groups/VML/pubs/mori-model_search_segmentation-iccv05.pdf" target="_blank" rel="external">NC05</a></strong>: 对边缘的遵循不好. 时间复杂度$O(N^{\frac{1}{2}})$<ul>
<li>The Normalized cuts algorithm recursively partitions a graph of all pixels in the image using contour and texture cues, globally minimizing a cost function defined on the edges at the partition boundaries.</li>
</ul>
</li>
<li><strong><a href="http://link.springer.com/article/10.1023/B:VISI.0000022288.19776.77" target="_blank" rel="external">GS04</a></strong>: 对边缘遵循较好, 但是生成的超像素大小与形状不规则, 不能严格控制超像素的个数. 时间复杂度$O(NlogN)$<ul>
<li>It performs an agglomerative clustering of pixels as nodes on a graph such that each superpixel is the minimum spanning tree of the constituent pixels.</li>
</ul>
</li>
<li><strong><a href="http://ieeexplore.ieee.org/document/4587471/?arnumber=4587471&amp;tag=1" target="_blank" rel="external">SL08</a></strong>: 时间复杂度$O(N^{\frac{3}{2}}logN)$, 但是没有算上预先生成边缘图(boundary map)所耗的时间<ul>
<li>Moore et al. propose a method to generate superpixels that conform to a grid by finding optimal paths, or seams, that split the image into smaller vertical or horizontal regions. Optimal paths are found using a graph cuts method similar to Seam Carving.</li>
</ul>
</li>
<li><strong><a href="http://link.springer.com/chapter/10.1007/978-3-642-15555-0_16" target="_blank" rel="external">GCa10 &amp; GCb10</a></strong>: Veksler et al. use a global optimization approach similar to the texture synthesis work. Superpixels are obtained by stitching together overlapping image patches such that each pixel belongs to only one of the overlapping regions. They suggest two variants of their method, one for generating compact superpixels (GCa10) and one for constant- intensity superpixels (GCb10).</li>
</ul>
<h3 id="Gradient-Ascent-Based-Algorithms"><a href="#Gradient-Ascent-Based-Algorithms" class="headerlink" title="Gradient-Ascent-Based Algorithms"></a>Gradient-Ascent-Based Algorithms</h3><p>这类方法生对像素生成一个随机的初始聚类, 然后不断迭代优化, 直到满足收敛条件.</p>
<p>相关的方法有:</p>
<ul>
<li><strong><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1000236" target="_blank" rel="external">MS02</a></strong>: 一种比较老的方法, 生成的超像素形状不规整, 而且对于超像素的数量大小等均不能控制. 同时, 时间复杂度是$O(N^2)$, 非常慢.<ul>
<li>Mean shift, an iterative mode-seeking procedure for locating local maxima of a density function, is applied to find modes in the color or intensity feature space of an image. Pixels that converge to the same mode define the superpixels. MS02</li>
</ul>
</li>
<li><strong><a href="http://link.springer.com/chapter/10.1007/978-3-540-88693-8_52" target="_blank" rel="external">QS08</a></strong>: 对边界的遵循比较好, 但是速度相当慢. 时间复杂度$O(dN^2)$.<ul>
<li>Quick shift also uses a mode-seeking segmentation scheme. It initializes the segmentation using a medoid shift procedure. It then moves each point in the feature space to the nearest neighbor that increases the Parzen density estimate.</li>
</ul>
</li>
<li><strong><a href="https://pdfs.semanticscholar.org/a381/9dda9a5f00dbb8cd3413ca7422e37a0d5794.pdf" target="_blank" rel="external">WS91</a></strong>: 对边界遵循不好, 形状不规整, 不能严格控制超像素的数量, 但是速度快. 时间复杂度$O(NlogN)$.<ul>
<li>The watershed approach performs a gradient ascent starting from local minima to produce watersheds, lines that separate catchment basins. The </li>
</ul>
</li>
<li><strong><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4912213" target="_blank" rel="external">TP09</a></strong>: 生成的超像素具有一致的大小, 紧凑性. 宣称时间复杂度$O(N)$, 但是实际上很慢, 而且对边界的遵循不好. <ul>
<li>The Turbopixel method progressively dilates a set of seed locations using level-set-based geometric flow. The geometric flow relies on local image gradients, aiming to regularly distribute superpixels on the image plane.</li>
</ul>
</li>
</ul>
<h2 id="SLIC-Superpixels"><a href="#SLIC-Superpixels" class="headerlink" title="SLIC Superpixels"></a>SLIC Superpixels</h2><p>(To be continued…)</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文章介绍了当前 State-of-the-Art 的5种&lt;strong&gt;超像素 (Superpixel)&lt;/strong&gt; 的算法, 并主要从其对于图像边缘信息的拟合程度 (their ability to adhere to image boundaries), 速度, 内存利用效率, 以及它们对于图像分割性能的影响 (their impact on segmentation performance) 来综合评价.&lt;/p&gt;
&lt;p&gt;同时, 本文还提出了一种 &lt;strong&gt;SLIC (simple linear iterative clustering)&lt;/strong&gt; 的算法, 用的是 k-means clustering 的方法.&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Superpixel" scheme="http://www.yuthon.com/tags/Superpixel/"/>
    
  </entry>
  
  <entry>
    <title>Notes for CS231n Recurrent Neural Network</title>
    <link href="http://www.yuthon.com/2016/10/30/Notes-for-CS231n-RNN/"/>
    <id>http://www.yuthon.com/2016/10/30/Notes-for-CS231n-RNN/</id>
    <published>2016-10-30T06:59:17.000Z</published>
    <updated>2016-10-31T11:42:10.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>从 RNN 开始, CS231n 的 Lecture Notes 就没有了, 因此我根据上课时的 Slides 整理了一些需要重视的知识点. 还可以参考这些文章或是视频来加深理解:</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/22107715?refer=intelligentunit" target="_blank" rel="external">[原创翻译]循环神经网络惊人的有效性（上）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22230074?refer=intelligentunit" target="_blank" rel="external">[原创翻译]循环神经网络惊人的有效性（下）</a></li>
<li><a href="https://www.youtube.com/watch?v=Ukgii7Yd_cU" target="_blank" rel="external">Recurrent Neural Networks (Video, recommend)</a></li>
<li><a href="http://james371507.wixsite.com/hylee/single-post/2016/03/20/Recurrent-Neural-Network-RNN-Note-of-Stanford-CS231n" target="_blank" rel="external">Recurrent Neural Network (RNN) (Note of Stanford CS231n)</a></li>
</ul>
</blockquote>
<a id="more"></a>
<h1 id="Lecture-10"><a href="#Lecture-10" class="headerlink" title="Lecture 10"></a>Lecture 10</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Recurrent Networks offer a lot of flexibility:</p>
<p> <img src="/images/RNN_flexibility.png" alt="RNN_flexibility"></p>
<ol>
<li><strong>one to one</strong>: Vanilla Neural Networks</li>
<li><strong>one to many</strong>: e.g. Image Captioning (image -&gt; sequence of words)</li>
<li><strong>many to one</strong>: e.g. Sentiment Classification (sequence of words -&gt; sentiment)</li>
<li><strong>many to many</strong>: <ul>
<li>e.g. Machine Translation (seq of words -&gt; seq of words)</li>
<li>e.g. Video classification on frame level</li>
</ul>
</li>
</ol>
<p>RNN can also do sequential precessing of fix inputs (Multiple Object Recognition with Visual Attention, Ba et al.) or fixed outputs (DRAW: A Recurrent Neural Network For Image Generation, Gregor et al.).</p>
<h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><h3 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h3><p>Usually we want to predict a vector at some time steps. To achieve this goal, we can process a sequence of vectors $x$ by applying a recurrence formula at every time step:</p>
<p> <img src="/images/RNN_concept.png" alt="RNN_concept"></p>
<blockquote>
<p>Notice: the same function and the same set of parameters are used at every time step. That’s to say, we use <strong>shared weights</strong>.</p>
</blockquote>
<p><strong>(Vanilla) Recurrent Neural Network</strong></p>
<p>The state consists of a single “hidden” vector $h$:</p>
<ul>
<li>$h_t = tanh (W_{hh} h_{t-1} + W_{xh} x_t)$</li>
<li>$y_t = W_{hy} h_t$</li>
</ul>
<h3 id="Example-Character-level-language-model"><a href="#Example-Character-level-language-model" class="headerlink" title="Example: Character-level language model"></a>Example: Character-level language model</h3><p>We have a vocabulary of four characters $\begin{bmatrix} h &amp; e &amp; l &amp; o \end{bmatrix}$, and the example training sequence is “hello”.</p>
<p><img src="/images/character-level_language_model_example.png" alt="character-level_language_model_example"></p>
<p>And we can look its <a href="https://gist.github.com/karpathy/d4dee566867f8291f086" target="_blank" rel="external">the implement</a>.</p>
<p><strong>Data I/O</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)</div><div class="line">BSD License</div><div class="line">"""</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># data I/O</span></div><div class="line">data = open(<span class="string">'input.txt'</span>, <span class="string">'r'</span>).read() <span class="comment"># should be simple plain text file</span></div><div class="line">chars = list(set(data))</div><div class="line">data_size, vocab_size = len(data), len(chars)</div><div class="line"><span class="keyword">print</span> <span class="string">'data has %d characters, %d unique.'</span> % (data_size, vocab_size)</div><div class="line">char_to_ix = &#123; ch:i <span class="keyword">for</span> i,ch <span class="keyword">in</span> enumerate(chars) &#125;</div><div class="line">ix_to_char = &#123; i:ch <span class="keyword">for</span> i,ch <span class="keyword">in</span> enumerate(chars) &#125;</div></pre></td></tr></table></figure>
<p><strong>Initializations</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># hyperparameters</span></div><div class="line">hidden_size = <span class="number">100</span> <span class="comment"># size of hidden layer of neurons</span></div><div class="line">seq_length = <span class="number">25</span> <span class="comment"># number of steps to unroll the RNN for</span></div><div class="line">learning_rate = <span class="number">1e-1</span></div><div class="line"></div><div class="line"><span class="comment"># model parameters</span></div><div class="line">Wxh = np.random.randn(hidden_size, vocab_size)*<span class="number">0.01</span> <span class="comment"># input to hidden</span></div><div class="line">Whh = np.random.randn(hidden_size, hidden_size)*<span class="number">0.01</span> <span class="comment"># hidden to hidden</span></div><div class="line">Why = np.random.randn(vocab_size, hidden_size)*<span class="number">0.01</span> <span class="comment"># hidden to output</span></div><div class="line">bh = np.zeros((hidden_size, <span class="number">1</span>)) <span class="comment"># hidden bias</span></div><div class="line">by = np.zeros((vocab_size, <span class="number">1</span>)) <span class="comment"># output bias</span></div></pre></td></tr></table></figure>
<p><strong>Main Loop</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">n, p = <span class="number">0</span>, <span class="number">0</span></div><div class="line">mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)</div><div class="line">mbh, mby = np.zeros_like(bh), np.zeros_like(by) <span class="comment"># memory variables for Adagrad</span></div><div class="line">smooth_loss = -np.log(<span class="number">1.0</span>/vocab_size)*seq_length <span class="comment"># loss at iteration 0</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">  <span class="comment"># prepare inputs (we're sweeping from left to right in steps seq_length long)</span></div><div class="line">  <span class="keyword">if</span> p+seq_length+<span class="number">1</span> &gt;= len(data) <span class="keyword">or</span> n == <span class="number">0</span>: </div><div class="line">    hprev = np.zeros((hidden_size,<span class="number">1</span>)) <span class="comment"># reset RNN memory</span></div><div class="line">    p = <span class="number">0</span> <span class="comment"># go from start of data</span></div><div class="line">  inputs = [char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> data[p:p+seq_length]]</div><div class="line">  targets = [char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> data[p+<span class="number">1</span>:p+seq_length+<span class="number">1</span>]]</div><div class="line"></div><div class="line">  <span class="comment"># sample from the model now and then</span></div><div class="line">  <span class="keyword">if</span> n % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">    sample_ix = sample(hprev, inputs[<span class="number">0</span>], <span class="number">200</span>)</div><div class="line">    txt = <span class="string">''</span>.join(ix_to_char[ix] <span class="keyword">for</span> ix <span class="keyword">in</span> sample_ix)</div><div class="line">    <span class="keyword">print</span> <span class="string">'----\n %s \n----'</span> % (txt, )</div><div class="line"></div><div class="line">  <span class="comment"># forward seq_length characters through the net and fetch gradient</span></div><div class="line">  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)</div><div class="line">  smooth_loss = smooth_loss * <span class="number">0.999</span> + loss * <span class="number">0.001</span></div><div class="line">  <span class="keyword">if</span> n % <span class="number">100</span> == <span class="number">0</span>: <span class="keyword">print</span> <span class="string">'iter %d, loss: %f'</span> % (n, smooth_loss) <span class="comment"># print progress</span></div><div class="line">  </div><div class="line">  <span class="comment"># perform parameter update with Adagrad</span></div><div class="line">  <span class="keyword">for</span> param, dparam, mem <span class="keyword">in</span> zip([Wxh, Whh, Why, bh, by], </div><div class="line">                                [dWxh, dWhh, dWhy, dbh, dby], </div><div class="line">                                [mWxh, mWhh, mWhy, mbh, mby]):</div><div class="line">    mem += dparam * dparam</div><div class="line">    param += -learning_rate * dparam / np.sqrt(mem + <span class="number">1e-8</span>) <span class="comment"># adagrad update</span></div><div class="line"></div><div class="line">  p += seq_length <span class="comment"># move data pointer</span></div><div class="line">  n += <span class="number">1</span> <span class="comment"># iteration counter</span></div></pre></td></tr></table></figure>
<p><strong>Loss function</strong></p>
<ul>
<li>forward pass (compute loss)</li>
<li>backward pass (compute param gradient)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lossFun</span><span class="params">(inputs, targets, hprev)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line">  inputs,targets are both list of integers.</div><div class="line">  hprev is Hx1 array of initial hidden state</div><div class="line">  returns the loss, gradients on model parameters, and last hidden state</div><div class="line">  """</div><div class="line">  xs, hs, ys, ps = &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;</div><div class="line">  hs[<span class="number">-1</span>] = np.copy(hprev)</div><div class="line">  loss = <span class="number">0</span></div><div class="line">  <span class="comment"># forward pass</span></div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(len(inputs)):</div><div class="line">    xs[t] = np.zeros((vocab_size,<span class="number">1</span>)) <span class="comment"># encode in 1-of-k representation</span></div><div class="line">    xs[t][inputs[t]] = <span class="number">1</span></div><div class="line">    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t<span class="number">-1</span>]) + bh) <span class="comment"># hidden state</span></div><div class="line">    ys[t] = np.dot(Why, hs[t]) + by <span class="comment"># unnormalized log probabilities for next chars</span></div><div class="line">    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) <span class="comment"># probabilities for next chars</span></div><div class="line">    loss += -np.log(ps[t][targets[t],<span class="number">0</span>]) <span class="comment"># softmax (cross-entropy loss)</span></div><div class="line">  <span class="comment"># backward pass: compute gradients going backwards</span></div><div class="line">  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)</div><div class="line">  dbh, dby = np.zeros_like(bh), np.zeros_like(by)</div><div class="line">  dhnext = np.zeros_like(hs[<span class="number">0</span>])</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(len(inputs))):</div><div class="line">    dy = np.copy(ps[t])</div><div class="line">    dy[targets[t]] -= <span class="number">1</span> <span class="comment"># backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here</span></div><div class="line">    dWhy += np.dot(dy, hs[t].T)</div><div class="line">    dby += dy</div><div class="line">    dh = np.dot(Why.T, dy) + dhnext <span class="comment"># backprop into h</span></div><div class="line">    dhraw = (<span class="number">1</span> - hs[t] * hs[t]) * dh <span class="comment"># backprop through tanh nonlinearity</span></div><div class="line">    dbh += dhraw</div><div class="line">    dWxh += np.dot(dhraw, xs[t].T)</div><div class="line">    dWhh += np.dot(dhraw, hs[t<span class="number">-1</span>].T)</div><div class="line">    dhnext = np.dot(Whh.T, dhraw)</div><div class="line">  <span class="keyword">for</span> dparam <span class="keyword">in</span> [dWxh, dWhh, dWhy, dbh, dby]:</div><div class="line">    np.clip(dparam, <span class="number">-5</span>, <span class="number">5</span>, out=dparam) <span class="comment"># clip to mitigate exploding gradients</span></div><div class="line">  <span class="keyword">return</span> loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)<span class="number">-1</span>]</div></pre></td></tr></table></figure>
<p><strong>Sampling</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(h, seed_ix, n)</span>:</span></div><div class="line">  <span class="string">""" </span></div><div class="line">  sample a sequence of integers from the model </div><div class="line">  h is memory state, seed_ix is seed letter for first time step</div><div class="line">  """</div><div class="line">  x = np.zeros((vocab_size, <span class="number">1</span>))</div><div class="line">  x[seed_ix] = <span class="number">1</span></div><div class="line">  ixes = []</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(n):</div><div class="line">    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)</div><div class="line">    y = np.dot(Why, h) + by</div><div class="line">    p = np.exp(y) / np.sum(np.exp(y))</div><div class="line">    ix = np.random.choice(range(vocab_size), p=p.ravel())</div><div class="line">    x = np.zeros((vocab_size, <span class="number">1</span>))</div><div class="line">    x[ix] = <span class="number">1</span></div><div class="line">    ixes.append(ix)</div><div class="line">  <span class="keyword">return</span> ixes</div></pre></td></tr></table></figure>
<p><strong>Gradient Check</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># gradient checking</span></div><div class="line"><span class="keyword">from</span> random <span class="keyword">import</span> uniform</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradCheck</span><span class="params">(inputs, target, hprev)</span>:</span></div><div class="line">  <span class="keyword">global</span> Wxh, Whh, Why, bh, by</div><div class="line">  num_checks, delta = <span class="number">10</span>, <span class="number">1e-5</span></div><div class="line">  _, dWxh, dWhh, dWhy, dbh, dby, _ = lossFun(inputs, targets, hprev)</div><div class="line">  <span class="keyword">for</span> param,dparam,name <span class="keyword">in</span> zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [<span class="string">'Wxh'</span>, <span class="string">'Whh'</span>, <span class="string">'Why'</span>, <span class="string">'bh'</span>, <span class="string">'by'</span>]):</div><div class="line">    s0 = dparam.shape</div><div class="line">    s1 = param.shape</div><div class="line">    <span class="keyword">assert</span> s0 == s1, <span class="string">'Error dims dont match: %s and %s.'</span> % (`s0`, `s1`)</div><div class="line">    <span class="keyword">print</span> name</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_checks):</div><div class="line">      ri = int(uniform(<span class="number">0</span>,param.size))</div><div class="line">      <span class="comment"># evaluate cost at [x + delta] and [x - delta]</span></div><div class="line">      old_val = param.flat[ri]</div><div class="line">      param.flat[ri] = old_val + delta</div><div class="line">      cg0, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)</div><div class="line">      param.flat[ri] = old_val - delta</div><div class="line">      cg1, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)</div><div class="line">      param.flat[ri] = old_val <span class="comment"># reset old value for this parameter</span></div><div class="line">      <span class="comment"># fetch both numerical and analytic gradient</span></div><div class="line">      grad_analytic = dparam.flat[ri]</div><div class="line">      grad_numerical = (cg0 - cg1) / ( <span class="number">2</span> * delta )</div><div class="line">      rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)</div><div class="line">      <span class="keyword">print</span> <span class="string">'%f, %f =&gt; %e '</span> % (grad_numerical, grad_analytic, rel_error)</div><div class="line">      <span class="comment"># rel_error should be on order of 1e-7 or less</span></div></pre></td></tr></table></figure>
<p><strong>Results</strong></p>
<p>Using Shakespeare’s sonnet as input:</p>
<p> <img src="/images/RNN_text_generator.png" alt="RNN_text_generator"></p>
<h3 id="Example-Image-Captioning"><a href="#Example-Image-Captioning" class="headerlink" title="Example: Image Captioning"></a>Example: Image Captioning</h3><p>We use CNN to recognize objects and use RNN to generate captions.</p>
<p><img src="/images/image_captioning_structure.png" alt="image_captioning_structure"></p>
<p>Cut the last two layers from CNN and connect it to RNN:</p>
<p><img src="/images/image_captioning_structure_1.png" alt="image_captioning_structure_1"></p>
<p>And smaple the output from previous layer to next layer as input:</p>
<p><img src="/images/image_captioning_structure_2.png" alt="image_captioning_structure_2"></p>
<p>Sampling is stoped when meeting an END</p>
<p><img src="/images/image_captioning_structure_3.png" alt="image_captioning_structure_3"></p>
<p>Finally, we’ll get a complete sentence (using <a href="http://mscoco.org" target="_blank" rel="external">Microsoft COCO dataset</a>). The first row are good, but the second row may be not satisfactory.</p>
<p><img src="/images/image_captioning_result.png" alt="image_captioning_result"></p>
<blockquote>
<p><strong>Reference</strong>:</p>
<ul>
<li>Explain Images with Multimodal Recurrent Neural Networks, Mao et al.</li>
<li>Deep Visual-Semantic Alignments for Generating Image Descriptions, Karpathy and Fei-Fei</li>
<li>Show and Tell: A Neural Image Caption Generator, Vinyals et al.</li>
<li>Long-term Recurrent Convolutional Networks for Visual Recognition andDescription, Donahue et al.</li>
<li>Learning a Recurrent Visual Representation for Image CaptionGeneration, Chen and Zitnick</li>
</ul>
</blockquote>
<h3 id="More-examples"><a href="#More-examples" class="headerlink" title="More examples"></a>More examples</h3><p>We can also use RNN to generate open source textbooks written in LaTex, or generate C code from Linux source code, or searching for interpretable cells.</p>
<h2 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long Short Term Memory (LSTM)"></a>Long Short Term Memory (LSTM)</h2><h3 id="Vanishing-Exploding-gradients"><a href="#Vanishing-Exploding-gradients" class="headerlink" title="Vanishing/Exploding gradients"></a>Vanishing/Exploding gradients</h3><ul>
<li>Exploding gradients<ul>
<li>Truncated BPTT</li>
<li><strong>Clip gradients at threshold</strong> (something like anti-windup in control science LOL)</li>
<li>RMSProp to adjust learning rate</li>
</ul>
</li>
<li>Vanishing gradients<ul>
<li>Harder to detect</li>
<li>Weight Initialization</li>
<li>ReLU activation functions</li>
<li>RMSProp</li>
<li><strong>LSTM, GRUs</strong> (&lt;– That’s why we use LSTM)</li>
</ul>
</li>
</ul>
<h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><p>LSTM is proposed in [Hochreiter et al., 1997]. GRU is a knid of simplified LSTM.</p>
<p> <img src="/images/LSTM_diagram.png" alt="LSTM_diagram"></p>
<blockquote>
<p>ResNet is to PlainNet what LSTM is to RNN, kind of.</p>
<p> <img src="/images/plainnet_vs_resnet.png" alt="plainnet_vs_resnet"></p>
</blockquote>
<h3 id="Concept-1"><a href="#Concept-1" class="headerlink" title="Concept"></a>Concept</h3><p> <img src="/images/LSTM_structure.png" alt="LSTM_structure"></p>
<p>LSTM have two states, one is <strong>cell state</strong> ($c$), another is <strong>hidden state</strong> ($h$):</p>
<ul>
<li>$i$: input gate, “add to memory”, decides whether do we want to add value to this cell.</li>
<li>$f$: forget gate, “flush the memory”, decides whether to shut off the cell and reset the counter.</li>
<li>$o$: output gate, “get from memory”, decides how much do we want to get from this cell.</li>
<li>$g$: input, decides how much do we want to add to this cell.</li>
</ul>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>RNNs allow a lot of flexibility inarchitecture design</li>
<li>Vanilla RNNs are simple but don’twork very well</li>
<li>Common to use LSTM or GRU: theiradditive interactions improve gradient flow</li>
<li>Backward flow of gradients in RNNcan explode or vanish. Exploding is controlled with gradient clipping.Vanishing is controlled with additive interactions (LSTM)</li>
<li>Better/simpler architectures are ahot topic of current research</li>
<li>Better understanding (boththeoretical and empirical) is needed.</li>
</ul>
<p>(To be improved by adding extra materials…)</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;从 RNN 开始, CS231n 的 Lecture Notes 就没有了, 因此我根据上课时的 Slides 整理了一些需要重视的知识点. 还可以参考这些文章或是视频来加深理解:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/22107715?refer=intelligentunit&quot;&gt;[原创翻译]循环神经网络惊人的有效性（上）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/22230074?refer=intelligentunit&quot;&gt;[原创翻译]循环神经网络惊人的有效性（下）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Ukgii7Yd_cU&quot;&gt;Recurrent Neural Networks (Video, recommend)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://james371507.wixsite.com/hylee/single-post/2016/03/20/Recurrent-Neural-Network-RNN-Note-of-Stanford-CS231n&quot;&gt;Recurrent Neural Network (RNN) (Note of Stanford CS231n)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="CS231n" scheme="http://www.yuthon.com/tags/CS231n/"/>
    
      <category term="Recurrent Neural Network" scheme="http://www.yuthon.com/tags/Recurrent-Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Traffic Prediction Using LSTM</title>
    <link href="http://www.yuthon.com/2016/10/30/Traffic-Prediction-Using-LSTM/"/>
    <id>http://www.yuthon.com/2016/10/30/Traffic-Prediction-Using-LSTM/</id>
    <published>2016-10-30T05:53:32.000Z</published>
    <updated>2016-11-01T12:16:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近上的一门课 “无线传感器网络” 快要结束了, 于是所谓的大作业的 DDL 也压上来了. TAT</p>
<p>不过这门课虽然说是讲无线传感器网络的, 但是大作业的要求却额外的宽松, 只要是和数据分析有关的就好了. 老师还给了些数据集, 比如说公共自行车的出借与归入记录啊, 出租车在各个路段的行驶速度啊, 或者是顺丰快递途径各个城市需要的时间啊这类的. 当然也可以自己选题. </p>
<p>我当然是想自己选题的, 然而想了一圈没想到什么好的方案, 于是只好回到了老师给的题目上面来, 选了道路速度预测这样的题目. 刚好之前在 CS231n 上看了 RNN 和 LSTM, 心想这总比传统方法好点吧, 于是就开始干了. (于是就有了之前的那篇装 CUDA 和 TF)</p>
<p>I wanna traffic prediction, I learn LSTM.</p>
<p>ugh, Traffic prediction using LSTM!</p>
<p>(此处应有 PPAP)</p>
<a id="more"></a>
<h2 id="RNN-与-LSTM-基本原理"><a href="#RNN-与-LSTM-基本原理" class="headerlink" title="RNN 与 LSTM 基本原理"></a>RNN 与 LSTM 基本原理</h2><p>直接看我 CS231n 相关的课程笔记吧</p>
<p><a href="http://www.yuthon.com/2016/10/30/Notes-for-CS231n-RNN/">Notes for CS231n Recurrent Neural Network</a></p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>详细代码可见我Github上的项目 <strong><a href="https://github.com/corenel/traffic-prediction" target="_blank" rel="external">traffic-prediction</a></strong>. 为了课堂展示我还做了一个pdf, 可以从此处下载.</p>
<p>本次用了两层的 LSTM, 中间加了 Dropout:</p>
<p><img src="/images/traffic_prediction_model.png" alt="traffic_prediction_model"></p>
<p>输入是一个 4 元素的向量, 分别是星期几, 是否周末, 小时与分钟. </p>
<p>$Input = \begin{bmatrix}Weekday &amp; isWeekend &amp; Hour &amp; Minute\end{bmatrix}$</p>
<p>输出自然是道路上此刻的速度</p>
<p>$Output = \begin{bmatrix} Velocity \end{bmatrix}$</p>
<blockquote>
<p>话说 Keras 竟然能用 graphviz 直接输出模型的结构图, 真是方便</p>
</blockquote>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>老师给的数据集简直弱爆了, 一条路上总共2000+条数据, 还是按照小时计的, 训练出来的结果惨不忍睹.</p>
<p>于是在网上找到了 <a href="http://pems.dot.ca.gov/" target="_blank" rel="external">Caltrans Performance Measurement System (PeMS)</a> 这个网站, 里面数据是每 5 分钟采样一次的, 比前面的那个不知高到哪里去了.</p>
<p>此次选取的是 16444 路段, 时间是 2016-05-01 到 2016-10-26 总共 6 个月 5W+ 条数据.</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>一天(2016-10-26)和一周(2016.10.20 - 2016.10.26)的预测如下:</p>
<p><img src="/images/traffic_prediction_result_1.png" alt="traffic_prediction_result_1"></p>
<p><img src="/images/traffic_prediction_result_2.png" alt="traffic_prediction_result_2"></p>
<p>可以看出, 总体的趋势还是不错的, 但是高峰的部分还是有些够不上. 同时, 也确实预测到了周末与工作日的速度的区别.</p>
<blockquote>
<p>matplotlib 可以用 ggplot 的样式, 好看多了</p>
</blockquote>
<h2 id="Deeper"><a href="#Deeper" class="headerlink" title="Deeper"></a>Deeper</h2><p>使用了3层LSTM, MSE有一定下降, 但是高峰期跟不上的问题还是没有解决</p>
<p> <img src="/images/traffic_prediction_result_4.png" alt="traffic_prediction_result_4"></p>
<p> <img src="/images/traffic_prediction_result_5.png" alt="traffic_prediction_result_5"></p>
<p> <img src="/images/traffic_prediction_result_6.png" alt="traffic_prediction_result_6"></p>
<h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><ul>
<li>加深层数</li>
<li>仔细考虑输入向量的长度和内容, 还可加入假日, 天气等(老师给的数据集有, 但是PeMS没)</li>
<li>使用 Stateful LSTM 的尝试失败了</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近上的一门课 “无线传感器网络” 快要结束了, 于是所谓的大作业的 DDL 也压上来了. TAT&lt;/p&gt;
&lt;p&gt;不过这门课虽然说是讲无线传感器网络的, 但是大作业的要求却额外的宽松, 只要是和数据分析有关的就好了. 老师还给了些数据集, 比如说公共自行车的出借与归入记录啊, 出租车在各个路段的行驶速度啊, 或者是顺丰快递途径各个城市需要的时间啊这类的. 当然也可以自己选题. &lt;/p&gt;
&lt;p&gt;我当然是想自己选题的, 然而想了一圈没想到什么好的方案, 于是只好回到了老师给的题目上面来, 选了道路速度预测这样的题目. 刚好之前在 CS231n 上看了 RNN 和 LSTM, 心想这总比传统方法好点吧, 于是就开始干了. (于是就有了之前的那篇装 CUDA 和 TF)&lt;/p&gt;
&lt;p&gt;I wanna traffic prediction, I learn LSTM.&lt;/p&gt;
&lt;p&gt;ugh, Traffic prediction using LSTM!&lt;/p&gt;
&lt;p&gt;(此处应有 PPAP)&lt;/p&gt;
    
    </summary>
    
      <category term="Projects" scheme="http://www.yuthon.com/categories/Projects/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="Traffic Prediction" scheme="http://www.yuthon.com/tags/Traffic-Prediction/"/>
    
      <category term="LSTM" scheme="http://www.yuthon.com/tags/LSTM/"/>
    
      <category term="RNN" scheme="http://www.yuthon.com/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>CUDA and Tensorflow Installation on Ubuntu 16.04</title>
    <link href="http://www.yuthon.com/2016/10/25/CUDA-and-Tensorflow-Installation-on-Ubuntu-16-04/"/>
    <id>http://www.yuthon.com/2016/10/25/CUDA-and-Tensorflow-Installation-on-Ubuntu-16-04/</id>
    <published>2016-10-25T12:53:50.000Z</published>
    <updated>2016-10-30T07:01:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>昨天折腾了一个下午开发环境的配置，记录一下其中遇到的坑。</p>
<a id="more"></a>
<h2 id="硬件配置"><a href="#硬件配置" class="headerlink" title="硬件配置"></a>硬件配置</h2><p>我的硬件配置(XPS 15 9550):</p>
<ul>
<li>CPU: i5 6300HQ</li>
<li>GPU: GTX960M 2G</li>
<li>内存: 16G DDR4 2133</li>
<li>硬盘: 512G SM951 NVMe</li>
</ul>
<blockquote>
<p>基本上就只处于玩票的状态, 实验室快给我配1080啊~~~</p>
</blockquote>
<p>上一个 NVIDIA 钦定的 DevBox <a href="http://www.nvidia.com/object/deep-learning-system.html" target="_blank" rel="external">配置</a>:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">GPUs</td>
<td>8x Tesla P100</td>
</tr>
<tr>
<td style="text-align:left">TFLOPS (GPU FP16 /CPU FP32)</td>
<td>170/3</td>
</tr>
<tr>
<td style="text-align:left">GPU Memory</td>
<td>16 GB per GPU</td>
</tr>
<tr>
<td style="text-align:left">CPU</td>
<td>Dual 20-core Intel® Xeon®E5-2698 v4 2.2 GHz</td>
</tr>
<tr>
<td style="text-align:left">NVIDIA CUDA® Cores</td>
<td>28672</td>
</tr>
<tr>
<td style="text-align:left">System Memory</td>
<td>512 GB 2133 MHz DDR4</td>
</tr>
<tr>
<td style="text-align:left">Storage</td>
<td>4x 1.92 TB SSD RAID 0</td>
</tr>
<tr>
<td style="text-align:left">Network</td>
<td>Dual 10 GbE, 4 IB EDR</td>
</tr>
<tr>
<td style="text-align:left">Software</td>
<td>Ubuntu Server Linux OSDGX-1 Recommended GPUDriver</td>
</tr>
<tr>
<td style="text-align:left">System Weight</td>
<td>134 lbs</td>
</tr>
<tr>
<td style="text-align:left">System Dimensions</td>
<td>866 D x 444 W x 131 H (mm)</td>
</tr>
<tr>
<td style="text-align:left">Packing Dimensions</td>
<td>1180 D x 730 W x 284 H (mm)</td>
</tr>
<tr>
<td style="text-align:left">Maximum Power Requirements</td>
<td>3200W</td>
</tr>
<tr>
<td style="text-align:left">Operating Temperature Range</td>
<td>10 - 30°C</td>
</tr>
</tbody>
</table>
<p>这简直是吾辈梦想神机啊… 然而要 <strong>$129000</strong> !</p>
<h2 id="系统环境"><a href="#系统环境" class="headerlink" title="系统环境"></a>系统环境</h2><p>推荐是用 Ubuntu 最新的 LTS 版本 16.04.1, 对 Skylake 系列的 CPU 和主板的支持都很不错. 关于在 XPS 15 9550 上的详细配置过程, 我计划稍后专门写一篇.</p>
<p>本来想在 macOS 上跑的, 奈何黑苹果不支持独显.</p>
<h2 id="安装-CUDA"><a href="#安装-CUDA" class="headerlink" title="安装 CUDA"></a>安装 CUDA</h2><p><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="external">CUDA</a> 与 <a href="https://developer.nvidia.com/cudnn" target="_blank" rel="external">cuDNN</a> 的安装在 NVIDIA 与 Tensorflow的官网上都有<a href="https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#optional-install-cuda-gpus-on-linux" target="_blank" rel="external">详细说明</a>, 此处仅就一些关键环节作出说明.</p>
<ul>
<li><p>禁用开源的 Nouveau 驱动</p>
<ul>
<li><p>首先看看有没有在使用这个开源驱动:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ lsmod | grep nouveau</div></pre></td></tr></table></figure>
</li>
<li><p>创建<code>/etc/modprobe.d/blacklist-nouveau.conf</code>文件, 并写入以下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">blacklist nouveau</div><div class="line">options nouveau modeset=0</div></pre></td></tr></table></figure>
</li>
<li><p>重启kernel initramfs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo update-initramfs -u</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>从ppa源下载最新版的驱动(&gt;364), 或者使用 CUDA-Toolkit 自带的驱动:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get purge nvidia-*</div><div class="line">$ sudo add-apt-repository ppa:graphics-drivers/ppa</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install nvidia-370</div></pre></td></tr></table></figure>
</li>
<li><p>下载 CUDA 的 runfile (local) 版本, 不要使用apt-get 的方式, 保证获取到的是最新的版本 (目前是8.0).</p>
</li>
<li><p>安装过程中, 需要<code>Ctrl+Alt+F1</code>切换到 tty 界面, 然后关闭 X server:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo service lightdm stop</div></pre></td></tr></table></figure>
<p>之后再执行安装过程</p>
</li>
<li><p>安装过程中, 如果是像我一样的 Intel 核显 + NVIDIA 独显的, <strong>绝对不要装 OpenGL</strong>, 否则重启后会陷入 login loop.</p>
</li>
<li><p>安装完成之后, 设置环境变量:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export LD_LIBRARY_PATH=&quot;$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64&quot;</div><div class="line">export CUDA_HOME=/usr/local/cuda</div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="安装-cuDNN"><a href="#安装-cuDNN" class="headerlink" title="安装 cuDNN"></a>安装 cuDNN</h2><p>从<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="external">官网</a>下载之后, 执行以下命令 (目前最新版本5.1):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">tar xvzf cudnn-8.0-linux-x64-v5.1-ga.tgz</div><div class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda/include</div><div class="line">sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</div><div class="line">sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*</div></pre></td></tr></table></figure>
<h2 id="安装Tensorflow"><a href="#安装Tensorflow" class="headerlink" title="安装Tensorflow"></a>安装Tensorflow</h2><p>这是最纠结的一步, 之前按照别人教程说最好从源码编译支持 GPU. 但是由于国内的网络环境, 源码编译需要下一堆的依赖包, 速度超级慢. 因此还是使用官网推荐的 pip 安装方式:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># Get pip</div><div class="line">$ sudo apt-get install python-pip python-dev</div><div class="line"></div><div class="line"># Ubuntu/Linux 64-bit, GPU enabled, Python 2.7</div><div class="line"># Requires CUDA toolkit 8.0 and CuDNN v5. For other versions, see &quot;Install from sources&quot; below.</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc1-cp27-none-linux_x86_64.whl</div><div class="line"></div><div class="line"># Python 2</div><div class="line">$ sudo pip install --upgrade $TF_BINARY_URL</div></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>可以先把<code>tensorflow-0.11.0rc1-cp27-none-linux_x86_64.whl</code>下下来, 本地执行安装命令</li>
<li>可以使用国内的 pip 源, 加快安装依赖的速度</li>
</ul>
</blockquote>
<h2 id="测试-Tensorflow"><a href="#测试-Tensorflow" class="headerlink" title="测试 Tensorflow"></a>测试 Tensorflow</h2><p>用下面的小例子来测试下 Tensorflow 安装得成不成功:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$ python</div><div class="line">...</div><div class="line">&gt;&gt;&gt; import tensorflow as tf</div><div class="line">&gt;&gt;&gt; hello = tf.constant(&apos;Hello, TensorFlow!&apos;)</div><div class="line">&gt;&gt;&gt; sess = tf.Session()</div><div class="line">&gt;&gt;&gt; print(sess.run(hello))</div><div class="line">Hello, TensorFlow!</div><div class="line">&gt;&gt;&gt; a = tf.constant(10)</div><div class="line">&gt;&gt;&gt; b = tf.constant(32)</div><div class="line">&gt;&gt;&gt; print(sess.run(a + b))</div><div class="line">42</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure>
<p>在 <code>import</code>的同时, 还会显示 CUDA 方面与 GPU 方面的信息.</p>
<p>至此大功告成!</p>
<h2 id="Docker-for-tensorflow"><a href="#Docker-for-tensorflow" class="headerlink" title="Docker for tensorflow"></a>Docker for tensorflow</h2><p>其实在这期间还试过直接用装了 Tensorflow 的 Docker 镜像, 也有支持 GPU 的版本. 各位如有兴趣可以试试:</p>
<ul>
<li><a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker" target="_blank" rel="external">Github - Using TensorFlow via Docker</a></li>
<li><a href="https://github.com/saiprashanths/dl-docker" target="_blank" rel="external">Github - An all-in-one Docker image for deep learning. Contains all the popular DL frameworks (TensorFlow, Theano, Torch, Caffe, etc.)</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;昨天折腾了一个下午开发环境的配置，记录一下其中遇到的坑。&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="CUDA" scheme="http://www.yuthon.com/tags/CUDA/"/>
    
      <category term="Tensorflow" scheme="http://www.yuthon.com/tags/Tensorflow/"/>
    
      <category term="Ubuntu" scheme="http://www.yuthon.com/tags/Ubuntu/"/>
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Notes for CS231n Convolutional Neural Network</title>
    <link href="http://www.yuthon.com/2016/10/19/Notes-for-CS231n-CNN/"/>
    <id>http://www.yuthon.com/2016/10/19/Notes-for-CS231n-CNN/</id>
    <published>2016-10-19T03:06:30.000Z</published>
    <updated>2016-10-20T12:23:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:</p>
<ul>
<li><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">Convolutional Neural Networks: Architectures, Convolution / Pooling Layers</a></li>
</ul>
<p>或者可以看知乎专栏中的中文翻译:</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：卷积神经网络笔记 </a></li>
</ul>
<p>另外, 本文主要根据讲课的 Slides 上的顺序来, 与 Lecture Notes 的顺序略有不同.</p>
</blockquote>
<a id="more"></a>
<h1 id="Lecture-7"><a href="#Lecture-7" class="headerlink" title="Lecture 7"></a>Lecture 7</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>CNN 主要有以下的层(layer):</p>
<ul>
<li>卷积层 (Conv Layer): 通过不同的 filter 进行卷积操作, 来增加 depth</li>
<li>ReLU 层</li>
<li>汇聚层 / 池化层 (Pooling Layer): 进行 down-sampling, 减小空间尺寸</li>
<li>全连接层 (Full-connected Layer): 放在最后进行 classification, 相当于普通的 NN</li>
</ul>
<p><img src="/images/CNN_structure.png" alt="CNN_structure"></p>
<blockquote>
<p>CNN 相对于 NN 来说, 其结构基于输入数据是图像这么一个假设. 基于该假设, 我们就向结构中添加了一些特有的性质. 这些特有属性使得前向传播函数实现起来更高效, 并且大幅度降低了网络中参数的数量. 这也是 CNN 更适用于图像方面的原因.</p>
</blockquote>
<h3 id="Conv-layer"><a href="#Conv-layer" class="headerlink" title="Conv layer"></a>Conv layer</h3><p>主要需要了解以下几个概念:</p>
<ul>
<li><p><strong>滤波器(Filter)</strong>: 又叫卷积核 (Kernel), 尺寸较小 (例如5x5x3). 通过在输入数据上滑动来生成新的 Activation Map / Feature Map.</p>
<p><img src="/images/conv_layer_filter.png" alt="conv_layer_filter"></p>
<ul>
<li>滤波器的深度须与输入数据的深度一致. 也就是说输入 32x32x3 的图像, 其对应的滤波器的尺寸必须是 FxFx3.</li>
<li>下一层的深度取决于这层用了几个滤波器</li>
<li>滤波器的尺寸又称感受野 (Receptive Field)</li>
</ul>
</li>
<li><p><strong>步长 (Stride)</strong>: 即指滤波器每次移动几个像素. 通常步长为奇数.</p>
<p><img src="/images/CNN_stride.png" alt="CNN_stride"></p>
</li>
<li><p><strong>零填充 (Zero-padding)</strong>: 用来保证滤波器完整平滑地划过输入数据, 不出现非整数的问题. 同时还能够用来保持输入与输出数据具有相同的尺寸, 即令$P=(F-1)/2$.</p>
<p><img src="/images/CNN_padding.png" alt="CNN_padding"></p>
</li>
</ul>
<p>如是这般, 宽度与高度不断缩小, 深度不断增加, 信息提取得更为抽象.</p>
<p><img src="/images/CNN_layers.png" alt="CNN_layers"></p>
<p><strong>总结</strong></p>
<ul>
<li>输入数据尺寸$W_1 \times H_1 \times D_1$</li>
<li>需要的超参数<ul>
<li>滤波器数量$K$, 通常是2的几次幂, 例如32, 64, 128, 512等</li>
<li>滤波器尺寸$F$, 通常为1, 3, 5等</li>
<li>步长$S$, 通常为1或2</li>
<li>零填充数量$P$</li>
</ul>
</li>
<li>输出数据尺寸$W_2 \times H_2 \times D_2$<ul>
<li>$W_2 = (W_1 - F + 2P) / S + 1$</li>
<li>$H_2 = (H_1 - F + 2P) / S + 1$ (通常$W_1=H_1,W_2=H_2$)</li>
<li>$D_2 = K$</li>
</ul>
</li>
<li>就参数共享来说, 每个滤波器有$F\cdot F\cdot D_1$个权重参数, 总共有$(F\cdot F\cdot D_1)\cdot K$个权重参数 (weights) 和$K$个偏差参数 (biases).</li>
<li>在输出数据体中, 第$d$层 (尺寸$W_2\times H_2$) 深度切片(depth slice)是由第$d$个滤波器在输入数据体上以$S$为补偿进行有效的卷积, 并且偏移了第$d$个偏差之后得到的.</li>
<li>有时候会有$1\times 1\times D$的滤波器, 其也是有效的. 因为它有深度, 实际上进行的是一个$D$维的点积.</li>
</ul>
<h2 id="Pooling-layer"><a href="#Pooling-layer" class="headerlink" title="Pooling layer"></a>Pooling layer</h2><ul>
<li>makes the representations smaller and more manageable </li>
<li><p>operates over each activation map independently</p>
<p><img src="/images/pooling_layer.png" alt="pooling_layer"></p>
</li>
</ul>
<p><strong>总结</strong></p>
<ul>
<li>输入数据尺寸$W_1 \times H_1 \times D_1$</li>
<li>需要的超参数<ul>
<li>滤波器尺寸$F$, 通常为2或3</li>
<li>步长$S$, 通常为2</li>
</ul>
</li>
<li>输出数据尺寸$W_2 \times H_2 \times D_2$<ul>
<li>$W_2 = (W_1 - F) / S + 1$</li>
<li>$H_2 = (H_1 - F) / S + 1$ (通常$W_1=H_1,W_2=H_2$)</li>
<li>$D_2 = D_1$</li>
</ul>
</li>
<li>由于是固定的计算, 因此没有引入参数.</li>
<li>通常不在汇聚层中使用零填充</li>
</ul>
<h2 id="Case-study"><a href="#Case-study" class="headerlink" title="Case study"></a>Case study</h2><p>To be continued…</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;Convolutional Neural Networks: Architectures, Convolution / Pooling Layers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;或者可以看知乎专栏中的中文翻译:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit&quot;&gt;CS231n课程笔记翻译：卷积神经网络笔记 &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另外, 本文主要根据讲课的 Slides 上的顺序来, 与 Lecture Notes 的顺序略有不同.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="CS231n" scheme="http://www.yuthon.com/tags/CS231n/"/>
    
      <category term="Convolutional Neural Network" scheme="http://www.yuthon.com/tags/Convolutional-Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Notes for CS231n Neural Network</title>
    <link href="http://www.yuthon.com/2016/10/16/Notes-for-CS231n-NN/"/>
    <id>http://www.yuthon.com/2016/10/16/Notes-for-CS231n-NN/</id>
    <published>2016-10-16T13:04:26.000Z</published>
    <updated>2016-10-18T11:45:51.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:</p>
<ul>
<li><a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="external">Neural Networks Part 1: Setting up the Architecture</a></li>
<li><a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="external">Neural Networks Part 2: Setting up the Data and the Loss</a></li>
<li><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">Neural Networks Part 3: Learning and Evaluation</a></li>
</ul>
<p>或者可以看知乎专栏中的中文翻译:</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记1（上）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记1（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记2 </a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记3（上）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记3（下）</a></li>
</ul>
<p>另外, 本文主要根据讲课的 Slides 上的顺序来, 与 Lecture Notes 的顺序略有不同.</p>
</blockquote>
<a id="more"></a>
<h1 id="Lecture-5"><a href="#Lecture-5" class="headerlink" title="Lecture 5"></a>Lecture 5</h1><h2 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h2><p>课程中主要讲了Sigmoid, tanh, ReLU, Leaky ReLU, Maxout 以及 ELU 这几种激活函数. </p>
<p><img src="/images/activation_functions.png" alt="activation_functions"></p>
<ul>
<li>Sigmoid 由于以下原因, 基本不使用<ul>
<li>函数饱和使得梯度消失(Saturated neurons “kill” the gradients)</li>
<li>函数并非以零为中心(zero-centered)</li>
<li>指数运算消耗大量计算资源</li>
</ul>
</li>
<li>tanh 相对于 Sigmoid 来说, 多了零中心这一个特性, 但还是不常用</li>
<li>重头戏 ReLU (Rectified Linear Unit):<ul>
<li>在正半轴上没有饱和现象</li>
<li>线性结构省下了很多计算资源, 可以直接对矩阵进行阈值计算来实现, 速度是 sigmoid/tanh 的6倍</li>
<li>然而由于负半轴直接是0, 训练的时候会”死掉”(die), 因此就有了 Leaky ReLU 和 ELU (Exponential Linear Units), 以及更加通用的 Maxout (代价是消耗两倍的计算资源)</li>
</ul>
</li>
</ul>
<p><strong>实践中一般就直接选 ReLU, 同时注意 Learning Rate 的调整. 实在不行用 Leaky ReLU 或者 Mahout 碰碰运气. 还可以试试 tanh. 坚决别用 Sigmoid.</strong></p>
<h2 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h2><p>有很多数据预处理的方法, 比如零中心化(zero-centering), 归一化(normalization), PCA(Principal Component Analysis, 主成分分析)和白化(Whitening).</p>
<ul>
<li><p>零中心化(zero-centering): 主要方法就是均值减法, 将数据的中心移到原点上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Assume X [NxD] is data matrix, each example in a row</span></div><div class="line">X -= np.mean(X, axis=<span class="number">0</span>)</div></pre></td></tr></table></figure>
<p>零中心化主要有两种做法(e.g. consider CIFAR-10 example with [32,32,3] images):</p>
<ul>
<li><p>Subtract the mean image (e.g.AlexNet)  (mean image = [32,32,3] array)</p>
</li>
<li><p>Subtract per-channel mean (e.g.VGGNet)  (mean along each channel = 3 numbers)</p>
</li>
</ul>
</li>
<li><p>归一化(normalization): 使得数据所有维度的范围基本相等, 当然由于图像像素的数值范围本身基本是一致的(一般为0-255), 所以不一定要用.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">X /= np.std(X, axis=<span class="number">0</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>PCA 和白化在 CNN 中并没有什么用, 就不介绍了.</p>
<p><img src="/images/data_preprocessing.png" alt="data_preprocessing"></p>
</li>
</ul>
<p><strong>实践中一般就只做零中心化, 其他几样基本都不用做.</strong></p>
<blockquote>
<p>以下引自知乎专栏[智能单元]所翻译的课程讲义:</p>
<p><strong>常见错误。</strong>进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。<strong>应该怎么做呢？应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。</strong></p>
<p><strong>译者注：此处确为初学者常见错误，请务必注意！</strong></p>
</blockquote>
<h2 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h2><p>由于各种原因, 将 Weight 全部初始化为0, 或者是小随机数的方法都不大好(一个是由于对称性, 另一个是由于梯度信号太小). 建议使用的是下面这个(配合 ReLU):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">w = np.random.randn(n) * sqrt(<span class="number">2.0</span>/n)</div></pre></td></tr></table></figure>
<p>或者 Xavier initialization:</p>
<p> <img src="/images/xavier_init.png" alt="xavier_init"></p>
<p>另外就是还推荐 <strong>Batch Normalization</strong> (批量归一化), 通常应用在全连接层之后, 激活函数之前. 具体参见论文[Ioffe and Szegedy, 2015]. </p>
<p><img src="/images/batch_normalizaition.png" alt="batch_normalizaition"></p>
<ul>
<li>Improves gradient flow through thenetwork</li>
<li>Allows higher learning rates</li>
<li>Reduces the strong dependence on initialization</li>
<li>Acts as a form of regularization in afunny way, and slightly reduces the need for dropout, maybe</li>
</ul>
<h2 id="Babysitting-the-Learning-Process"><a href="#Babysitting-the-Learning-Process" class="headerlink" title="Babysitting the Learning Process"></a>Babysitting the Learning Process</h2><h3 id="Double-check-that-the-loss-is-reasonable"><a href="#Double-check-that-the-loss-is-reasonable" class="headerlink" title="Double check that the loss is reasonable"></a>Double check that the loss is reasonable</h3><ul>
<li>首先不使用 regularization, 观察 loss 是否合理(下例中对于 CIFAR-10 的初始 loss 应近似等于$log(0.1)=2.31$)</li>
<li><p>然后再开启 regularization, 观察 loss 是否上升</p>
<p><img src="/images/loss_double_check.png" alt="loss_double_check"></p>
</li>
</ul>
<h3 id="Other-sanity-check-tips"><a href="#Other-sanity-check-tips" class="headerlink" title="Other sanity check tips"></a>Other sanity check tips</h3><ul>
<li><p>首先在一个小数据集上进行训练(可先设 regualrization 为0), 看看是否过拟合, 确保算法的正确性.</p>
<p><img src="/images/overfit_on_a_small_portion_of_training_data.png" alt="overfit_on_a_small_portion_of_training_data"></p>
</li>
<li><p>之后再从一个小的 regularization 开始, 寻找合适的能够使 loss 下降的 learning rate.</p>
<ul>
<li><p>如果几次 epoch 后, loss 没没有下降, 说明 learning rate 太小了</p>
<p> <img src="/images/loss_barely_changing.png" alt="loss_barely_changing"></p>
</li>
<li><p>如果 loss 爆炸了, 那么说明 learning rate 太大了</p>
<p> <img src="/images/loss_exploding.png" alt="loss_exploding"></p>
</li>
<li><p>通常 learning rate 的范围是$[1e-3, 1e-5]$</p>
</li>
</ul>
</li>
</ul>
<h2 id="Hyperparameter-Optimization"><a href="#Hyperparameter-Optimization" class="headerlink" title="Hyperparameter Optimization"></a>Hyperparameter Optimization</h2><ul>
<li><p><strong>从粗放(coarse)到细致(fine)地分段搜索</strong>, 先大范围小周期(1-5 epoch足矣), 然后再根据结果小范围长周期</p>
<ul>
<li>First stage: only a few epochs to get rough idea of what params work</li>
<li>Second stage: longer running time, finer search</li>
<li>… (repeat as necessary)</li>
</ul>
<blockquote>
<p>If the cost is ever &gt; 3 * original cost, break out early</p>
</blockquote>
</li>
<li><p><strong>在对数尺度上进行搜索</strong>, 例如<code>learning_rate = 10 ** uniform(-6, 1)</code>. 当然有些超参数还是按原来的, 比如 <code>dropout = uniform(0,1)</code></p>
</li>
<li><p><strong>小心边界上的最优值</strong>, 否则可能会错过更好的参数搜索范围.</p>
<p> <img src="/images/coarse_search.png" alt="coarse_search"></p>
<p> <img src="/images/finer_search.png" alt="finer_search"></p>
</li>
<li><p><strong>随机搜索优于网格搜索</strong></p>
<p> <img src="/images/random_search_vs_grid_search .png" alt="random_search_vs_grid_search "></p>
</li>
</ul>
<h1 id="Lecture-6"><a href="#Lecture-6" class="headerlink" title="Lecture 6"></a>Lecture 6</h1><h2 id="Parameter-Updates"><a href="#Parameter-Updates" class="headerlink" title="Parameter Updates"></a>Parameter Updates</h2><p> 参数更新有很多种方法, 常见的如下图:<img src="/images/parameter_update.png" alt="parameter_update"></p>
<ul>
<li><p>最普通的就是SGD, 仅仅按照负梯度来更新</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x += - learning_rate * dx</div></pre></td></tr></table></figure>
<p><img src="/images/sgd.png" alt="sgd"></p>
</li>
<li><p>其次就是各种动量方法, 比如 <strong>Momentum</strong>,  以及其衍生的 <strong>Nesterov</strong> 方法. 其主要思想就是在任何具有持续梯度的方向上保持一个会慢慢消失的动量, 使得梯度下降更为圆滑.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Momentum update</span></div><div class="line">v = mu * v - learning_rate * dx <span class="comment"># integrate velocity</span></div><div class="line">x += v <span class="comment"># integrate position</span></div><div class="line"></div><div class="line"><span class="comment"># Mesterov momentum update rewrite</span></div><div class="line">v_prev = v</div><div class="line">v = mu * v - learning_rate * dx</div><div class="line">x += -mu * v_prev + (<span class="number">1</span> + mu) * v</div></pre></td></tr></table></figure>
<p><img src="/images/momentum_and_Nesterov.png" alt="momentum_and_Nesterov"></p>
<blockquote>
<ul>
<li>v 初始为 0</li>
<li>mu 一般取 0.5, 0.9 或 0.99. 有时候可以先 0.5, 然后慢慢变成 0.99</li>
</ul>
</blockquote>
</li>
<li><p>然后就是逐步改 learning rate 的方法, 比如 AdaGrad 或者 RMSProp (Hinton 大神在 Coursera 课上提出的改进方法)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># AdaGrad</span></div><div class="line">cache += dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div><div class="line"></div><div class="line"><span class="comment"># RMSProp</span></div><div class="line">cache = decay_rate * cache + (<span class="number">1</span> - decay_rate) * dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>cache 尺寸与 dx 相同</li>
<li>eps 取值在 1e-4 到 1e-8 之间, 主要是为了防止分母为 0.</li>
<li>AdaGrad 通常过早停止学习, RMSProp 通过引入一个梯度平方的滑动平均改善了它.</li>
</ul>
</blockquote>
</li>
<li><p>最后就是集上述方法之大成的 <strong>Adam</strong>, 在大多数的实践中都是一个很好的选择.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Adam</span></div><div class="line">m ,v = <span class="comment"># ... initialize cacahe to zeros</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> xrange(<span class="number">1</span>, big_number):</div><div class="line">    dx = <span class="comment"># ... evaluate gradient</span></div><div class="line">    m = beta1 * m + (<span class="number">1</span> - beta1) * dx <span class="comment"># update first momentum</span></div><div class="line">    v = beta2 * v + (<span class="number">1</span> - beta2) * (dx ** <span class="number">2</span>) <span class="comment"># update second momentum</span></div><div class="line">    mb = m / (<span class="number">1</span> - beta1 ** t) <span class="comment"># bias correction</span></div><div class="line">    vb = v / (<span class="number">1</span> - beta2 ** t) <span class="comment"># bias correction</span></div><div class="line">    x += - learning_rate * mb / (np.sqrt(vb) + <span class="number">1e-7</span>) <span class="comment"># RMSProp-like</span></div></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>The bias correction compensates for the fact that m,v are initialized at zero and need<br>some time to “warm up”. Only relevant in first few iterations when t is small.</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="Learning-rate-decay"><a href="#Learning-rate-decay" class="headerlink" title="Learning rate decay"></a>Learning rate decay</h3><p>主要是为了让 learning rate 随着训练时间的推移慢慢变小, 防止系统动能太大, 到最后在最优点旁边跳来跳去.</p>
<ul>
<li><strong>step decay</strong>: e.g. decay learning rate by half every few epochs.</li>
<li><strong>exponential decay</strong>: $\alpha = \alpha_0 e^{-kt}$</li>
<li><strong>1/t decay</strong>: $\alpha = \alpha_0 / (1+kt)$</li>
</ul>
<h3 id="Second-order-optimization-methods"><a href="#Second-order-optimization-methods" class="headerlink" title="Second order optimization methods"></a>Second order optimization methods</h3><p>主要是一些基于牛顿法的二阶最优化方法, 包括 L-BGFS 之类的. 其优点是根本就没有 learning rate 这个超参数, 而缺点则是 Hessian 矩阵实在是太大了, 非常耗费时间与空间, 因此在 DL 和 CNN 中基本不使用.</p>
<h2 id="Evaluation-Model-Ensembles"><a href="#Evaluation-Model-Ensembles" class="headerlink" title="Evaluation: Model Ensembles"></a>Evaluation: Model Ensembles</h2><ul>
<li><p>训练多个独立的模型, 然后在测试的时候对其结果进行平均, 一般能得到 2% 的额外性能提升;</p>
</li>
<li><p>平均单个模型的多个记录点 (check point) 上的参数, 也能获得一些提升</p>
</li>
<li><p>训练的时候对参数进行平滑操作, 并用于测试集 (keep track of (and use at test time) a running average</p>
<p>parameter vector)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">While <span class="keyword">True</span>:</div><div class="line">    data_batch = dataset.sample_data_batch()</div><div class="line">    loss = network.forward(data_batch)</div><div class="line">    dx = network.backward()</div><div class="line">    x += - learning_rate * dx</div><div class="line">    x_test = <span class="number">0.995</span> * x_test + <span class="number">0.005</span> * x <span class="comment"># use for test set</span></div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Regularization-dropout"><a href="#Regularization-dropout" class="headerlink" title="Regularization (dropout)"></a>Regularization (dropout)</h2><p>Dropout 算是很常用的一种方法了, 主要就是在前向传播的时候随机设置某些神经元为零 (“randomly set some neurons to zero in the forward pass”).</p>
<p> <img src="/images/dropout.png" alt="dropout"></p>
<p>其主要想法是让网络具有一定的冗余能力 (Forces the network to have a<br>redundant representation), 或者说是训练出了一个大的集成网络 (Dropout is training a large ensemble of models (that share parameters), each binary mask is one model, gets trained on only ~one datapoint.)</p>
<p> <img src="/images/dropout_a_good_idea.png" alt="dropout_a_good_idea"></p>
<p>具体实现如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">p = <span class="number">0.5</span> <span class="comment"># probability of keeping a unit active. higher = less dropout</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span> </div><div class="line">    <span class="string">""" X contains the datat """</span></div><div class="line">    </div><div class="line">    <span class="comment"># forward pass for example 3-layer neural network</span></div><div class="line">    H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) </div><div class="line">    U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># First dropout mask. Notice /p! </span></div><div class="line">    H1 *= U1 <span class="comment"># drop! </span></div><div class="line">    H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2) </div><div class="line">    U2 = (np.random.rand(*H2.shape) &lt; p) / p <span class="comment"># Second dropout mask. Notice /p!  </span></div><div class="line">    H2 *= U2 <span class="comment"># drop! </span></div><div class="line">    out = np.dot(W3, H2) + b3 </div><div class="line">    </div><div class="line">    <span class="comment"># backward pass: compute geadients ... (not shown) </span></div><div class="line">    <span class="comment"># parameter update... (not shown) </span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="comment"># ensembled forward pass </span></div><div class="line">    H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) <span class="comment"># no scaling necessary </span></div><div class="line">    H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2) </div><div class="line">    out = np.dot(W3, H2) + b3</div></pre></td></tr></table></figure>
<h2 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h2><p>主要就是通过数值法计算梯度, 然后和通过后向传播得到的解析梯度比较, 看看误差大不大, 防止手贱算错梯度导致后面算法全乱了.</p>
<ol>
<li>用中心化公式$\frac{df(x)}{dx} = \frac{f(x+h - f(x-h)}{2h}$计算数值梯度, $h$取 $1e-5$ 左右.</li>
<li>使用相对误差$\frac{|f^{‘}_a - f^{‘}_n|}{max(|f^{‘}_a|, |f^{‘}_n|)}$</li>
</ol>
<p>同时还有些注意事项, 参见 <a href="http://cs231n.github.io/neural-networks-3/#gradcheck" target="_blank" rel="external">Gradient Checks</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://cs231n.github.io/neural-networks-1/&quot;&gt;Neural Networks Part 1: Setting up the Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cs231n.github.io/neural-networks-2/&quot;&gt;Neural Networks Part 2: Setting up the Data and the Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cs231n.github.io/neural-networks-3/&quot;&gt;Neural Networks Part 3: Learning and Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;或者可以看知乎专栏中的中文翻译:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit&quot;&gt;CS231n课程笔记翻译：神经网络笔记1（上）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit&quot;&gt;CS231n课程笔记翻译：神经网络笔记1（下）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit&quot;&gt;CS231n课程笔记翻译：神经网络笔记2 &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit&quot;&gt;CS231n课程笔记翻译：神经网络笔记3（上）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit&quot;&gt;CS231n课程笔记翻译：神经网络笔记3（下）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另外, 本文主要根据讲课的 Slides 上的顺序来, 与 Lecture Notes 的顺序略有不同.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="CS231n" scheme="http://www.yuthon.com/tags/CS231n/"/>
    
      <category term="Neural Network" scheme="http://www.yuthon.com/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>B-Human Code 浅析 - ScanGridProvider</title>
    <link href="http://www.yuthon.com/2016/10/08/B-Human-Code-Brief-Analysis-2/"/>
    <id>http://www.yuthon.com/2016/10/08/B-Human-Code-Brief-Analysis-2/</id>
    <published>2016-10-08T11:18:19.000Z</published>
    <updated>2016-10-09T08:36:32.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>本系列的文章主要是根据 B-Human 的开源代码库<code>CodeRelease2015</code>以及历年的 Team Description Paper 写成. 由于个人的 C++ 水平有限, 难免有缺漏之处, 望发现后不吝赐教.</p>
</blockquote>
<h1 id="B-Human-Code-浅析"><a href="#B-Human-Code-浅析" class="headerlink" title="B-Human Code 浅析"></a>B-Human Code 浅析</h1><h2 id="Perception"><a href="#Perception" class="headerlink" title="Perception"></a>Perception</h2><h3 id="ScanGrid"><a href="#ScanGrid" class="headerlink" title="ScanGrid"></a>ScanGrid</h3><p>首先来看一个基础的类<code>ScanGrid</code>, 里面定义了由扫描线组成的网格. 这个类在<code>ScanGridProvider</code>以及<code>LineScanner</code> 中都有用到. </p>
<a id="more"></a>
<p>头文件如下:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">STREAMABLE(ScanGrid,</div><div class="line">&#123;</div><div class="line">  STREAMABLE(Line,</div><div class="line">  &#123;</div><div class="line">    Line() = <span class="keyword">default</span>;</div><div class="line">    Line(<span class="keyword">int</span> x, <span class="keyword">int</span> y, <span class="keyword">unsigned</span> yMaxIndex),</div><div class="line"></div><div class="line">    (<span class="keyword">int</span>) x, <span class="comment">/**&lt; x coordinate of the scanline. */</span></div><div class="line">    (<span class="keyword">int</span>) yMax, <span class="comment">/**&lt; Maximum y coordinate (exclusive). */</span></div><div class="line">    (<span class="keyword">unsigned</span>) yMaxIndex, <span class="comment">/**&lt; Index of the lowest y coordinate relevant for this scanline. */</span></div><div class="line">  &#125;);</div><div class="line"></div><div class="line">  <span class="keyword">void</span> draw() <span class="keyword">const</span>,</div><div class="line"></div><div class="line">  (<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;) y, <span class="comment">/**&lt; All possible y coordinates of pixels to be scanned. */</span></div><div class="line">  (<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Line&gt;) lines, <span class="comment">/**&lt; Decription of all scanlines. */</span></div><div class="line">  (<span class="keyword">int</span>)(<span class="number">0</span>) fieldLimit, <span class="comment">/**&lt; Upper bound for all scanlines (exclusive). */</span></div><div class="line">  (<span class="keyword">unsigned</span>)(<span class="number">0</span>) lowResStart, <span class="comment">/**&lt; First index of low res grid. */</span></div><div class="line">  (<span class="keyword">unsigned</span>)(<span class="number">1</span>) lowResStep, <span class="comment">/**&lt; Steps between low res grid lines. */</span></div><div class="line">&#125;);</div><div class="line"></div><div class="line"><span class="keyword">inline</span> ScanGrid::Line::Line(<span class="keyword">int</span> x, <span class="keyword">int</span> yMax, <span class="keyword">unsigned</span> yMaxIndex) :</div><div class="line">  x(x), yMax(yMax), yMaxIndex(yMaxIndex)</div><div class="line">&#123;&#125;</div></pre></td></tr></table></figure>
<p>原本的注释已经很清楚了, 我就不再多说什么.</p>
<h3 id="ScanGridProvider"><a href="#ScanGridProvider" class="headerlink" title="ScanGridProvider"></a>ScanGridProvider</h3><p>这个模块主要是提供了一个由一堆竖线组成的扫描图像的网格. 生成的网格可供<code>LineScanner</code>使用. 头文件如下:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">MODULE(ScanGridProvider,</div><div class="line">&#123;,</div><div class="line">  REQUIRES(BodyContour),</div><div class="line">  REQUIRES(CameraInfo),</div><div class="line">  REQUIRES(CameraMatrix),</div><div class="line">  REQUIRES(FieldDimensions),</div><div class="line">  PROVIDES(ScanGrid),</div><div class="line">  DEFINES_PARAMETERS(</div><div class="line">  &#123;,</div><div class="line">    (<span class="keyword">int</span>)(<span class="number">3</span>) minStepSize, <span class="comment">/**&lt; The minimum pixel distance between two neigboring scanlines. */</span></div><div class="line">    (<span class="keyword">int</span>)(<span class="number">25</span>) minNumOfLowResScanlines, <span class="comment">/**&lt; The minimum number of scanlines for low resolution. */</span></div><div class="line">    (<span class="keyword">float</span>)(<span class="number">0.9f</span>) lineWidthRatio, <span class="comment">/**&lt; The ratio of field line width that is sampled when scanning the image. */</span></div><div class="line">    (<span class="keyword">float</span>)(<span class="number">0.8f</span>) ballWidthRatio, <span class="comment">/**&lt; The ratio of ball width that is sampled when scanning the image. */</span></div><div class="line">  &#125;),</div><div class="line">&#125;);</div><div class="line"></div><div class="line"><span class="keyword">class</span> ScanGridProvider : <span class="keyword">public</span> ScanGridProviderBase</div><div class="line">&#123;</div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">update</span><span class="params">(ScanGrid&amp; scanGrid)</span></span>;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>看看具体实现.</p>
<p>首先是一段初始化以及是否继续生成网格的判断:</p>
<ul>
<li>摄像头参数矩阵未给定</li>
<li>场地边界上的最远点不在视野内</li>
<li>场地边界高度超过图像高度(视野完全在场地之内?)</li>
<li>不能将图像左下与右下两点射影到场地上(视野完全在场地之外?)</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">void</span> ScanGridProvider::update(ScanGrid&amp; scanGrid)</div><div class="line">&#123;</div><div class="line">  <span class="comment">// 初始化网格信息</span></div><div class="line">  scanGrid.y.clear();</div><div class="line">  scanGrid.lines.clear();</div><div class="line"></div><div class="line">  <span class="keyword">if</span>(!theCameraMatrix.isValid)</div><div class="line">    <span class="keyword">return</span>; <span class="comment">// Cannot compute grid without camera matrix</span></div><div class="line"></div><div class="line">  <span class="comment">// Compute the furthest point away that could be part of the field given an unknown own position.</span></div><div class="line">  Vector2f pointInImage;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">float</span> fieldDiagional = Vector2f(theFieldDimensions.boundary.x.getSize(), theFieldDimensions.boundary.y.getSize()).norm();</div><div class="line">  <span class="keyword">if</span>(!Transformation::robotWithCameraRotationToImage(Vector2f(fieldDiagional, <span class="number">0</span>), theCameraMatrix, theCameraInfo, pointInImage))</div><div class="line">    <span class="keyword">return</span>; <span class="comment">// Cannot project furthest possible point to image -&gt; no grid in image</span></div><div class="line"></div><div class="line">  scanGrid.fieldLimit = <span class="built_in">std</span>::max(<span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(pointInImage.y()), <span class="number">-1</span>);</div><div class="line">  <span class="keyword">if</span>(scanGrid.fieldLimit &gt;= theCameraInfo.height)</div><div class="line">    <span class="keyword">return</span>; <span class="comment">// Image is above field limit -&gt; no grid in image</span></div><div class="line"></div><div class="line">  <span class="comment">// Determine the maximum distance between scanlines at the bottom of the image not to miss the ball.</span></div><div class="line">  Vector2f leftOnField;</div><div class="line">  Vector2f rightOnField;</div><div class="line">  <span class="keyword">if</span>(!Transformation::imageToRobotWithCameraRotation(Vector2i(<span class="number">0</span>, theCameraInfo.height - <span class="number">1</span>), theCameraMatrix, theCameraInfo, leftOnField) ||</div><div class="line">     !Transformation::imageToRobotWithCameraRotation(Vector2i(theCameraInfo.width, theCameraInfo.height - <span class="number">1</span>), theCameraMatrix, theCameraInfo, rightOnField))</div><div class="line">    <span class="keyword">return</span>; <span class="comment">// Cannot project lower image border to field -&gt; no grid</span></div></pre></td></tr></table></figure>
<p>接下来是要设置 x 方向的最大步长, 主要的考虑因素是扫描线的最小数量, 以及底部预计球的大小.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> xStepUpperBound = theCameraInfo.width / minNumOfLowResScanlines;</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxXStep = <span class="built_in">std</span>::min(xStepUpperBound,</div><div class="line">                              <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(theCameraInfo.width *</div><div class="line">                                               theFieldDimensions.ballRadius * <span class="number">2.f</span> * </div><div class="line">                                               ballWidthRatio / </div><div class="line">                                               (leftOnField - rightOnField).norm()));</div></pre></td></tr></table></figure>
<p>之后自下而上选取能够用来采样的 y 的值</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">Vector2f pointOnField = (leftOnField + rightOnField) / <span class="number">2.f</span>;</div><div class="line"></div><div class="line"><span class="comment">// Determine vertical sampling points of the grid</span></div><div class="line">scanGrid.y.reserve(theCameraInfo.height);</div><div class="line"><span class="keyword">const</span> <span class="keyword">float</span> fieldStep = theFieldDimensions.fieldLinesWidth * lineWidthRatio;</div><div class="line"><span class="keyword">bool</span> singleSteps = <span class="literal">false</span>;</div><div class="line"><span class="comment">// scanGrid.fieldLimit: Upper bound for all scanlines (exclusive).</span></div><div class="line"><span class="keyword">for</span>(<span class="keyword">int</span> y = theCameraInfo.height - <span class="number">1</span>; y &gt; scanGrid.fieldLimit;)</div><div class="line">&#123;</div><div class="line">  scanGrid.y.emplace_back(y);</div><div class="line">  <span class="comment">// Calc next vertical position for all scanlines.</span></div><div class="line">  <span class="keyword">if</span>(singleSteps)</div><div class="line">    --y;</div><div class="line">  <span class="keyword">else</span></div><div class="line">  &#123;</div><div class="line">    pointOnField.x() += fieldStep;</div><div class="line">    <span class="keyword">if</span>(!Transformation::robotWithCameraRotationToImage(pointOnField, theCameraMatrix, theCameraInfo, pointInImage))</div><div class="line">      <span class="keyword">break</span>;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> y2 = y;</div><div class="line">    y = <span class="built_in">std</span>::min(y2 - <span class="number">1</span>, <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(pointInImage.y() + <span class="number">0.5</span>));</div><div class="line">    singleSteps = y2 - <span class="number">1</span> == y;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>接下来是要设置 x 方向的最小步长, 主要的考虑因素是图像顶部预计球的大小.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Determine the maximum distance between scanlines at the top of the image not to miss the ball. Do not go below minStepSize.</span></div><div class="line"><span class="keyword">int</span> minXStep = minStepSize;</div><div class="line"><span class="keyword">if</span>(Transformation::imageToRobotWithCameraRotation(Vector2i(<span class="number">0</span>, <span class="number">0</span>), theCameraMatrix, theCameraInfo, leftOnField) &amp;&amp;</div><div class="line">   Transformation::imageToRobotWithCameraRotation(Vector2i(theCameraInfo.width, <span class="number">0</span>), theCameraMatrix, theCameraInfo, rightOnField))</div><div class="line">  minXStep = <span class="built_in">std</span>::max(minXStep, <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(theCameraInfo.width *</div><div class="line">                                                 theFieldDimensions.ballRadius *</div><div class="line">                                                 <span class="number">2.f</span> * ballWidthRatio / </div><div class="line">                                                 (leftOnField - rightOnField).norm()));</div><div class="line">minXStep = <span class="built_in">std</span>::min(xStepUpperBound, minXStep);</div></pre></td></tr></table></figure>
<p>然后是确认一个 x 方向的次大步长, 主要是满足$maxXStep2 = minXStep * 2^n, maxXStep2 &lt;= maxXStep$</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Determine a max step size that fulfills maxXStep2 = minXStep * 2^n, maxXStep2 &lt;= maxXStep.</span></div><div class="line"><span class="comment">// Also compute lower y coordinates for the different lengths of scanlines.</span></div><div class="line"><span class="keyword">int</span> maxXStep2 = minXStep;</div><div class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; yStarts;</div><div class="line"><span class="keyword">while</span>(maxXStep2 * <span class="number">2</span> &lt;= maxXStep)</div><div class="line">&#123;</div><div class="line">  <span class="keyword">float</span> distance = Geometry::getDistanceBySize(theCameraInfo, theFieldDimensions.ballRadius * ballWidthRatio, <span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(maxXStep2));</div><div class="line">  VERIFY(Transformation::robotWithCameraRotationToImage(Vector2f(distance, <span class="number">0</span>), theCameraMatrix, theCameraInfo, pointInImage));</div><div class="line">  yStarts.push_back(<span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(pointInImage.y() + <span class="number">0.5f</span>));</div><div class="line">  maxXStep2 *= <span class="number">2</span>;</div><div class="line">&#125;</div><div class="line">yStarts.push_back(theCameraInfo.height);</div></pre></td></tr></table></figure>
<p>根据上一步计算出的<code>maxXStep2</code>, 建立一个2为比值的等比数列作为扫描线的长度.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Determine a pattern with the different lengths of scan lines, in which the longest appears once,</span></div><div class="line"><span class="comment">// the second longest twice, etc. The pattern starts with the longest.</span></div><div class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; yStarts2(maxXStep2 / minXStep);</div><div class="line"><span class="keyword">for</span>(<span class="keyword">size_t</span> i = <span class="number">0</span>, step = <span class="number">1</span>; i &lt; yStarts.size(); ++i, step *= <span class="number">2</span>)</div><div class="line">  <span class="keyword">for</span>(<span class="keyword">size_t</span> j = <span class="number">0</span>; j &lt; yStarts2.size(); j += step)</div><div class="line">    yStarts2[j] = yStarts[i];</div></pre></td></tr></table></figure>
<p>初始化扫描线与颜色区域的信息</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Initialize the scan states and the regions.</span></div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> xStart = theCameraInfo.width % (theCameraInfo.width / minXStep - <span class="number">1</span>) / <span class="number">2</span>;</div><div class="line">scanGrid.lines.reserve((theCameraInfo.width - xStart) / minXStep);</div><div class="line"><span class="keyword">size_t</span> i = yStarts2.size() / <span class="number">2</span>; <span class="comment">// Start with the second longest scanline.</span></div><div class="line"><span class="keyword">for</span>(<span class="keyword">int</span> x = xStart; x &lt; theCameraInfo.width; x += minXStep)</div><div class="line">&#123;</div><div class="line">  <span class="keyword">int</span> yMax = <span class="built_in">std</span>::min(yStarts2[i++], theCameraInfo.height);</div><div class="line">  i %= yStarts2.size();</div><div class="line">  theBodyContour.clipBottom(x, yMax);</div><div class="line">  <span class="keyword">const</span> <span class="keyword">size_t</span> yMaxIndex = <span class="built_in">std</span>::upper_bound(scanGrid.y.begin(), scanGrid.y.end(), yMax + <span class="number">1</span>, <span class="built_in">std</span>::greater&lt;<span class="keyword">int</span>&gt;()) - scanGrid.y.begin();</div><div class="line">  scanGrid.lines.emplace_back(x, yMax, <span class="keyword">static_cast</span>&lt;<span class="keyword">unsigned</span>&gt;(yMaxIndex));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>设置低分辨率的扫描线的信息</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Set low resolution scanline info</span></div><div class="line">scanGrid.lowResStep = maxXStep2 / minXStep;</div><div class="line">scanGrid.lowResStart = scanGrid.lowResStep / <span class="number">2</span>;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本系列的文章主要是根据 B-Human 的开源代码库&lt;code&gt;CodeRelease2015&lt;/code&gt;以及历年的 Team Description Paper 写成. 由于个人的 C++ 水平有限, 难免有缺漏之处, 望发现后不吝赐教.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;B-Human-Code-浅析&quot;&gt;&lt;a href=&quot;#B-Human-Code-浅析&quot; class=&quot;headerlink&quot; title=&quot;B-Human Code 浅析&quot;&gt;&lt;/a&gt;B-Human Code 浅析&lt;/h1&gt;&lt;h2 id=&quot;Perception&quot;&gt;&lt;a href=&quot;#Perception&quot; class=&quot;headerlink&quot; title=&quot;Perception&quot;&gt;&lt;/a&gt;Perception&lt;/h2&gt;&lt;h3 id=&quot;ScanGrid&quot;&gt;&lt;a href=&quot;#ScanGrid&quot; class=&quot;headerlink&quot; title=&quot;ScanGrid&quot;&gt;&lt;/a&gt;ScanGrid&lt;/h3&gt;&lt;p&gt;首先来看一个基础的类&lt;code&gt;ScanGrid&lt;/code&gt;, 里面定义了由扫描线组成的网格. 这个类在&lt;code&gt;ScanGridProvider&lt;/code&gt;以及&lt;code&gt;LineScanner&lt;/code&gt; 中都有用到. &lt;/p&gt;
    
    </summary>
    
      <category term="Code Anaylsis" scheme="http://www.yuthon.com/categories/Code-Anaylsis/"/>
    
    
      <category term="RoboCup" scheme="http://www.yuthon.com/tags/RoboCup/"/>
    
      <category term="C++" scheme="http://www.yuthon.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>B-Human Code 浅析 - LineScanner</title>
    <link href="http://www.yuthon.com/2016/10/07/B-Human-Code-Brief-Analysis-1/"/>
    <id>http://www.yuthon.com/2016/10/07/B-Human-Code-Brief-Analysis-1/</id>
    <published>2016-10-07T13:58:08.000Z</published>
    <updated>2016-10-17T06:02:27.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>由于 ZJUDancer 组内需要重写视觉相关部分的代码, 因此最近开始看其他组视觉方面的实现. 而 B-Human 是标准组的老牌强队, 常年冠军, 很有借鉴价值.</p>
<p>本系列的文章主要是根据他们的开源代码库<code>CodeRelease2015</code>以及历年的 Team Description Paper 写成. 由于个人的 C++ 水平有限, 难免有缺漏之处, 望发现后不吝赐教.</p>
</blockquote>
<h1 id="B-Human-Code-浅析"><a href="#B-Human-Code-浅析" class="headerlink" title="B-Human Code 浅析"></a>B-Human Code 浅析</h1><h2 id="Perception"><a href="#Perception" class="headerlink" title="Perception"></a>Perception</h2><h3 id="LineScanner"><a href="#LineScanner" class="headerlink" title="LineScanner"></a>LineScanner</h3><blockquote>
<p>The file declares a class that creates colored regions on a single vertical scanline.</p>
<p>The class is based on Arne Böckmann’s original implementation of this idea.</p>
<p>@author Thomas Röfer</p>
</blockquote>
<p>这个类提供了以竖线的形式扫描图像, 并且找出线上具有相同颜色的区域的功能. 对于之后对场地边缘, 球, 机器人, 球门等的检测提供了基础.</p>
<a id="more"></a>
<p><img src="/images/vertical_scan.png" alt="vertical_scan.png"></p>
<p>在<code>LineScanner.h</code>中的类定义:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> LineScanner</div><div class="line">&#123;</div><div class="line"><span class="keyword">private</span>:</div><div class="line">  <span class="keyword">const</span> ColorTable&amp; colorTable;</div><div class="line">  <span class="keyword">const</span> Image&amp; image;</div><div class="line">  <span class="keyword">const</span> ScanGrid&amp; scanGrid;</div><div class="line"></div><div class="line"><span class="keyword">public</span>:</div><div class="line">  LineScanner(<span class="keyword">const</span> ColorTable&amp; colorTable,</div><div class="line">              <span class="keyword">const</span> Image&amp; image,</div><div class="line">              <span class="keyword">const</span> ScanGrid&amp; scanGrid)</div><div class="line">  : colorTable(colorTable),</div><div class="line">    image(image),</div><div class="line">    scanGrid(scanGrid) &#123;&#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Scan vertically and add a new scanline with colored regions.</div><div class="line">   * @param line Description of the line to be scanned.</div><div class="line">   * @param top Upper bound for the pixels scanned (exclusive).</div><div class="line">   * @param minColorRatio The ratio of pixels of a different color that is expected </div><div class="line">   *                      after an edge (relative to the step width).</div><div class="line">   * @param scanlineRegions A new scanline containing the regions found will be added</div><div class="line">   *                        to this object.</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">scan</span><span class="params">(<span class="keyword">const</span> ScanGrid::Line&amp; line, <span class="keyword">const</span> <span class="keyword">int</span> top, <span class="keyword">const</span> <span class="keyword">float</span> minColorRatio, ScanlineRegions&amp; scanlineRegions)</span> <span class="keyword">const</span></span>;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>主要看<code>scan</code> 这个成员函数. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">void</span> LineScanner::scan(<span class="keyword">const</span> ScanGrid::Line&amp; line, <span class="keyword">const</span> <span class="keyword">int</span> top, <span class="keyword">const</span> <span class="keyword">float</span> minColorRatio, ScanlineRegions&amp; scanlineRegions) <span class="keyword">const</span> &#123;...&#125;</div></pre></td></tr></table></figure>
<p>这个函数有四个输入参数, 分别是</p>
<ul>
<li>要扫描的线<code>line</code></li>
<li>扫描的上界<code>top</code></li>
<li>对于不同颜色的区分度的比值<code>minColorRatio</code>(与之后判断隔了几个不同颜色的点的两个区域是否要连在一起有关)</li>
<li>扫描后得到的区域<code>scanlineRegions</code></li>
</ul>
<p>然后来看看具体的实现. 首先是一些变量的指定:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">void</span> LineScanner::scan(<span class="keyword">const</span> ScanGrid::Line&amp; line, <span class="keyword">const</span> <span class="keyword">int</span> top, <span class="keyword">const</span> <span class="keyword">float</span> minColorRatio, ScanlineRegions&amp; scanlineRegions) <span class="keyword">const</span></div><div class="line">&#123;</div><div class="line">  <span class="comment">// 当前竖线的横坐标</span></div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> x = line.x;</div><div class="line">  <span class="comment">// std::deque::emplace_back Appends a new element to the end of the container.</span></div><div class="line">  <span class="comment">// 在区域 scanlineRegions 的 scanlines 向量容器的最后加一个用 x 初始化的元素. </span></div><div class="line">  scanlineRegions.scanlines.emplace_back(x); </div><div class="line">  <span class="comment">// std::array::back Returns reference to the last element in the container.</span></div><div class="line">  <span class="comment">// 前面新建的元素内的 regions 向量</span></div><div class="line">  <span class="keyword">auto</span>&amp; regions = scanlineRegions.scanlines.back().regions; </div><div class="line"></div><div class="line">  <span class="comment">// 竖线的起始纵坐标指针</span></div><div class="line">  <span class="comment">// ScanGridProvider.h 实现中</span></div><div class="line">  <span class="comment">// const size_t yMaxIndex = std::upper_bound(scanGrid.y.begin(), scanGrid.y.end(), yMax + 1, std::greater&lt;int&gt;()) - scanGrid.y.begin();</span></div><div class="line">  <span class="keyword">auto</span> y = scanGrid.y.begin() + line.yMaxIndex;</div><div class="line">  <span class="comment">// 竖线的终止纵坐标指针</span></div><div class="line">  <span class="keyword">const</span> <span class="keyword">auto</span> yEnd = scanGrid.y.end();</div><div class="line">  <span class="comment">// 扫描步长</span></div><div class="line">  <span class="keyword">const</span> <span class="keyword">int</span> widthStep = image.widthStep;</div><div class="line">  </div><div class="line">  <span class="keyword">if</span>(y != yEnd &amp;&amp; *y &gt; top &amp;&amp; line.yMax - <span class="number">1</span> &gt; top) &#123;...&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>接着是一个条件判断, 主要是看这条竖线能不能用来扫描:</p>
<ul>
<li>起始纵坐标指针<code>y</code> 是否已经与终止纵坐标指针<code>yEnd</code>重合</li>
<li>起始纵坐标<code>*y</code>是否高于场地上界<code>top</code></li>
<li>竖线纵坐标最大值<code>yMax</code> 是否高于场地上界<code>top</code></li>
</ul>
<p>在这个<code>if</code>语句里面, 我们开始进行对于竖线的颜色扫描与区域生成的工作.</p>
<p>首先还是一些变量的指定, 以及操作完成之后将检测到的:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span>(y != yEnd &amp;&amp; *y &gt; top &amp;&amp; line.yMax - <span class="number">1</span> &gt; top)</div><div class="line">&#123;</div><div class="line">  <span class="comment">// 前一个点的纵坐标</span></div><div class="line">  <span class="comment">// ScanGridProvider.h 实现中 int yMax = std::min(yStarts2[i++], theCameraInfo.height);</span></div><div class="line">  <span class="keyword">int</span> prevY = line.yMax - <span class="number">1</span> &gt; *y ? line.yMax - <span class="number">1</span> : *y++;</div><div class="line">  <span class="comment">// 当前点的纵坐标</span></div><div class="line">  <span class="keyword">int</span> currentY = prevY + <span class="number">1</span>;</div><div class="line">  <span class="comment">// 前一个点的像素信息</span></div><div class="line">  <span class="keyword">const</span> Image::Pixel* pImg = &amp;image[prevY][x];</div><div class="line">  <span class="comment">// 前一个点的颜色分类</span></div><div class="line">  ColorTable::Colors currentColor = colorTable[*pImg];</div><div class="line">  </div><div class="line">  <span class="keyword">for</span>(; y != yEnd &amp;&amp; *y &gt; top; ++y) &#123;...&#125;</div><div class="line">  </div><div class="line">  ASSERT(currentY &gt; top + <span class="number">1</span>);</div><div class="line">  regions.emplace_back(currentY, top + <span class="number">1</span>, currentColor);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>中间是一个<code>for</code> 循环, 用来遍历竖线上的所有点.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span>(; y != yEnd &amp;&amp; *y &gt; top; ++y)</div><div class="line">&#123;</div><div class="line">  <span class="comment">// 下一个点的像素信息</span></div><div class="line">  pImg += (*y - prevY) * widthStep;</div><div class="line"></div><div class="line">  <span class="comment">// 下一点的颜色分类</span></div><div class="line">  <span class="keyword">const</span> ColorTable::Colors&amp; color = colorTable[*pImg];</div><div class="line">  <span class="comment">// If color changes, determine edge position between last and current scanpoint</span></div><div class="line">  <span class="keyword">if</span>(color.colors != currentColor.colors)</div><div class="line">  &#123;</div><div class="line">    <span class="comment">// 根据minColorRatio设定不同颜色区域相隔的像素数量阈值</span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> otherColorThreshold = <span class="built_in">std</span>::max(<span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;((prevY - *y) * minColorRatio), <span class="number">1</span>);</div><div class="line">    <span class="comment">// 起始纵坐标向上移阈值个点的纵坐标</span></div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> yMin = <span class="built_in">std</span>::max(*y - otherColorThreshold + <span class="number">1</span>, <span class="number">0</span>);</div><div class="line">    <span class="comment">// 不同颜色的像素数量计数器</span></div><div class="line">    <span class="keyword">int</span> counter = <span class="number">0</span>;</div><div class="line">    <span class="comment">// 前两个点的纵坐标</span></div><div class="line">    <span class="keyword">int</span> yy = <span class="built_in">std</span>::min(prevY - <span class="number">1</span>, line.yMax - <span class="number">1</span>);</div><div class="line">    <span class="comment">// 遍历从 yy 到 yMin , 计数不同颜色像素的个数</span></div><div class="line">    <span class="keyword">for</span>(<span class="keyword">const</span> Image::Pixel* pImg2 = pImg + (yy - *y) * widthStep; yy &gt;= yMin &amp;&amp; counter &lt; otherColorThreshold; --yy, pImg2 -= image.widthStep)</div><div class="line">      <span class="keyword">if</span>(colorTable[*pImg2].colors != currentColor.colors)</div><div class="line">        ++counter;</div><div class="line">      <span class="keyword">else</span></div><div class="line">        counter = <span class="number">0</span>;</div><div class="line"></div><div class="line">    <span class="comment">// Enough pixels of different colors were found: end previous region and start a new one.</span></div><div class="line">    <span class="comment">// 如果发现了足够数量的不同颜色像素, 那么就结束原来的区域, 并开始记录一个新的区域</span></div><div class="line">    <span class="keyword">if</span>(counter == otherColorThreshold)</div><div class="line">    &#123;</div><div class="line">      yy += otherColorThreshold + <span class="number">1</span>;</div><div class="line">      ASSERT(currentY &gt; yy);</div><div class="line">      regions.emplace_back(currentY, yy, currentColor);</div><div class="line">      currentColor = color;</div><div class="line">      currentY = yy;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  prevY = *y;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>(本文中略有缺陷, 待分析了<code>ScanGridProvider</code>之后再做补充)</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;由于 ZJUDancer 组内需要重写视觉相关部分的代码, 因此最近开始看其他组视觉方面的实现. 而 B-Human 是标准组的老牌强队, 常年冠军, 很有借鉴价值.&lt;/p&gt;
&lt;p&gt;本系列的文章主要是根据他们的开源代码库&lt;code&gt;CodeRelease2015&lt;/code&gt;以及历年的 Team Description Paper 写成. 由于个人的 C++ 水平有限, 难免有缺漏之处, 望发现后不吝赐教.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;B-Human-Code-浅析&quot;&gt;&lt;a href=&quot;#B-Human-Code-浅析&quot; class=&quot;headerlink&quot; title=&quot;B-Human Code 浅析&quot;&gt;&lt;/a&gt;B-Human Code 浅析&lt;/h1&gt;&lt;h2 id=&quot;Perception&quot;&gt;&lt;a href=&quot;#Perception&quot; class=&quot;headerlink&quot; title=&quot;Perception&quot;&gt;&lt;/a&gt;Perception&lt;/h2&gt;&lt;h3 id=&quot;LineScanner&quot;&gt;&lt;a href=&quot;#LineScanner&quot; class=&quot;headerlink&quot; title=&quot;LineScanner&quot;&gt;&lt;/a&gt;LineScanner&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;The file declares a class that creates colored regions on a single vertical scanline.&lt;/p&gt;
&lt;p&gt;The class is based on Arne Böckmann’s original implementation of this idea.&lt;/p&gt;
&lt;p&gt;@author Thomas Röfer&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这个类提供了以竖线的形式扫描图像, 并且找出线上具有相同颜色的区域的功能. 对于之后对场地边缘, 球, 机器人, 球门等的检测提供了基础.&lt;/p&gt;
    
    </summary>
    
      <category term="Code Anaylsis" scheme="http://www.yuthon.com/categories/Code-Anaylsis/"/>
    
    
      <category term="RoboCup" scheme="http://www.yuthon.com/tags/RoboCup/"/>
    
      <category term="C++" scheme="http://www.yuthon.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Notes for Algorithm Design and Analysis - Week 1</title>
    <link href="http://www.yuthon.com/2016/10/06/Coursera-Algorithm-Design-Analysis-Week-1/"/>
    <id>http://www.yuthon.com/2016/10/06/Coursera-Algorithm-Design-Analysis-Week-1/</id>
    <published>2016-10-06T06:27:03.000Z</published>
    <updated>2016-10-07T09:34:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Why-study-Algorithm"><a href="#Why-study-Algorithm" class="headerlink" title="Why study Algorithm?"></a>Why study Algorithm?</h2><ul>
<li>important for all other branches of computer science</li>
<li>plays a key role in modern technological innovation</li>
<li>provides novel “lens” on processes outside of computer science and technology</li>
<li>challenging (i.e., good for the brain!)</li>
<li>fun</li>
</ul>
<a id="more"></a>
<h2 id="Integer-Multiplication"><a href="#Integer-Multiplication" class="headerlink" title="Integer Multiplication"></a>Integer Multiplication</h2><ul>
<li>Input: 2 n-digit numbers $x$ and $y$</li>
<li>Output: product $x*y$</li>
</ul>
<h3 id="The-Grade-School-Algorithm"><a href="#The-Grade-School-Algorithm" class="headerlink" title="The Grade School Algorithm"></a>The Grade School Algorithm</h3><p><img src="/images/grade-school-algorithm.png" alt="The Grade School Algorithm"></p>
<ul>
<li>There’re roughly $n$ operations per row, and we have $n$ rows. </li>
<li>So the number of operations overall is about $n^2$.</li>
</ul>
<h3 id="The-Algorithm-Designer’s-Mantra"><a href="#The-Algorithm-Designer’s-Mantra" class="headerlink" title="The Algorithm Designer’s Mantra"></a>The Algorithm Designer’s Mantra</h3><blockquote>
<p>“Perhaps the most important principle for the good algorithm designer is to refuse to be content.”</p>
<p>— Aho, Hopcroft, and Ullman, <em>The Design and Analysis of Computer Algorithms</em>, 1974</p>
</blockquote>
<p><strong>Can we do better</strong> (than the “obvious” method)?</p>
<h2 id="Karatsuba-Multiplication"><a href="#Karatsuba-Multiplication" class="headerlink" title="Karatsuba Multiplication"></a>Karatsuba Multiplication</h2><h3 id="A-Recursive-Algorithm"><a href="#A-Recursive-Algorithm" class="headerlink" title="A Recursive Algorithm"></a>A Recursive Algorithm</h3><ul>
<li>Write $x=10^{n/2}a+b$ and $y=10^{n/2}c+d$<ul>
<li>Where $a,b,c,d$ are $n/2$-digit numbers</li>
</ul>
</li>
<li>Then $x\cdot y = (10^{n/2}a+b)(10^{n/2}c+d) = 10^n ac + 10^{n/2} (ad+bc) +bd \ \ \ \ (*)$</li>
</ul>
<p><strong>Idea</strong>: recurseively compute $ac, ad, bc, bd$, then compute $(*)$ in the obvious way. </p>
<blockquote>
<p>Of course, simple base case should be omitted.</p>
</blockquote>
<h3 id="Karatsuba-Multiplication-1"><a href="#Karatsuba-Multiplication-1" class="headerlink" title="Karatsuba Multiplication"></a>Karatsuba Multiplication</h3><p>$x\cdot y = 10^n ac + 10^{n/2} (ad+bc) +bd$</p>
<ul>
<li>Recursively compute $ac$</li>
<li>Recursively compute $bd$</li>
<li>Recursively compute $(a+b)(c+d) = ac+bd+ad+bc$<ul>
<li>Gauss’ Trick: $(3)-(1)-(2)=ad+bc$</li>
</ul>
</li>
</ul>
<p><strong>Upshot</strong>: only need 3 recursive multiplications (and some additions)</p>
<h2 id="About-The-Course"><a href="#About-The-Course" class="headerlink" title="About The Course"></a>About The Course</h2><h3 id="Course-Topics"><a href="#Course-Topics" class="headerlink" title="Course Topics"></a>Course Topics</h3><ul>
<li>Vocabulary for design and analysisof algorithms<ul>
<li>E.g., “Big Oh” notation</li>
<li>“sweet spot” for highGlevel reasoning about algorithms</li>
</ul>
</li>
<li>Divide and conquer algorithm design paradigm<ul>
<li>Will apply to: Integer multiplication, sorting, matrix multiplication, closest pair</li>
<li>General analysis methods (“Master Method/Theorem”)</li>
</ul>
</li>
<li>Randomization in algorithm design<ul>
<li>Will apply to: QuickSort, primality testing, graph partitioning, hashing.</li>
</ul>
</li>
<li>Primitives for reasoning about graphs<ul>
<li>Connectivity information, shortest paths, structure of information and social networks.</li>
</ul>
</li>
<li>Use and implementation of data structures<ul>
<li>Heaps, balanced binary search trees, hashing and some variants (e.g., bloom ﬁlters)</li>
</ul>
</li>
</ul>
<h3 id="Topics-in-Sequel-Course"><a href="#Topics-in-Sequel-Course" class="headerlink" title="Topics in Sequel Course"></a>Topics in Sequel Course</h3><ul>
<li>Greedy algorithm design paradigm</li>
<li>Dynamic programming algorithm design paradigm</li>
<li>NP-complete problems and what to do about them</li>
<li>Fast heuristics with provable guarantees</li>
<li>Fast exact algorithms for special cases</li>
<li>Exact algorithms that beat brute-force search</li>
</ul>
<h3 id="Skills-You’ll-Lean"><a href="#Skills-You’ll-Lean" class="headerlink" title="Skills You’ll Lean"></a>Skills You’ll Lean</h3><ul>
<li>Become a better programmer</li>
<li>Sharpen your mathematical and analytical skills</li>
<li>Start “thinking algorithmically”</li>
<li>Literacy with computer science’s “greatest hits”</li>
<li>Ace your technical interviews</li>
</ul>
<h3 id="Who-Are-You"><a href="#Who-Are-You" class="headerlink" title="Who Are You?"></a>Who Are You?</h3><ul>
<li>It doesn’t really matter.    (It’s a free course, after all.)</li>
<li>Ideally, you know some programming.</li>
<li>Doesn’t matter which language(s) you know.<ul>
<li>But you should be capable of translating high-level algorithm descriptions into working programs in some programming language.</li>
</ul>
</li>
<li>Some (perhaps rusty) mathematical experience.<ul>
<li>Basic discrete math, proofs by induction, etc.</li>
</ul>
</li>
</ul>
<h3 id="Supporting-Materials"><a href="#Supporting-Materials" class="headerlink" title="Supporting Materials"></a>Supporting Materials</h3><ul>
<li>All (annotated) slides available from course site.</li>
<li>No required textbook.A few of the many good ones:<ul>
<li>Kleinberg/Tardos, <em>Algorithm Design</em>, 2005.</li>
<li>Dasgupta/Papadimitriou/Vazirani, <em>Algorithms</em>, 2006.</li>
<li>Cormen/Leiserson/Rivest/Stein, <em>Introduction to Algorithms</em>, 2009 (3rd edition).</li>
<li>Mehlhorn/Sanders, <em>Data Structures and Algorithms: The Basic Toolbox</em>, 2008.</li>
</ul>
</li>
<li>No speciﬁc development environment required.<ul>
<li>But you should be able to write and execute programs.</li>
</ul>
</li>
</ul>
<h3 id="Assessment"><a href="#Assessment" class="headerlink" title="Assessment"></a>Assessment</h3><ul>
<li>No grades per se. (Details on a certiﬁcate of  accomplishment TBA.)</li>
<li>Weekly homeworks.<ul>
<li>Test understand of material</li>
<li>Synchronize students, greatly helps discussion forum</li>
<li>Intellectual challenge</li>
</ul>
</li>
<li>Assessment tools currently just a “1.0” technology.<ul>
<li>We’ll do our best!</li>
</ul>
</li>
<li>Will sometimes propose harder “challenge problems”<ul>
<li>Will not be graded; discuss solutions via course forum</li>
</ul>
</li>
</ul>
<h2 id="Merge-Sort-Motivation-and-Example"><a href="#Merge-Sort-Motivation-and-Example" class="headerlink" title="Merge Sort: Motivation and Example"></a>Merge Sort: Motivation and Example</h2><h3 id="Why-Study-Merge-Sort"><a href="#Why-Study-Merge-Sort" class="headerlink" title="Why Study Merge Sort?"></a>Why Study Merge Sort?</h3><ul>
<li>Good introduction to divide &amp; conquer<ul>
<li>Improves over Selection, Insertion, Bubble sorts</li>
</ul>
</li>
<li>Calibrate your preparation</li>
<li>Motivates guiding principles for algorithm  analysis (worst-case and asymptotic analysis)</li>
<li>Analysis generalizes to “Master Method”</li>
</ul>
<h3 id="The-Sorting-Problem"><a href="#The-Sorting-Problem" class="headerlink" title="The Sorting Problem"></a>The Sorting Problem</h3><ul>
<li><strong>Input</strong>: array of numbers, unsorted</li>
<li><strong>Output</strong>: Same numbers, sorted in increasing or decreasing order.</li>
</ul>
<h3 id="Merge-Sort-Example"><a href="#Merge-Sort-Example" class="headerlink" title="Merge Sort: Example"></a>Merge Sort: Example</h3><p><img src="/images/merge-sort-example.png" alt="merge-sort-example.png"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;h2 id=&quot;Why-study-Algorithm&quot;&gt;&lt;a href=&quot;#Why-study-Algorithm&quot; class=&quot;headerlink&quot; title=&quot;Why study Algorithm?&quot;&gt;&lt;/a&gt;Why study Algorithm?&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;important for all other branches of computer science&lt;/li&gt;
&lt;li&gt;plays a key role in modern technological innovation&lt;/li&gt;
&lt;li&gt;provides novel “lens” on processes outside of computer science and technology&lt;/li&gt;
&lt;li&gt;challenging (i.e., good for the brain!)&lt;/li&gt;
&lt;li&gt;fun&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Coursera" scheme="http://www.yuthon.com/tags/Coursera/"/>
    
      <category term="Algorithm" scheme="http://www.yuthon.com/tags/Algorithm/"/>
    
      <category term="Data Structure" scheme="http://www.yuthon.com/tags/Data-Structure/"/>
    
  </entry>
  
  <entry>
    <title>Redmine configuration on Ubuntu 14.04</title>
    <link href="http://www.yuthon.com/2016/09/15/Redmine-Configuration-on-Ubuntu-14-04/"/>
    <id>http://www.yuthon.com/2016/09/15/Redmine-Configuration-on-Ubuntu-14-04/</id>
    <published>2016-09-15T11:02:37.000Z</published>
    <updated>2016-09-18T03:40:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近实验室要搞团队协作与项目管理，所以梅老板派我去装个 Redmine。</p>
<p>本文参考了 <a href="[http://www.redmine.org/projects/redmine/wiki/HowTo_Install_Redmine_on_Ubuntu_step_by_step](http://www.redmine.org/projects/redmine/wiki/HowTo_Install_Redmine_on_Ubuntu_step_by_step">HowTo Install Redmine on Ubuntu step by step</a>) 这篇官网的文章，并且根据实际情况有所改动。</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>首先自然是安装 Redmine 以及相关依赖：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install apache2 libapache2-mod-passenger</div><div class="line">$ sudo apt-get install mysql-server mysql-client</div><div class="line">$ sudo apt-get install redmine redmine-mysql</div></pre></td></tr></table></figure>
<p>只得注意的是，安装 MySQL 的时候会要求设置数据库<code>root</code>用户的密码，这个密码在之后安装 Redmine 的时候需要。</p>
<a id="more"></a>
<p>同时注意安装<code>bundler</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo gem update</div><div class="line">$ sudo gem install bundler</div></pre></td></tr></table></figure>
<p>这时候 Redmine 应该已经可用了，可以到<code>/usr/share/redmine</code>下直接用 WEBrick 来测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ sudo bundle exec ruby script/rails server webrick -e production</div><div class="line">=&gt; Booting WEBrick</div><div class="line">=&gt; Rails 3.2.16 application starting in production on http://0.0.0.0:3000</div><div class="line">=&gt; Call with -d to detach</div><div class="line">=&gt; Ctrl-C to shutdown server</div><div class="line">[2016-09-15 00:18:34] INFO  WEBrick 1.3.1</div><div class="line">[2016-09-15 00:18:34] INFO  ruby 1.9.3 (2013-11-22) [x86_64-linux]</div><div class="line">[2016-09-15 00:18:34] INFO  WEBrick::HTTPServer#start: pid=12337 port=3000</div></pre></td></tr></table></figure>
<p>能用的话，我们接下来来配置 Apache。</p>
<h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><ol>
<li><p>首先打开<code>/etc/apache2/mods-available/passenger.conf</code>，加一行<code>PassengerDefaultUser www-data</code>。之后整个文件看起来是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;IfModule mod_passenger.c&gt;</div><div class="line">  PassengerDefaultUser www-data</div><div class="line">  PassengerRoot /usr/lib/ruby/vendor_ruby/phusion_passenger/locations.ini</div><div class="line">  PassengerDefaultRuby /usr/bin/ruby</div><div class="line">&lt;/IfModule&gt;</div></pre></td></tr></table></figure>
</li>
<li><p>然后创建软链接，把 Redmine 的文件目录和 Apache 的根目录连起来</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo ln -s /usr/share/redmine/public /var/www/html/redmine</div></pre></td></tr></table></figure>
</li>
<li><p>接下来编辑<code>/etc/apache2/sites-available/000-default.conf</code>，把以下内容插在<code>&lt;VirtualHost&gt;...&lt;/VirtualHost&gt;</code>之间。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&lt;Directory /var/www/html/redmine&gt;</div><div class="line">    RailsBaseURI /redmine</div><div class="line">    PassengerResolveSymlinksInDocumentRoot on</div><div class="line">&lt;/Directory&gt;</div></pre></td></tr></table></figure>
<p>由于我手上的这台服务器的 80 端口被 Gitlab 占着，所以还需要换个端口，比如1234（不要忘了同时修改<code>/etc/apache2/ports.conf</code>中的监听端口号）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;VirtualHost *:1234&gt;</div></pre></td></tr></table></figure>
<p>同时，还可以设置服务器的地址：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ServerName localhost</div></pre></td></tr></table></figure>
<p>最后改完的<code>/etc/apache2/sites-available/000-default.conf</code>看起来如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&lt;VirtualHost *:1234&gt;</div><div class="line">        ServerAdmin webmaster@localhost</div><div class="line">        DocumentRoot /var/www</div><div class="line">        ServerName localhost</div><div class="line">        </div><div class="line">        ErrorLog $&#123;APACHE_LOG_DIR&#125;/error.log</div><div class="line">        CustomLog $&#123;APACHE_LOG_DIR&#125;/access.log combined</div><div class="line">        </div><div class="line">        &lt;Directory /var/www/html/redmine&gt;</div><div class="line">                RailsBaseURI /redmine</div><div class="line">                PassengerResolveSymlinksInDocumentRoot on</div><div class="line">        &lt;/Directory&gt;</div><div class="line">&lt;/VirtualHost&gt;</div></pre></td></tr></table></figure>
</li>
<li><p>创建并修改Gemfile.lock的权限：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo touch /usr/share/redmine/Gemfile.lock</div><div class="line">$ sudo chown www-data:www-data /usr/share/redmine/Gemfile.lock</div></pre></td></tr></table></figure>
</li>
<li><p>修改<code>/etc/apache2/apache2.conf</code>，添加一行设置 Passenger 的根目录。不然只能访问到 Redmine 下的文件目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">PassengerAppRoot /usr/share/redmine</div></pre></td></tr></table></figure>
</li>
<li><p>重启 Apache：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo service apache2 restart</div></pre></td></tr></table></figure>
</li>
<li><p>此时已经可以直接通过浏览器访问 <a href="http://127.0.0.1:1234" target="_blank" rel="external">http://127.0.0.1:1234</a> 了。</p>
</li>
</ol>
<p>后续配置持续更新中</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近实验室要搞团队协作与项目管理，所以梅老板派我去装个 Redmine。&lt;/p&gt;
&lt;p&gt;本文参考了 &lt;a href=&quot;[http://www.redmine.org/projects/redmine/wiki/HowTo_Install_Redmine_on_Ubuntu_step_by_step](http://www.redmine.org/projects/redmine/wiki/HowTo_Install_Redmine_on_Ubuntu_step_by_step&quot;&gt;HowTo Install Redmine on Ubuntu step by step&lt;/a&gt;) 这篇官网的文章，并且根据实际情况有所改动。&lt;/p&gt;
&lt;h1 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h1&gt;&lt;p&gt;首先自然是安装 Redmine 以及相关依赖：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;$ sudo apt-get install apache2 libapache2-mod-passenger&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;$ sudo apt-get install mysql-server mysql-client&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;$ sudo apt-get install redmine redmine-mysql&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;只得注意的是，安装 MySQL 的时候会要求设置数据库&lt;code&gt;root&lt;/code&gt;用户的密码，这个密码在之后安装 Redmine 的时候需要。&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Redmine" scheme="http://www.yuthon.com/tags/Redmine/"/>
    
      <category term="Apache2" scheme="http://www.yuthon.com/tags/Apache2/"/>
    
      <category term="MySQL" scheme="http://www.yuthon.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>Notes for Machine Learning - Week 6</title>
    <link href="http://www.yuthon.com/2016/09/10/Coursera-Machine-Learning-Week-6/"/>
    <id>http://www.yuthon.com/2016/09/10/Coursera-Machine-Learning-Week-6/</id>
    <published>2016-09-10T07:18:41.000Z</published>
    <updated>2016-09-21T14:09:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Advice-for-Applying-Machine-Learning"><a href="#Advice-for-Applying-Machine-Learning" class="headerlink" title="Advice for Applying Machine Learning"></a>Advice for Applying Machine Learning</h1><h2 id="Evaluating-a-Learning-Algorithm"><a href="#Evaluating-a-Learning-Algorithm" class="headerlink" title="Evaluating a Learning Algorithm"></a>Evaluating a Learning Algorithm</h2><h3 id="Deciding-What-to-Try-Next"><a href="#Deciding-What-to-Try-Next" class="headerlink" title="Deciding What to Try Next"></a>Deciding What to Try Next</h3><p>Errors in your predictions can be troubleshooted by:</p>
<ul>
<li>Getting more training examples</li>
<li>Trying smaller sets of features</li>
<li>Trying additional features</li>
<li>Trying adding polynomial features</li>
<li>Increasing or decreasing $\lambda$</li>
</ul>
<p>Don’t just pick one of these avenues at random. We’ll explore diagnostic techniques for choosing one of the above solutions in the following sections.</p>
<p>In the next few sections, We’ll first talk about how evaluate your learning algorithms and after that we’ll talk about some of these diagnostics which will hopefully let you much more effectively select more of the useful things to try mixing if your goal to improve the machine learning system. </p>
<a id="more"></a>
<h3 id="Evaluating-a-Hypothesis"><a href="#Evaluating-a-Hypothesis" class="headerlink" title="Evaluating a Hypothesis"></a>Evaluating a Hypothesis</h3><p>A hypothesis may have low error for the training examples but still be inaccurate (because of overfitting). And it may fail to generalize to new examples not in training set.</p>
<p>With a given dataset of training examples, we can split up the data into two sets: a <strong>training set</strong> and a <strong>test set</strong>. (normally 70% for training set and 30% for test set)</p>
<p>The training/testing procedure using these two sets is then:</p>
<ol>
<li><p>Learn $\Theta$ and minimize $J_{train}(\Theta)$ using the training set</p>
</li>
<li><p>Compute the test set error $J_{test}(\Theta)$</p>
<ul>
<li><p>For linear regression: $J_{test}(\Theta) = \dfrac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_\Theta(x^{(i)}_{test}) - y^{(i)}_{test})^2$</p>
</li>
<li><p>For classification ~ Misclassification error (aka 0/1 misclassification error): </p>
<p>$err(h_\Theta(x),y) = \begin{cases} 1 &amp; \mbox{if } h_\Theta(x) \geq 0.5\ and\ y = 0\ or\ h_\Theta(x) &lt; 0.5\ and\ y = 1 \\ 0 &amp; otherwise \end{cases}$</p>
<ul>
<li>This gives us a binary 0 or 1 error result based on a misclassification.</li>
</ul>
</li>
<li><p>The average test error for the test set is</p>
<p>$\text{Test Error} = \dfrac{1}{m_{test}} \sum^{m_{test}}_{i=1} err(h_\Theta(x^{(i)}_{test}), y^{(i)}_{test})$</p>
<ul>
<li>This gives us the proportion of the test data that was misclassified.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="Model-Selection-and-Train-Validation-Test-Sets"><a href="#Model-Selection-and-Train-Validation-Test-Sets" class="headerlink" title="Model Selection and Train/Validation/Test Sets"></a>Model Selection and Train/Validation/Test Sets</h3><p>Once parameters $\theta _0, \theta _1, \dots , \theta _4$ were fit to some set of data (training set), the error of the parameters as measured on that data (the training error $J(\theta)$ ) is likely to be lower than the actual generalization error.</p>
<ul>
<li>Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis.</li>
<li>The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than any other data set.</li>
</ul>
<p>In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result.</p>
<p><strong>Without the Validation Set (bad method)</strong></p>
<ol>
<li>Optimize the parameters in $\Theta$ using the training set for each polynomial degree.</li>
<li>Find the polynomial degree d with the least error using the test set.</li>
<li>Estimate the generalization error also using the test set with $J_{test}(\Theta^{(d)})$, (d = theta from polynomial with lower error);</li>
</ol>
<p>In this case, we have trained one variable, d, or the degree of the polynomial, using the test set. I.e., our extra parameter is fit to the test set. This will cause our error value to be greater for any other set of data. <u>Then the performance of the fitted model on the training set is not predictive of how well the hypothesis will generalize to new examples.</u></p>
<p><strong>Use of the CV set</strong></p>
<p>To solve this, we can introduce a third set, the <strong>Cross Validation Set</strong> (交叉验证集), to serve as an intermediate set that we can train d with. Then our test set will give us an accurate, non-optimistic error.</p>
<p>One example way to break down our dataset into the three sets is:</p>
<ul>
<li>Training set: 60%</li>
<li>Cross validation set: 20%</li>
<li>Test set: 20%</li>
</ul>
<p>We can now calculate three separate error values for the three different sets.</p>
<p><strong>With the Validation Set (note: this method presumes we do not also use the CV set for regularization)</strong></p>
<ol>
<li>Optimize the parameters in $\Theta$ using the training set for each polynomial degree.</li>
<li>Find the polynomial degree d with the least error using the cross validation set.</li>
<li>Estimate the generalization error using the test set with $J_{test}(\Theta^{(d)})$, (d = theta from polynomial with lower error);</li>
</ol>
<p>This way, <u>the degree of the polynomial d has not been trained using the test set.</u></p>
<blockquote>
<p>Be aware that using the CV set to select ‘d’ means that we cannot also use it for the validation curve process of setting the lambda value.</p>
</blockquote>
<h2 id="Bias-vs-Variance"><a href="#Bias-vs-Variance" class="headerlink" title="Bias vs. Variance"></a>Bias vs. Variance</h2><h3 id="Diagnosing-Bias-vs-Variance"><a href="#Diagnosing-Bias-vs-Variance" class="headerlink" title="Diagnosing Bias vs. Variance"></a>Diagnosing Bias vs. Variance</h3><p>We’ll examine the relationship between the degree of the polynomial $d$ and the underfitting or overfitting of our hypothesis.</p>
<ul>
<li>We need to distinguish whether <strong>bias</strong> (偏差) or <strong>variance</strong> (方差) is the problem contributing to bad predictions.</li>
<li><u>High bias is underfitting and high variance is overfitting.</u> We need to find a golden mean between these two.</li>
</ul>
<p>The training error will tend to <strong>decrease</strong> as we increase the degree d of the polynomial.</p>
<p>At the same time, the cross validation error will tend to <strong>decrease</strong> as we increase d up to a point, and then it will <strong>increase</strong> as d is increased, forming a convex curve.</p>
<ul>
<li><strong>High bias (underfitting)</strong>: both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ will be high, and $J_{CV}(\Theta) \approx J_{train}(\Theta)$.</li>
<li><strong>High variance (overfitting)</strong>: $J_{train}(\Theta)$ will be low but $J_{CV}(\Theta)$ will be high. And $J_{CV}(\Theta) \gg J_{train}(\Theta)$.</li>
</ul>
<p><img src="/images/Features-and-polynom-degree.png" alt="Features-and-polynom-degree"></p>
<h3 id="Regularization-and-Bias-Variance"><a href="#Regularization-and-Bias-Variance" class="headerlink" title="Regularization and Bias/Variance"></a>Regularization and Bias/Variance</h3><p>The relationship of $\lambda$ to the training set and the variance set is as follows:</p>
<ul>
<li><strong>Low $\lambda$ (High variance, overfitting)</strong>: $J_{train}(\Theta)$ is low and $J_{CV}(\Theta)$ is high (high variance/overfitting).</li>
<li><strong>Intermediate λ</strong>: $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ are somewhat low and Jtrain(Θ)≈JCV(Θ).</li>
<li><strong>Large $\lambda$ (High bias, underfitting)</strong>: both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ will be high (underfitting/high bias)<ul>
<li>A large lambda heavily penalizes all the $\Theta$ parameters, which greatly simplifies the line of our resulting function, so causes underfitting.</li>
</ul>
</li>
</ul>
<p><img src="/images/Features-and-polynom-degree-fix.png" alt="Features-and-polynom-degree-fix.png"></p>
<p>In order to choose the model and the regularization $\lambda$, we need:</p>
<ol>
<li>Create a list of lambda (i.e. $\lambda \in {0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24}$);</li>
<li>Select a lambda to compute;</li>
<li>Create a model set like degree of the polynomial or others;</li>
<li>Select a model to learn $\Theta$;</li>
<li>Learn the parameter $\Theta$ for the model selected, using $J_{train}(\Theta)$ <strong>with</strong> $\lambda$ selected (this will learn $\Theta$ for the next step);</li>
<li>Compute the train error using the learned $\Theta$ (computed with λ ) on the $J_{train}(\Theta)$ <strong>without</strong> regularization or $\lambda = 0$;</li>
<li>Compute the cross validation error using the learned $\Theta$ (computed with λ) on the $J_{CV}(\Theta)​$ <strong>without</strong> regularization or $\lambda = 0$;</li>
<li>Do this for the entire model set and lambdas, then <u>select the best combo that produces the lowest error on the cross validation set</u>;</li>
<li>Now if you need visualize to help you understand your decision, you can plot to the figure like above with: ($\lambda$ x Cost $J_{train}(\Theta)$) and ($\lambda$ x Cost $J_{CV}(\Theta)$);</li>
<li>Now using the best combo $\Theta$ and $\lambda$, apply it on Jtest(Θ) to see if it has a good generalization of the problem.</li>
<li>To help decide the best polynomial degree and $\lambda$ to use, we can diagnose with the learning curves, that is the next subject.</li>
</ol>
<h3 id="Learning-Curves"><a href="#Learning-Curves" class="headerlink" title="Learning Curves"></a>Learning Curves</h3><p>Supposed we use $h_\theta(x) = \theta_0 + \theta_1x + \theta_2x^2$, it’s clear that when $m=1, 2, 3$, we’ll get $0$ errors because we can always find a quadratic curve that exactly touches given points.</p>
<ul>
<li>As the training set gets larger, the error for a quadratic function increases.</li>
<li>The error value will plateau out after a certain m, or training set size.</li>
</ul>
<p><img src="/images/learning-curves-m.png" alt="learning-curves-m.png"></p>
<p><img src="/images/typical-learning-curve.png" alt="typical-learning-curve.png"></p>
<h4 id="High-Bias"><a href="#High-Bias" class="headerlink" title="High Bias"></a>High Bias</h4><ul>
<li><strong>Low training set size</strong>:  $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be high.</li>
<li><strong>Large training set size</strong>: both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ will be high with $J_{train}(\Theta) \approx J_{CV}(\Theta)$.</li>
</ul>
<p>If a learning algorithm is suffering from <strong>high bias</strong>, getting more training data <strong>will not (by itself) help much</strong>.</p>
<p><img src="/images/learning-curves-high-bias.png" alt="learning-curves-high-bias"></p>
<p><img src="/images/typical-learning-curve-for-high-bias.png" alt="typical-learning-curve-for-high-bias"></p>
<h4 id="High-Variance"><a href="#High-Variance" class="headerlink" title="High Variance"></a>High Variance</h4><ul>
<li><strong>Low training set size</strong>:  $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be high.</li>
<li><strong>Large training set size</strong>: $J_{train}(\Theta)$  increases with training set size and $J_{CV}(\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\Theta) &lt; J_{CV}(\Theta)$ but the difference between them remains significant.</li>
</ul>
<p>If a learning algorithm is suffering from <strong>high variance</strong>, getting more training data is <strong>likely to help.</strong></p>
<p><img src="/images/learning-curves-high-variance.png" alt="learning-curves-high-variance"></p>
<p><img src="/images/typical-learning-curve-for-high-variance.png" alt="typical-learning-curve-for-high-variance"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Advice-for-Applying-Machine-Learning&quot;&gt;&lt;a href=&quot;#Advice-for-Applying-Machine-Learning&quot; class=&quot;headerlink&quot; title=&quot;Advice for Applying Machine Learning&quot;&gt;&lt;/a&gt;Advice for Applying Machine Learning&lt;/h1&gt;&lt;h2 id=&quot;Evaluating-a-Learning-Algorithm&quot;&gt;&lt;a href=&quot;#Evaluating-a-Learning-Algorithm&quot; class=&quot;headerlink&quot; title=&quot;Evaluating a Learning Algorithm&quot;&gt;&lt;/a&gt;Evaluating a Learning Algorithm&lt;/h2&gt;&lt;h3 id=&quot;Deciding-What-to-Try-Next&quot;&gt;&lt;a href=&quot;#Deciding-What-to-Try-Next&quot; class=&quot;headerlink&quot; title=&quot;Deciding What to Try Next&quot;&gt;&lt;/a&gt;Deciding What to Try Next&lt;/h3&gt;&lt;p&gt;Errors in your predictions can be troubleshooted by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Getting more training examples&lt;/li&gt;
&lt;li&gt;Trying smaller sets of features&lt;/li&gt;
&lt;li&gt;Trying additional features&lt;/li&gt;
&lt;li&gt;Trying adding polynomial features&lt;/li&gt;
&lt;li&gt;Increasing or decreasing $\lambda$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Don’t just pick one of these avenues at random. We’ll explore diagnostic techniques for choosing one of the above solutions in the following sections.&lt;/p&gt;
&lt;p&gt;In the next few sections, We’ll first talk about how evaluate your learning algorithms and after that we’ll talk about some of these diagnostics which will hopefully let you much more effectively select more of the useful things to try mixing if your goal to improve the machine learning system. &lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Coursera" scheme="http://www.yuthon.com/tags/Coursera/"/>
    
      <category term="Machine Learning" scheme="http://www.yuthon.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Notes for Machine Learning - Week 5</title>
    <link href="http://www.yuthon.com/2016/08/17/Coursera-Machine-Learning-Week-5/"/>
    <id>http://www.yuthon.com/2016/08/17/Coursera-Machine-Learning-Week-5/</id>
    <published>2016-08-17T03:57:03.000Z</published>
    <updated>2016-09-10T07:20:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Neural-Networks-Learning"><a href="#Neural-Networks-Learning" class="headerlink" title="Neural Networks: Learning"></a>Neural Networks: Learning</h1><h2 id="Cost-Function-and-Backpropagation"><a href="#Cost-Function-and-Backpropagation" class="headerlink" title="Cost Function and Backpropagation"></a>Cost Function and Backpropagation</h2><h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><p>Let’s first define a few variables that we will need to use:</p>
<ul>
<li>$L$ = total number of layers in the network</li>
<li>$s_l$ = number of units (not counting bias unit) in layer $l$</li>
<li>$K$ = number of output units/classes</li>
</ul>
<p>Recall that the cost function for regularized logistic regression was:</p>
<p>$J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$</p>
<p>For neural networks, it is going to be slightly more complicated:</p>
<p>$J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2$</p>
<a id="more"></a>
<ul>
<li>$h_\Theta (x) \in R^K$, $(h_\Theta (x))_i$ = $i^{th}$ output</li>
<li><u>In the first part of the equation</u>, the double sum simply adds up the logistic regression costs calculated for each cell in the output layer</li>
<li><u>In the regularization part</u>, the triple sum simply adds up the squares of all the individual $\Theta$s in the entire network.<ul>
<li><u>The number of columns</u> in our current theta matrix is equal to the number of nodes in our current layer (<u>including</u> the bias unit).</li>
<li><u>The number of rows</u> in our current theta matrix is equal to the number of nodes in the next layer (<u>excluding</u> the bias unit).<ul>
<li>This is like a bias unit and by analogy to what we were doing for logistic progression, we won’t sum over those terms in our regularization term because <u>we don’t want to regularize them</u> and string their values as zero. </li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Backpropagation-Algorithm"><a href="#Backpropagation-Algorithm" class="headerlink" title="Backpropagation Algorithm"></a>Backpropagation Algorithm</h3><p><strong>“Backpropagation” (后向搜索)</strong> is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression.</p>
<p>Our goal is try to find parameters $\Theta$ to try to minimize $J(\Theta)$. </p>
<p>In order to use either gradient descent or one of the advance optimization algorithms. What we need to do therefore is to write code that takes this input the parameters theta and computes $J(\Theta)$ and $\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)$.</p>
<h4 id="Gradient-compuation"><a href="#Gradient-compuation" class="headerlink" title="Gradient compuation"></a>Gradient compuation</h4><p>Given one training example $(x,y)$</p>
<p><img src="/images/gradient_computation.png" alt="gradient_computation"></p>
<p>Forward propagation:</p>
<ul>
<li>$a^{(1)} = x$</li>
<li>$z^{(2)} = \Theta ^{(1)} a ^{(1)}$</li>
<li>$a^{(2)} = g(z^{(2)})\ (add\ a^{(2)}_0)$</li>
<li>$z^{(3)} = \Theta ^{(1)} a ^{(2)}$</li>
<li>$a^{(3)} = g(z^{(3)})\ (add\ a^{(3)}_0)$</li>
<li>$z^{(4)} = \Theta ^{(3)} a ^{(3)}$</li>
<li>$a^{(2)} = h_\Theta (x) =  g(z^{(3)})$</li>
</ul>
<p>In backpropagation we’re going to compute for every node:</p>
<p>$\delta_j^{(l)}$ = “error” of node j in layer $l$ ($s_{l+1}$ elements vector)</p>
<p>For each output unit (layer $L = 4$):</p>
<p>$\delta ^{(4)} = a^{(4)} - y$</p>
<p>To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left:</p>
<p>$\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ g’(z^{(l)})$</p>
<p>The g-prime derivative terms can also be written out as:</p>
<p>$g’(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})$</p>
<p>There is no $\delta ^{(1)}$ term, because the first layer corresponds to the input layer and that’s just the feature we observed in our training sets, so that doesn’t have any error associated with that.</p>
<p>It’s possible to prove that if you ignore regularation, then the partial derivative terms you want are exactly given by the activations and these delta terms. </p>
<p>$\dfrac{\partial J(\Theta)}{\partial \Theta_{i,j}^{(l)}} = a^{(i)}_j \delta^{(l+1)}_i\ (\text{ignoring }\lambda)$</p>
<h4 id="Backpropagation-Algorithm-1"><a href="#Backpropagation-Algorithm-1" class="headerlink" title="Backpropagation Algorithm"></a>Backpropagation Algorithm</h4><ul>
<li>Training set $\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace$</li>
<li>Set $\Delta^{(l)}_{i,j} := 0$ (for all $l, i, j$)</li>
<li>For $i=1$ to $m$<ul>
<li>Set $a^{(1)} := x^{(t)}$</li>
<li>Perform forward propagation to compute $a^{(l)}$ for $l = 2,3,\dots ,L$</li>
<li>Using $y^{(i)}$, compute $\delta^{(L)} = a^{(L)} - y^{(t)}$</li>
<li>Compute $\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}$</li>
<li>$\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}$  or with vectorization, $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$</li>
</ul>
</li>
<li>$D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)$ <strong>If</strong> $j\ne 0$ </li>
<li>$D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}$ <strong>If</strong> $j = 0$</li>
</ul>
<p>The capital-delta matrix is used as an “accumulator” to add up our values as we go along and eventually compute our partial derivative.</p>
<p> the $D_{i,j}^{(l)}$ terms are the partial derivatives and the results we are looking for:</p>
<p>$\dfrac{\partial J(\Theta)}{\partial \Theta_{i,j}^{(l)}} = D_{i,j}^{(l)}$</p>
<h3 id="Backpropagation-Intuition"><a href="#Backpropagation-Intuition" class="headerlink" title="Backpropagation Intuition"></a>Backpropagation Intuition</h3><h4 id="Forward-propagation"><a href="#Forward-propagation" class="headerlink" title="Forward propagation"></a>Forward propagation</h4><p><img src="/images/forward_propagation_intuition.png" alt="forward_propagation_intuition.png"></p>
<h4 id="What’s-backpropagation-doing"><a href="#What’s-backpropagation-doing" class="headerlink" title="What’s backpropagation doing?"></a>What’s backpropagation doing?</h4><p>The cost function is:</p>
<p>$J(\theta) = - \frac{1}{m} \sum_{t=1}^m\sum_{k=1}^K  \left[ y^{(t)}_k \ \log (h_\theta (x^{(t)}))_k + (1 - y^{(t)}_k)\ \log (1 - h_\theta(x^{(t)})_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} ( \theta_{j,i}^{(l)})^2$</p>
<p>Focusing on  a single example $x^{(i)}, y^{(i)}$, the case of 1 output unit, and ignoring regularization ($\lambda = 0$), </p>
<p>$cost(t) =y^{(t)} \ \log (h_\theta (x^{(t)})) + (1 - y^{(t)})\ \log (1 - h_\theta(x^{(t)}))$</p>
<p>Intuitively, $\theta ^{(l)}_j$ is the “error” for $a ^{(l)}_j$ (unit $j$ in layer $l$). More formally, the delta values are actually the derivative of the cost function:</p>
<p>$\delta_j^{(l)} = \dfrac{\partial}{\partial z_j^{(l)}} cost(t)$</p>
<p><img src="/images/backward_propagation_intuition.png" alt="backward_propagation_intuition.png"></p>
<p>In above, we can compute</p>
<p>$\delta ^{(4)}_1 = y^{(i)} - a^{(4)}_1$</p>
<p>$\delta ^{(3)}_2 = \Theta ^{(3)}_{12} \delta^{(4)}_1$</p>
<p>$\delta ^{(2)}_2 = \Theta ^{(2)}_{12} \delta^{(3)}_1 + \Theta ^{(2)}_{22} \delta^{(3)}_2$</p>
<h2 id="Backpropagation-in-Practice"><a href="#Backpropagation-in-Practice" class="headerlink" title="Backpropagation in Practice"></a>Backpropagation in Practice</h2><h3 id="Implementation-Note-Unrolling-Parameters"><a href="#Implementation-Note-Unrolling-Parameters" class="headerlink" title="Implementation Note: Unrolling Parameters"></a>Implementation Note: Unrolling Parameters</h3><p>We use following code to get the optimisation theta.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">function [jVal, gradient] = costFunction(theta)</div><div class="line">  ...</div><div class="line">optTheta = fminunc(@costFunction, initialTheta, options)</div></pre></td></tr></table></figure>
<p>Where <code>gradient</code>, <code>theta</code>, <code>initialTheta</code> are vectors of $n+1$ dimension.</p>
<p>In order to use optimizing functions such as <code>fminunc()</code>, we will want to “unroll” all the elements and put them into one long vector:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">thetaVec = [Theta1(:); Theta2(:); Theta3(:)];</div><div class="line">DVec = [D1(:); D2(:); D3(:)];</div></pre></td></tr></table></figure>
<p>If the dimensions of <code>Theta1</code> is $10\times11$, <code>Theta2</code> is $10\times 11$ and <code>Theta3</code> is $1\times 11$, then we can get back our original matrices from the “unrolled” versions as follows:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Theta1 = reshape(thetaVector(1:110),10,11)</div><div class="line">Theta2 = reshape(thetaVector(111:220),10,11)</div><div class="line">Theta3 = reshape(thetaVector(221:231),1,11)</div></pre></td></tr></table></figure>
<h3 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h3><p>Gradient checking will assure that our backpropagation works as intended.</p>
<h4 id="Numerical-estimation-of-gradients"><a href="#Numerical-estimation-of-gradients" class="headerlink" title="Numerical estimation of gradients"></a>Numerical estimation of gradients</h4><p><img src="/images/numerical_estimation_of_gradients.png" alt="numerical_estimation_of_gradients.png"></p>
<p>We can approximate the derivative of our cost function with:</p>
<p>$\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}$</p>
<p><strong>Implement</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gradApprox = (J(theta + EPSILON) - J(theta - EPSILON)) / (@ * EPSILON);</div></pre></td></tr></table></figure>
<h4 id="Gradient-Checking-1"><a href="#Gradient-Checking-1" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h4><p>With multiple theta matrices, we can approximate the derivative <strong>with respect to</strong> $\Theta _J$ as follows:</p>
<p>$\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}$</p>
<p>A good small value for ϵ (epsilon), guarantees the math above to become true. If the value be much smaller, may we will end up with numerical problems. The professor Andrew usually uses the value $\epsilon = 10^{-4}$.</p>
<p><strong>Implement</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">epsilon = 1e-4;</div><div class="line">for i = 1 : n,</div><div class="line">  thetaPlus = theta;</div><div class="line">  thetaPlus(i) += epsilon;</div><div class="line">  thetaMinus = theta;</div><div class="line">  thetaMinus(i) -= epsilon;</div><div class="line">  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)</div><div class="line">end;</div></pre></td></tr></table></figure>
<p>We then want to check that <code>gradApprox</code> $\approx$ <code>deltaVector</code>.</p>
<h4 id="Implement-Note"><a href="#Implement-Note" class="headerlink" title="Implement Note"></a>Implement Note</h4><ul>
<li>Implement backprop to compute <code>DVec</code> (unrolled $D^{(1)}, D^{(2)}, D^{(3)}$).</li>
<li>Implement numerical gradient check to compute <code>gradApprox</code>.</li>
<li>Make sure they give similar values.</li>
<li>Turn off gradient checking. Using backprop code for learning.</li>
</ul>
<h4 id="Important"><a href="#Important" class="headerlink" title="Important"></a>Important</h4><ul>
<li>Be sure to disable your gradient checking code before training your classifier. If you run numerical gradient computation on every iteration of gradient descent (or in the inner loop of <code>costFunction(...)</code>), your code will be <u>very</u> slow.</li>
</ul>
<h3 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h3><p>Initializing all theta weights to zero does not work with neural networks. When we backpropagate, all nodes will update to the same value repeatedly.</p>
<p>Instead we can randomly initialize our weights to break symmetry.</p>
<ul>
<li>Initialize each $\Theta^{(l)}_{ij}$ to a random value between $[-\epsilon, \epsilon]$<ul>
<li>$\epsilon = \dfrac{\sqrt{6}}{\sqrt{\mathrm{Loutput} + \mathrm{Linput}}}$</li>
<li>$\Theta^{(l)} = 2\epsilon\ rand(Loutput, Linput+1)-\epsilon$</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">% If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.</div><div class="line"></div><div class="line">Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;</div><div class="line">Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;</div><div class="line">Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;</div></pre></td></tr></table></figure>
<blockquote>
<p>Note: this epsilon is unrelated to the epsilon from Gradient Checking</p>
</blockquote>
<h3 id="Putting-It-Together"><a href="#Putting-It-Together" class="headerlink" title="Putting It Together"></a>Putting It Together</h3><p>First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers total.</p>
<ul>
<li>Number of input units = dimension of features $x^{(i)}$</li>
<li>Number of output units = number of classes</li>
<li>Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)</li>
<li>Defaults: 1 hidden layer. If more than 1 hidden layer, then the same number of units in every hidden layer.</li>
</ul>
<p><strong>Training a Neural Network</strong></p>
<ol>
<li>Randomly initialize the weights</li>
<li>Implement forward propagation to get $h_\theta(x^{(i)})$</li>
<li>Implement the cost function</li>
<li>Implement backpropagation to compute partial derivatives</li>
<li>Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.</li>
<li>Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.</li>
</ol>
<p>When we perform forward and back propagation, we loop on every training example:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">for i = 1:m,</div><div class="line">   Perform forward propagation and backpropagation using example (x(i),y(i))</div><div class="line">   (Get activations a(l) and delta terms d(l) for l = 2,...,L</div></pre></td></tr></table></figure>
<h1 id="Quiz"><a href="#Quiz" class="headerlink" title="Quiz"></a>Quiz</h1><ol>
<li>You are training a three layer neural network and would like to use backpropagation to compute the gradient of the cost function. In the backpropagation algorithm, one of the steps is to update $\Delta^{(2)}<em>{ij} := \Delta^{(2)}</em>{ij} +  \delta^{(3)}_i * (a^{(2)})_j$ for every $i,j$. Which of the following is a correct vectorization of this step?<ul>
<li>$\Delta(2) :=\Delta(2)+(a(3))^T \ast\delta(2)$</li>
<li>$\Delta(2) :=\Delta(2)+\delta(3) \ast (a(3))^T$</li>
<li>$\Delta(2) :=\Delta(2)+(a(2))^T \ast \delta(3)$</li>
<li><u>$\Delta(2) :=\Delta(2)+\delta(3) \ast (a(2))^T$</u></li>
</ul>
</li>
<li>Suppose <code>Theta1</code> is a $5\times 3$ matrix, and <code>Theta2</code> is a $4\times 6$ matrix. You set <code>thetaVec=[Theta1(:);Theta2(:)]</code>. Which of the following correctly recovers <code>Theta2</code>?<ul>
<li><u><code>reshape(thetaVec(16:39),4,6)</code></u></li>
<li><code>reshape(thetaVec(15:38),4,6)</code></li>
<li><code>reshape(thetaVec(16:24),4,6)</code></li>
<li><code>reshape(thetaVec(15:39),4,6)</code></li>
<li><code>reshape(thetaVec(16:39),6,4)</code></li>
</ul>
</li>
<li>Let $J(\theta) = 3\theta^3 + 2$. Let $\theta=1$, and $\epsilon=0.01$. Use the formula $\frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}$ to numerically compute an approximation to the derivative at $\theta=1$. What value do you get? (When $\theta=1$, the true/exact derivative is $ \frac{d J(\theta)}{ d\theta}=9$.)<ul>
<li>9</li>
<li>8.9997</li>
<li>11</li>
<li><u>9.0003</u></li>
</ul>
</li>
<li><p>Which of the following statements are true? Check all that apply.</p>
<ul>
<li>Computing the gradient of the cost function in a neural network has the same efficiency when we use backpropagation or when we numerically compute it using the method of gradient checking.</li>
<li><u>For computational efficiency, after we have performed gradient checking to verify that our backpropagation code is correct, we usually disable gradient checking before using backpropagation to train the network.</u></li>
<li><u>Using gradient checking can help verify if one’s implementation of backpropagation is bug-free.</u></li>
<li>Gradient checking is useful if we are using one of the advanced optimization methods (such as in fminunc) as our optimization algorithm. However, it serves little purpose if we are using gradient descent.</li>
<li>Using a large value of $\lambda$ cannot hurt the performance of your neural network; the only reason we do not set $\lambda$ to be too large is to avoid numerical problems.</li>
<li><u>If our neural network overfits the training set, one reasonable step to take is to increase the regularization parameter $\lambda$.</u></li>
<li>Gradient checking is useful if we are using gradient descent as our optimization algorithm. However, it serves little purpose if we are using one of the advanced optimization methods (such as in fminunc).</li>
</ul>
</li>
<li><p>Which of the following statements are true? Check all that apply.</p>
<ul>
<li>Suppose that the parameter $\theta(1)$ is a square matrix (meaning the number of rows equals the number of columns). If we replace $\theta(1)$ with its transpose ($\theta(1)^T$), then we have not changed the function that the network is computing.</li>
<li><u>Suppose we have a correct implementation of backpropagation, and are training a neural network using gradient descent. Suppose we plot $J(\theta)$ as a function of the number of iterations, and find that it is increasing rather than decreasing. One possible cause of this is that the learning rate $\alpha$ is too large.</u></li>
<li><u>If we are training a neural network using gradient descent, one reasonable “debugging” step to make sure it is working is to plot $J(\theta)$ as a function of the number of iterations, and make sure it is decreasing (or at least non-increasing) after each iteration.</u></li>
<li>Suppose we are using gradient descent with learning rate $\alpha$. For logistic regression and linear regression, $J(\theta)$ was a convex optimization problem and thus we did not want to choose a learning rate $\alpha$ that is too large. For a neural network however, $J(\theta)$ may not be convex, and thus choosing a very large value of $\alpha$ can only speed up convergence.</li>
<li>Suppose you have a three layer network with parameters $\theta(1)$ (controlling the function mapping from the inputs to the hidden units) and $\theta(2)$ (controlling the mapping from the hidden units to the outputs). If we set all the elements of $\theta(1)$ to be 0, and all the elements of $\theta(2)$ to be 1, then this suffices for symmetry breaking, since the neurons are no longer all computing the same function of the input.</li>
<li>If we initialize all the parameters of a neural network to ones instead of zeros, this will suffice for the purpose of “symmetry breaking” because the parameters are no longer symmetrically equal to zero.</li>
</ul>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Neural-Networks-Learning&quot;&gt;&lt;a href=&quot;#Neural-Networks-Learning&quot; class=&quot;headerlink&quot; title=&quot;Neural Networks: Learning&quot;&gt;&lt;/a&gt;Neural Networks: Learning&lt;/h1&gt;&lt;h2 id=&quot;Cost-Function-and-Backpropagation&quot;&gt;&lt;a href=&quot;#Cost-Function-and-Backpropagation&quot; class=&quot;headerlink&quot; title=&quot;Cost Function and Backpropagation&quot;&gt;&lt;/a&gt;Cost Function and Backpropagation&lt;/h2&gt;&lt;h3 id=&quot;Cost-Function&quot;&gt;&lt;a href=&quot;#Cost-Function&quot; class=&quot;headerlink&quot; title=&quot;Cost Function&quot;&gt;&lt;/a&gt;Cost Function&lt;/h3&gt;&lt;p&gt;Let’s first define a few variables that we will need to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$L$ = total number of layers in the network&lt;/li&gt;
&lt;li&gt;$s_l$ = number of units (not counting bias unit) in layer $l$&lt;/li&gt;
&lt;li&gt;$K$ = number of output units/classes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Recall that the cost function for regularized logistic regression was:&lt;/p&gt;
&lt;p&gt;$J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$&lt;/p&gt;
&lt;p&gt;For neural networks, it is going to be slightly more complicated:&lt;/p&gt;
&lt;p&gt;$J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2$&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Coursera" scheme="http://www.yuthon.com/tags/Coursera/"/>
    
      <category term="Machine Learning" scheme="http://www.yuthon.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Notes for Machine Learning - Week 4</title>
    <link href="http://www.yuthon.com/2016/08/15/Coursera-Machine-Learning-Week-4/"/>
    <id>http://www.yuthon.com/2016/08/15/Coursera-Machine-Learning-Week-4/</id>
    <published>2016-08-15T09:05:54.000Z</published>
    <updated>2016-10-17T07:04:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Neural-Networks-Representation"><a href="#Neural-Networks-Representation" class="headerlink" title="Neural Networks: Representation"></a>Neural Networks: Representation</h1><h2 id="Motivations"><a href="#Motivations" class="headerlink" title="Motivations"></a>Motivations</h2><h3 id="Non-linear-Hypotheses"><a href="#Non-linear-Hypotheses" class="headerlink" title="Non-linear Hypotheses"></a>Non-linear Hypotheses</h3><p><u>Performing linear regression with a complex set of data with many features is very unwieldy.</u> For 100 features, if we wanted to make them quadratic we would get 5050 resulting new features.</p>
<p>We can approximate the growth of the number of new features we get with all quadratic terms with $\mathcal{O}(n^2/2)$. And if you wanted to include all cubic terms in your hypothesis, the features would grow asymptotically at $\mathcal{O}(n^3)$. <u>These are very steep growths, so as the number of our features increase, the number of quadratic or cubic features increase very rapidly and becomes quickly impractical</u>.</p>
<p><strong>Example</strong>: let our training set be a collection of 50x50 pixel black-and-white photographs, and our goal will be to classify which ones are photos of cars. Our feature set size is then n=2500 if we compare every pair of pixels (7500 if RGB). Now let’s say we need to make a quadratic hypothesis function. With quadratic features, our growth is $\mathcal{O}(n^2/2)$. So our total features will be about 25002/2=3125000, which is very impractical.</p>
<a id="more"></a>
<p><img src="/images/car-examle.png" alt="car example"></p>
<h3 id="Neurons-and-the-Brain"><a href="#Neurons-and-the-Brain" class="headerlink" title="Neurons and the Brain"></a>Neurons and the Brain</h3><p>Origins: Algorithms that try to mimic the brain.</p>
<ul>
<li>Was very widely used in 80s and early 90s; popularity diminished in late 90s.</li>
<li>Recent resurgence: State-of-the-art technique for mant applications</li>
</ul>
<h4 id="The-“one-learning-algorithm”-hypothesis"><a href="#The-“one-learning-algorithm”-hypothesis" class="headerlink" title="The “one learning algorithm” hypothesis"></a>The “one learning algorithm” hypothesis</h4><p>There is evidence that the brain uses only one “learning algorithm” for all its different functions. Scientists have tried cutting (in an animal brain) the connection between the ears and the auditory cortex and rewiring the optical nerve with the auditory cortex to find that the auditory cortex literally learns to see.</p>
<p><img src="/images/The_one_learning_algorithm_hypothesis.png" alt="The &quot;one learning algorithm&quot; hypothesis"></p>
<p><img src="/images/sensor_representations_in_the_brain.png" alt="Sensor representations in the brain"></p>
<h2 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h2><h3 id="Model-Representation-I"><a href="#Model-Representation-I" class="headerlink" title="Model Representation I"></a>Model Representation I</h3><h4 id="Neuron-in-the-brain"><a href="#Neuron-in-the-brain" class="headerlink" title="Neuron in the brain"></a>Neuron in the brain</h4><p>At a very simple level, neurons are basically computational units that take input (<strong>dendrites</strong>, 树突) as electrical input (called “spikes”) that are channeled to outputs (<strong>axons</strong>, 轴突).</p>
<p><img src="/images/neruon_in_the_brain.png" alt="neruon_in_the_brain"></p>
<h4 id="Neuron-model-Logistic-unit"><a href="#Neuron-model-Logistic-unit" class="headerlink" title="Neuron model: Logistic unit"></a>Neuron model: Logistic unit</h4><ul>
<li>In our model, our dendrites are like the input features ($x_1 \cdots x_n$), and the output is the result of our hypothesis function $h_\theta (x)$:</li>
<li>In this model our $x_0$ input node is sometimes called the “<strong>bias unit</strong>.” It is always equal to 1.</li>
<li>In neural networks, we use the same logistic function as in classification: $\frac{1}{1 + e^{-\theta^Tx}}$. In neural networks however we sometimes call it a sigmoid (logistic) <strong>activation function</strong>.</li>
<li>Our $\theta$ parameters are sometimes instead called “<strong>weights</strong>“ in the neural networks model.</li>
</ul>
<p><img src="/images/neuron_model_logistic_unit.png" alt="neuron_model_logistic_unit"></p>
<h4 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h4><ul>
<li>The first layer is called the “<strong>input layer</strong>“ and the final layer the “<strong>output layer</strong>“, which gives the final value computed on the hypothesis.</li>
<li>We can have intermediate layers of nodes between the input and output layers called the “<strong>hidden layer</strong>“.</li>
<li>$a_i^{(j)}$ = “activation” of unit $i$ in layer $j$<ul>
<li>$a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3)$</li>
<li>$a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3)$</li>
<li>$a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3)$</li>
<li>$h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})$</li>
</ul>
</li>
<li>$\Theta^{(j)}$ = matrix of weights controlling function mapping from layer $j$ to layer $j+1$<ul>
<li>If network has $s<em>j$ units in layer $j$ and $s</em>{j+1}$ units in layer $j+1$, then $\Theta ^{(j)}$ will be of dimension $s<em>{j+1}×(s</em>{j}+1)$.</li>
</ul>
</li>
</ul>
<p><img src="/images/neural_network.png" alt="neural_network"></p>
<h3 id="Model-Representation-II"><a href="#Model-Representation-II" class="headerlink" title="Model Representation II"></a>Model Representation II</h3><h4 id="Forward-propagation-Vectorized-implementation"><a href="#Forward-propagation-Vectorized-implementation" class="headerlink" title="Forward propagation: Vectorized implementation"></a>Forward propagation: Vectorized implementation</h4><p>The vector representation of $x$ and $z^{(j)}$ is:</p>
<p>$x = \begin{bmatrix}x_0 \\ x_1 \\ \cdots \\ x_n\end{bmatrix} , z^{(j)} = \begin{bmatrix}z_1^{(j)} \\ z_2^{(j)} \\ \cdots \\ z_n^{(j)}\end{bmatrix}$</p>
<p>Setting $x=a^{(1)}$, we can rewrite the equation as:</p>
<p>$z^{(j)} = \Theta^{(j-1)}a^{(j-1)}$</p>
<p>Now we can get a vector of our activation nodes for layer $j$ as follows:</p>
<p>$a^{(j)} = g(z^{(j)})$</p>
<blockquote>
<p>We can then add a bias unit (equal to 1) to layer $j$ after we have computed $a^{(j)}$. This will be element $a^{(j)}_0$ and will be equal to 1.</p>
</blockquote>
<p>We then get our final result with:</p>
<p>$h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)})$</p>
<blockquote>
<p>This last theta matrix ($\Theta ^{(j)}$) will have only <strong>one row</strong> so that our result is a single number.</p>
</blockquote>
<p>All of this is called <strong>Forward propagation</strong> (前向传播). The forward propagation step in a neural network works where you start from the activations of the input layer and forward propagate that to the first hidden layer, then the second hidden layer, and then finally the output layer.</p>
<h4 id="Neural-Network-learning-its-own-features"><a href="#Neural-Network-learning-its-own-features" class="headerlink" title="Neural Network learning its own features"></a>Neural Network learning its own features</h4><p>The neural network, instead of being constrained to feed the features $x_1$, $x_2$, $x_3$ to logistic regression. It gets to learn its own features, $a_1$, $a_2$, $a_3$, to feed into the logistic regression.</p>
<p>Depending on what parameters it chooses for $\Theta _1$, you can learn some pretty interesting and complex features and therefore you can end up with a better hypotheses than using the raw features.</p>
<h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><h3 id="Examples-and-Intuitions-I"><a href="#Examples-and-Intuitions-I" class="headerlink" title="Examples and Intuitions I"></a>Examples and Intuitions I</h3><h4 id="AND-function"><a href="#AND-function" class="headerlink" title="AND function"></a>AND function</h4><p><img src="/images/and_function.png" alt="AND function"></p>
<p>We have:</p>
<ul>
<li>$x_1, x_2 \in {0,1}$</li>
<li>$y=x_1\ AND\ x_2$</li>
<li>$\Theta^{(1)} =\begin{bmatrix}-30 &amp; 20 &amp; 20\end{bmatrix}$</li>
</ul>
<p>And we know the plot of sigmoid function</p>
<p><img src="/images/sigmod_function_value.png" alt="sigmod_function_value"></p>
<p>So the results of $h_\Theta(x) = g(-30 + 20x_1 + 20x_2)$ are</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">$h_\Theta (x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-30) \approx 0$</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-10) \approx 0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-10) \approx 0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(10) \approx 1$</td>
</tr>
</tbody>
</table>
<h4 id="OR-function"><a href="#OR-function" class="headerlink" title="OR function"></a>OR function</h4><p>$\Theta^{(1)} =\begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix}$</p>
<p><img src="/images/or_function.png" alt="OR function"></p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">$h_\Theta (x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-10) \approx 0$</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(10) \approx 1$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(10) \approx 1$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(10) \approx 1$</td>
</tr>
</tbody>
</table>
<h3 id="Examples-and-Intuitions-II"><a href="#Examples-and-Intuitions-II" class="headerlink" title="Examples and Intuitions II"></a>Examples and Intuitions II</h3><h4 id="Negation-NOT-function"><a href="#Negation-NOT-function" class="headerlink" title="Negation (NOT function)"></a>Negation (NOT function)</h4><p>$\Theta^{(1)} =\begin{bmatrix}10 &amp; -20\end{bmatrix}$</p>
<p><img src="/images/not_function.png" alt="NOT function"></p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$h_\Theta (x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(10) \approx 1$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-10) \approx 0$</td>
</tr>
</tbody>
</table>
<h4 id="NOR-function"><a href="#NOR-function" class="headerlink" title="NOR function"></a>NOR function</h4><p>$\Theta^{(1)} = \begin{bmatrix}10 &amp; -20 &amp; -20\end{bmatrix}$</p>
<p><img src="/images/nor_function.png" alt="NOR function"></p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">$h_\Theta (x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(10) \approx 1$</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-10) \approx 0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-10) \approx 0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-30) \approx 0$</td>
</tr>
</tbody>
</table>
<h4 id="XNOR-function"><a href="#XNOR-function" class="headerlink" title="XNOR function"></a>XNOR function</h4><p><img src="/images/xnor_function.png" alt="XNOR function"></p>
<h3 id="Multiclass-Classification"><a href="#Multiclass-Classification" class="headerlink" title="Multiclass Classification"></a>Multiclass Classification</h3><p>To classify data into multiple classes, we let our hypothesis function return a vector of values. Say we wanted to classify our data into one of four final resulting classes:</p>
<p><img src="/images/NN_one_vs_all.png" alt="Multiple output units"></p>
<p>Our final layer of nodes, when multiplied by its theta matrix, will result in another vector, on which we will apply the $g()$ logistic function to get a vector of hypothesis values.</p>
<p>Our resulting hypothesis for one set of inputs may look like:</p>
<p>$h_\Theta(x) = \begin{bmatrix}0 \\ 0 \\ 1 \\ 0 \\ \end{bmatrix}$</p>
<p>In which case our resulting class is the third one down, or $h_\Theta (x)_3$.</p>
<p>We can define our set of resulting classes as $y$:</p>
<p>$y^{(i)} = \begin{bmatrix}1\\ 0\\ 0\\ 0\end{bmatrix},\ \begin{bmatrix}0 \\ 1\\ 0\\ 0\end{bmatrix},\ \begin{bmatrix}0\\ 0\\ 1\\ 0\end{bmatrix},\ \begin{bmatrix}0\\ 0\\ 0\\ 1\end{bmatrix}$</p>
<p>Our final value of our hypothesis for a set of inputs will be one of the elements in $y$.</p>
<h2 id="Quiz"><a href="#Quiz" class="headerlink" title="Quiz"></a>Quiz</h2><ol>
<li><p>Which of the following statements are true? Check all that apply.</p>
<ul>
<li><strong>TRUE</strong> If a neural network is overfitting the data, one solution would be to increase the regularization parameter $\lambda$.</li>
<li><strong>FALSE</strong> If a neural network is overfitting the data, one solution would be to decrease the regularization parameter $\lambda$.</li>
<li><strong>FALSE</strong> Suppose you have a multi-class classification problem with three classes, trained with a 3 layer network. Let $a^{(3)}_1 = (h_\Theta(x))_1$ be the activation of the first output unit, and similarly $a^{(3)}_2 = (h_\Theta(x))_2$ and $a^{(3)}_3 = (h_\Theta(x))_3$. Then for any input x, it must be the case that $a^{(3)}_1 + a^{(3)}_2 + a^{(3)}_3 = 1$.<ul>
<li>The outputs of a neural network are not probabilities, so their sum need not be 1.</li>
</ul>
</li>
<li><strong>TRUE</strong> In a neural network with many layers, we think of each successive layer as being able to use the earlier layers as features, so as to be able to compute increasingly complex functions.</li>
</ul>
</li>
<li><p>Consider the following neural network which takes two binary-valued inputs $x_1,x_2\in {0,1}$ and outputs $h_\Theta (x)$. Which of the following logical functions does it (approximately) compute?</p>
<p><img src="/images/quiz_4-2.png" alt="quiz_4-2"></p>
<ul>
<li>OR</li>
</ul>
</li>
<li><p>Consider the neural network given below. Which of the following equations correctly computes the activation $a{(3)}_1$? Note: $g(z)$ is the sigmoid activation function.</p>
<p><img src="/images/quiz_4-3.png" alt="quiz_4-3"></p>
<ul>
<li>$a_1^{(3)} = g(\Theta_{1,0}^{(2)}a_0^{(2)} + \Theta_{1,1}^{(2)}a_1^{(2)} + \Theta_{1,2}^{(2)}a_2^{(2)})$</li>
</ul>
</li>
<li><p>You have the following neural network:</p>
<p><img src="/images/quiz_4-4.png" alt="quiz_4-4"></p>
<p>You’d like to compute the activations of the hidden layer $a^{(2)}\in R^3$. One way to do so is the following Octave code:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">% Theta1 is Theta with superscript &quot;(1)&quot; from lecture</div><div class="line">% ie, the matrix of parmeters for the mapping from layer 1 (input) to layer 2</div><div class="line">% Theta1 has size 3x3</div><div class="line">% Assume &apos;sigmoid&apos; is a built-in function to compute 1 / (1 + exp(-z))</div><div class="line"></div><div class="line">a2 = zeros(3, 1);</div><div class="line">for i = 1:3</div><div class="line">  for j = 1:3</div><div class="line">    a2(i) = a2(i) + x(j) * Theta1(i, j);</div><div class="line">  end</div><div class="line">  a2(i) = sigmoid(a2(i));</div><div class="line">end</div></pre></td></tr></table></figure>
<p>You want to have a vectorized implementation of this (i.e., one that does not use for loops). Which of the following implementations correctly compute $a^{(2)}$? Check all that apply.</p>
<ul>
<li><code>a2 = sigmoid (Theta1 * x);</code></li>
</ul>
</li>
<li><p>You are using the neural network pictured below and have learned the parameters $\Theta^{(1)} = \begin{bmatrix} 1 &amp; 0.5 &amp; 1.9 \\ 1 &amp; 1.2 &amp; 2.7 \end{bmatrix}$ and $\Theta^{(2)} = \begin{bmatrix} 1 &amp;  -0.2 &amp; -1.7 \end{bmatrix}$. Suppose you swap the parameters for the first hidden layer between its two units so $\Theta^{(1)} = \begin{bmatrix} 1 &amp; 1.2 &amp; 2.7 \\ 1  &amp; 0.5 &amp; 1.9 \end{bmatrix}$ and also swap the output layer so $\Theta^{(2)} = \begin{bmatrix} 1 &amp; -1.7 &amp; -0.2 \end{bmatrix}$. How will this change the value of the output $h_\Theta (x)$?</p>
<p><img src="/images/quiz_4-5.png" alt="quiz_4-5"></p>
<ul>
<li>It will stay the same.</li>
</ul>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Neural-Networks-Representation&quot;&gt;&lt;a href=&quot;#Neural-Networks-Representation&quot; class=&quot;headerlink&quot; title=&quot;Neural Networks: Representation&quot;&gt;&lt;/a&gt;Neural Networks: Representation&lt;/h1&gt;&lt;h2 id=&quot;Motivations&quot;&gt;&lt;a href=&quot;#Motivations&quot; class=&quot;headerlink&quot; title=&quot;Motivations&quot;&gt;&lt;/a&gt;Motivations&lt;/h2&gt;&lt;h3 id=&quot;Non-linear-Hypotheses&quot;&gt;&lt;a href=&quot;#Non-linear-Hypotheses&quot; class=&quot;headerlink&quot; title=&quot;Non-linear Hypotheses&quot;&gt;&lt;/a&gt;Non-linear Hypotheses&lt;/h3&gt;&lt;p&gt;&lt;u&gt;Performing linear regression with a complex set of data with many features is very unwieldy.&lt;/u&gt; For 100 features, if we wanted to make them quadratic we would get 5050 resulting new features.&lt;/p&gt;
&lt;p&gt;We can approximate the growth of the number of new features we get with all quadratic terms with $\mathcal{O}(n^2/2)$. And if you wanted to include all cubic terms in your hypothesis, the features would grow asymptotically at $\mathcal{O}(n^3)$. &lt;u&gt;These are very steep growths, so as the number of our features increase, the number of quadratic or cubic features increase very rapidly and becomes quickly impractical&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: let our training set be a collection of 50x50 pixel black-and-white photographs, and our goal will be to classify which ones are photos of cars. Our feature set size is then n=2500 if we compare every pair of pixels (7500 if RGB). Now let’s say we need to make a quadratic hypothesis function. With quadratic features, our growth is $\mathcal{O}(n^2/2)$. So our total features will be about 25002/2=3125000, which is very impractical.&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Coursera" scheme="http://www.yuthon.com/tags/Coursera/"/>
    
      <category term="Machine Learning" scheme="http://www.yuthon.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Notes for Machine Learning - Week 3</title>
    <link href="http://www.yuthon.com/2016/08/05/Coursera-Machine-Learning-Week-3/"/>
    <id>http://www.yuthon.com/2016/08/05/Coursera-Machine-Learning-Week-3/</id>
    <published>2016-08-05T04:16:08.000Z</published>
    <updated>2016-08-15T05:47:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><h2 id="Classification-and-Representation"><a href="#Classification-and-Representation" class="headerlink" title="Classification and Representation"></a>Classification and Representation</h2><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><ul>
<li>Calssification Problem<ul>
<li>$y\in {0,1}$<ul>
<li>0: “Negative Class”,  负类</li>
<li>1: “Positive Class”, 正类</li>
</ul>
</li>
<li>One method is to use linear regression and map all predictions greater than 0.5 as a 1 and all less than 0.5 as a 0. This method doesn’t work well because classification is not actually a linear function.</li>
</ul>
</li>
<li>Logistic Regression (逻辑回归) : $0\le h_\theta \le 1$</li>
</ul>
<a id="more"></a>
<h3 id="Hypothesis-Representation"><a href="#Hypothesis-Representation" class="headerlink" title="Hypothesis Representation"></a>Hypothesis Representation</h3><h4 id="Logistic-Regression-Model"><a href="#Logistic-Regression-Model" class="headerlink" title="Logistic Regression Model"></a>Logistic Regression Model</h4><p>$h_\theta (x) = \frac{1}{1+e^{-\theta ^T x}}​$</p>
<ul>
<li><p>Want $0\le h_\theta(x)\le 1$</p>
<ul>
<li><p>$h_\theta (x) = g(\theta ^T x)$</p>
</li>
<li><p>$g(z) = \frac{1}{1+e^{-z}}$</p>
<ul>
<li><p>Called “Sigmod function” or “Logistic function”</p>
<p><img src="/images/sigmod_function.png" alt="sigmod function"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Interpretation-of-Hypothesis-Output"><a href="#Interpretation-of-Hypothesis-Output" class="headerlink" title="Interpretation of Hypothesis Output"></a>Interpretation of Hypothesis Output</h4><ul>
<li>$h_\theta (x)$ = estimated probability that $y=1$ on input $x$<ul>
<li>$h_\theta (x) = P(y=1|x; \theta)$<ul>
<li>probability that $y=1$, given $x$, parameterised by $\theta$.</li>
</ul>
</li>
<li>$P(y=0|x;\theta ) = 1 - P(y=1|x;\theta )$</li>
</ul>
</li>
</ul>
<h3 id="Decision-Boundary"><a href="#Decision-Boundary" class="headerlink" title="Decision Boundary"></a>Decision Boundary</h3><p>In order to get our discrete 0 or 1 classification, we can suppose</p>
<ul>
<li>$h_\theta(x) \geq 0.5 \rightarrow y = 1$</li>
<li>$h_\theta(x) &lt; 0.5 \rightarrow y = 0$</li>
</ul>
<p>The way our logistic function $g$ behaves is that when its input is greater than or equal to zero, its output is greater than or equal to 0.5:</p>
<ul>
<li>$g(z) \ge 0.5$ when $z\ge 0$, i.e., $\theta ^T x \ge 0$</li>
</ul>
<p>In conclusion, we can now say:</p>
<ul>
<li>$\theta^T x \geq 0 \Rightarrow y = 1$</li>
<li>$\theta^T x &lt; 0 \Rightarrow y = 0$</li>
</ul>
<h4 id="Decision-boundaries"><a href="#Decision-boundaries" class="headerlink" title="Decision boundaries"></a>Decision boundaries</h4><p>The <strong>decision boundary</strong> is the line that separates the area where $y=0$ and where $y=1$. It is created by our hypothesis function $\theta^T x = 0$.</p>
<p><img src="/images/dicision_boundary.png" alt="Disicion Boundary"></p>
<p>The decision boundary is a property, not of the trading set, but of the hypothesis $h_\theta(x)$ under the parameters. As long as we’re given parameter vector $\theta$, that defines the decision boundary. <u>But the training set is not what we use to define the decision boundary.</u></p>
<h4 id="Non-linear-decision-boundaries"><a href="#Non-linear-decision-boundaries" class="headerlink" title="Non-linear decision boundaries"></a>Non-linear decision boundaries</h4><p>The input to the sigmoid function $g(z)$ (e.g. $\theta ^T x$) doesn’t need to be linear, and could be a function that describes a circle (e.g. $z = \theta_0 + \theta _1 x_1 + \theta _2 x_2 + \theta _3 x_1^2 + \theta _4 x_2^2$) or any shape to fit our data.</p>
<p><img src="/images/nonlinear_decision_boundary.png" alt="nonlinear decision boundary"></p>
<h2 id="Logistic-Regression-Model-1"><a href="#Logistic-Regression-Model-1" class="headerlink" title="Logistic Regression Model"></a>Logistic Regression Model</h2><h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><p>We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function (凸函数).</p>
<p><img src="/images/non-convex_and_convex_function.png" alt="non-convex and convex function"></p>
<p>Instead, our cost function for logistic regression looks like:</p>
<p>$J(\theta) = \dfrac{1}{m} \sum<em>{i=1}^m \mathrm{Cost}(h</em>\theta(x^{(i)}),y^{(i)})$</p>
<p>$\mathrm{Cost}(h<em>\theta(x),y) = \begin{cases}-\log(h</em>\theta(x)) ,&amp;\text{if y = 1}\newline -\log(1-h_\theta(x)) ,&amp;\text{if y = 0}\end{cases}$</p>
<ul>
<li>$\mathrm{Cost} = 0$ if $y=1, h_\theta (x)=1$</li>
<li>But as $h_\theta (x) \to 0, \mathrm{Cost} \to \infty$<ul>
<li>Captures intuition that if $h_\theta (x) = 0$ (predict $P(y=1|x;\theta ) = 0$), but $y=1$, we’ll <strong>penalise</strong> learning algorithm by a very large cost.</li>
</ul>
</li>
</ul>
<p><img src="/images/Logistic_regression_cost_function_positive_class.png" alt="Logistic_regression_cost_function_positive_class"></p>
<p><img src="/images/Logistic_regression_cost_function_negative_class.png" alt="Logistic_regression_cost_function_negative_class"></p>
<h3 id="Simplified-Cost-Function-and-Gradient-Descent"><a href="#Simplified-Cost-Function-and-Gradient-Descent" class="headerlink" title="Simplified Cost Function and Gradient Descent"></a>Simplified Cost Function and Gradient Descent</h3><h4 id="Simplified-Cost-Function"><a href="#Simplified-Cost-Function" class="headerlink" title="Simplified Cost Function"></a>Simplified Cost Function</h4><p>We can compress our cost function’s two conditional cases into one case:</p>
<p>$\mathrm{Cost}(h_\theta(x),y) = - y \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))$</p>
<p>We can fully write out our entire cost function as follows:</p>
<p>$J(\theta) = - \frac{1}{m} \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]$</p>
<ul>
<li>This cost function can be derived from statistics using the principle of maximum likelihood estimation (极大似然估计). Which is an idea in statistics for how to efficiently find parameters’ data for different models.</li>
<li>And it also has a nice property that it is convex.</li>
</ul>
<p>A vectorized implementation is:</p>
<p>$h = g(X\theta)$</p>
<p>$ J(\theta)  = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right)$</p>
<h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><p>$Repeat \lbrace \\ \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta)= \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\ \rbrace$</p>
<ul>
<li><p>Notice that this algorithm is identical to the one we used in linear regression (only $h_\theta (x)$ changes). We still have to simultaneously update all values in theta.</p>
</li>
<li><p>A vectorized implementation is:</p>
<p>$\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})$</p>
</li>
<li><p>To make sure th learning rate $\alpha$ is set properly, you can plot $J(\theta)$ as a function of the number of iterations and make sure $J(\theta )$ is decreasing on every iteration.</p>
</li>
</ul>
<h3 id="Advanced-Optimization"><a href="#Advanced-Optimization" class="headerlink" title="Advanced Optimization"></a>Advanced Optimization</h3><p>Optimization algorithms</p>
<ul>
<li>Gradient descent</li>
<li>Conjugate gradient</li>
<li>BFGS</li>
<li>L-BFGS</li>
</ul>
<p>The avdantage of last three algorithms:</p>
<ul>
<li>No need to manually pick $\alpha$</li>
<li>Often faster than gradient descent</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>More complex</li>
</ul>
<h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><p>We first need to provide a function that evaluates the following two functions for a given input value $\theta$ :</p>
<ul>
<li>$J(\theta )$</li>
<li>$\frac{\partial}{\partial \theta _j} J(\theta )$</li>
</ul>
<p>We can write a single function that returns both of these:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">function [jVal, gradient] = costFunction(theta)</div><div class="line">  jVal = [...code to compute J(theta)...];</div><div class="line">  gradient = [...code to compute derivative of J(theta)...];</div><div class="line">end</div></pre></td></tr></table></figure>
<p>Then we can use octave’s “fminunc()” optimization algorithm along with the “optimset()” function that creates an object containing the options we want to send to “fminunc()”.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">options = optimset(&apos;GradObj&apos;, &apos;on&apos;, &apos;MaxIter&apos;, 100);</div><div class="line">initialTheta = zeros(2,1);</div><div class="line">[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);</div></pre></td></tr></table></figure>
<h2 id="Multiclass-Classification-One-vs-all"><a href="#Multiclass-Classification-One-vs-all" class="headerlink" title="Multiclass Classification: One-vs-all"></a>Multiclass Classification: One-vs-all</h2><p>Instead of $y = {0,1}$, we will expand our definition so that $y = {1,2…n}$. In this case we divide our problem into $n$ binary classification problems; in each one, we predict the probability that ‘y’ is a member of one of our classes.</p>
<p>That is, we can train a logistic regression classifier $h_\theta ^{(i)} (x)$ for each class $i$ to predict the probability that $y=i$.</p>
<p>$h_\theta^{(i)}(x) = P(y = i | x ; \theta)\ \ \ \  (i=1,2,3,\dots , n+1)$</p>
<p>On a new input $x$, to make a prediction, pick the class $i$ that maximizes $h_\theta ^{(i)} (x)$.</p>
<p><img src="/images/one_vs_all.png" alt="One-vs-all"></p>
<h1 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h1><h2 id="Solving-the-Problem-of-Overfitting"><a href="#Solving-the-Problem-of-Overfitting" class="headerlink" title="Solving the Problem of Overfitting"></a>Solving the Problem of Overfitting</h2><h3 id="The-Problem-of-Overfitting"><a href="#The-Problem-of-Overfitting" class="headerlink" title="The Problem of Overfitting"></a>The Problem of Overfitting</h3><p><img src="/images/overfitting.png" alt="overfitting"></p>
<ul>
<li><strong>High bias</strong> or <strong>underfitting</strong> is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. eg. if we take $h_\theta (x)=\theta _0+\theta _1x_1+\theta _2x_2$ then we are making an initial assumption that a linear model will fit the training data well and will be able to generalize but that may not be the case.</li>
<li>At the other extreme, <strong>overfitting</strong> or <strong>high variance</strong> is caused by a hypothesis function that fits the available data but does not generalize (泛化) well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</li>
</ul>
<p>This terminology is applied to both linear and logistic regression.</p>
<p><img src="/images/overfitting_2.png" alt="overfitting_2"></p>
<p>There are <strong>two</strong> main options to address the issue of overfitting:</p>
<ol>
<li>Reduce the number of features.<ul>
<li>Manually select which features to keep.</li>
<li>Use a model selection algorithm (later in the course).</li>
</ul>
</li>
<li>Regularization (正则化)<ul>
<li>Keep all the features, but reduce the parameters $\theta _j$.</li>
</ul>
</li>
</ol>
<p>Regularization works well when we have a lot of slightly useful features.</p>
<h3 id="Cost-Function-1"><a href="#Cost-Function-1" class="headerlink" title="Cost Function"></a>Cost Function</h3><h4 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h4><p>If we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost.</p>
<p>Say we wanted to make the following function more quadratic:</p>
<p>$\theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4$</p>
<p>We’ll want to eliminate the influence of $\theta _3x_3​$ and $\theta _4x_4​$. Without actually getting rid of these features or changing the form of our hypothesis, we can instead modify our <strong>cost function</strong>:</p>
<p>$min_\theta\ \dfrac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + 1000\cdot \theta _3^2 + 1000\cdot \theta _4^2$</p>
<p>We’ve added two extra terms at the end to inflate the cost of $\theta_3$ and $\theta_4$. Now, in order for the cost function to get close to zero, we will have to reduce the values of $\theta_3$ and $\theta_4$ to near zero. This will in turn greatly reduce the values of $\theta _3x_3$ and $\theta _4x_4$ in our hypothesis function.</p>
<p><img src="/images/regularization_intuition.png" alt="regularization_intuition"></p>
<h4 id="Regularization-1"><a href="#Regularization-1" class="headerlink" title="Regularization"></a>Regularization</h4><p>We could also regularize all of our theta parameters in a single summation:</p>
<p>$min_\theta \dfrac{1}{2m}\ \left[ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2 \right]$</p>
<p>The $\lambda$, or lambda, is the <strong>regularization parameter</strong>. It determines how much the costs of our theta parameters are inflated. </p>
<ul>
<li>Using the above cost function with the extra summation, we can smooth the output of our hypothesis function to reduce overfitting. </li>
<li>If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting. (fails to fit even the training set).</li>
</ul>
<h3 id="Regularized-Linear-Regression"><a href="#Regularized-Linear-Regression" class="headerlink" title="Regularized Linear Regression"></a>Regularized Linear Regression</h3><h4 id="Gradient-Descent-1"><a href="#Gradient-Descent-1" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><p>We will modify our gradient descent function to separate out $\theta_0$ from the rest of the parameters because <strong>we do not want to penalize $\theta_0$.</strong></p>
<p>$\text{Repeat}\ \lbrace \\ \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\ \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] \ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2…n\rbrace \\ \rbrace$</p>
<ul>
<li>The term $\frac{\lambda}{m}\theta_j$ performs our regularization.</li>
</ul>
<p>With some manipulation our update rule can also be represented as:</p>
<p>$\theta_j := \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$</p>
<ul>
<li>The first term in the above equation, $1 - \alpha\frac{\lambda}{m}$ will always be less than 1. Intuitively you can see it as reducing the value of $\theta _j$ by some amount on every update.</li>
<li>Notice that the second term is now exactly the same as it was before.</li>
</ul>
<h4 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a>Normal Equation</h4><p>To add in regularization, the equation is the same as our original, except that we add another term inside the parentheses:</p>
<p>$\theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty$</p>
<p>$L = \begin{bmatrix} 0 &amp; &amp; &amp; &amp; \\ &amp; 1 &amp; &amp; &amp; \\ &amp; &amp; 1 &amp; &amp; \\ &amp; &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; &amp; 1 \\ \end{bmatrix}$</p>
<p>$L$ should have dimension $(n+1)\times (n+1)$. Intuitively, this is the identity matrix (though we are not including $x_0$), multiplied with a single real number $\lambda$.</p>
<p>Recall that if $m\le n$, then $X^TX$ is non-invertible. However, when we add the term $\lambda \cdot L$, then $X^TX + \lambda \cdot L$ becomes invertible.</p>
<h3 id="Regularized-Logistic-Regression"><a href="#Regularized-Logistic-Regression" class="headerlink" title="Regularized Logistic Regression"></a>Regularized Logistic Regression</h3><h4 id="Cost-Function-2"><a href="#Cost-Function-2" class="headerlink" title="Cost Function"></a>Cost Function</h4><p>We can regularize this equation by adding a term to the end:</p>
<p>$J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$</p>
<p><strong>Note Well:</strong> The second sum, $\sum_{j=1}^n \theta_j^2$ <strong>means to explicitly exclude</strong> the bias term, $\theta _0$. I.e. the $\theta$ vector is indexed from 0 to n (holding $n+1$ values, $\theta _0$ through $\theta _n$), and this sum explicitly skips $\theta _0$, by running from 1 to n, skipping 0.</p>
<h4 id="Gradient-Descent-2"><a href="#Gradient-Descent-2" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><p>$\text{Repeat}\ \lbrace \\ \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\\ \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] \ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2…n\rbrace\\\ \rbrace$</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Logistic-Regression&quot;&gt;&lt;a href=&quot;#Logistic-Regression&quot; class=&quot;headerlink&quot; title=&quot;Logistic Regression&quot;&gt;&lt;/a&gt;Logistic Regression&lt;/h1&gt;&lt;h2 id=&quot;Classification-and-Representation&quot;&gt;&lt;a href=&quot;#Classification-and-Representation&quot; class=&quot;headerlink&quot; title=&quot;Classification and Representation&quot;&gt;&lt;/a&gt;Classification and Representation&lt;/h2&gt;&lt;h3 id=&quot;Classification&quot;&gt;&lt;a href=&quot;#Classification&quot; class=&quot;headerlink&quot; title=&quot;Classification&quot;&gt;&lt;/a&gt;Classification&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Calssification Problem&lt;ul&gt;
&lt;li&gt;$y\in {0,1}$&lt;ul&gt;
&lt;li&gt;0: “Negative Class”,  负类&lt;/li&gt;
&lt;li&gt;1: “Positive Class”, 正类&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;One method is to use linear regression and map all predictions greater than 0.5 as a 1 and all less than 0.5 as a 0. This method doesn’t work well because classification is not actually a linear function.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Logistic Regression (逻辑回归) : $0\le h_\theta \le 1$&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Coursera" scheme="http://www.yuthon.com/tags/Coursera/"/>
    
      <category term="Machine Learning" scheme="http://www.yuthon.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Exercise in Machine Learning</title>
    <link href="http://www.yuthon.com/2016/08/05/Coursera-Machine-Learning-Exercise/"/>
    <id>http://www.yuthon.com/2016/08/05/Coursera-Machine-Learning-Exercise/</id>
    <published>2016-08-05T04:12:54.000Z</published>
    <updated>2016-08-05T04:15:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>My exercise files and notes are put in <a href="https://github.com/corenel/Notes-for-Machine-Learning-in-Coursera" target="_blank" rel="external">Github</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;My exercise files and notes are put in &lt;a href=&quot;https://github.com/corenel/Notes-for-Machine-Learning-in-Coursera&quot; target=&quot;_blank&quot; rel=&quot;e
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Coursera" scheme="http://www.yuthon.com/tags/Coursera/"/>
    
      <category term="Machine Learning" scheme="http://www.yuthon.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Notes for Machine Learning - Week 2</title>
    <link href="http://www.yuthon.com/2016/07/27/Coursera-Machine-Learning-Week-2/"/>
    <id>http://www.yuthon.com/2016/07/27/Coursera-Machine-Learning-Week-2/</id>
    <published>2016-07-27T02:30:35.000Z</published>
    <updated>2016-08-01T13:40:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linear-Regression-with-Multiple-Variables"><a href="#Linear-Regression-with-Multiple-Variables" class="headerlink" title="Linear Regression with Multiple Variables"></a>Linear Regression with Multiple Variables</h1><h2 id="Multivariate-Linear-Regression"><a href="#Multivariate-Linear-Regression" class="headerlink" title="Multivariate Linear Regression"></a>Multivariate Linear Regression</h2><ul>
<li>Multiple features (variables)<ul>
<li>$n$ = number of features</li>
<li>$x^{(i)}$ = input (features) of $i^{th}$ training example.</li>
<li>$x^{(i)}_j$ = value of feature $j$ in $i^{th}$ training example.</li>
</ul>
</li>
<li>Hypotesis<ul>
<li>Previously: $h_\theta (x) = \theta_0 + \theta_1 x$</li>
<li>$h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n$<ul>
<li>For convenience of notation, define $x_0=1$</li>
<li>$x=\begin{bmatrix}x_0 \\ x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}, \theta = \begin{bmatrix}\theta_0 \\ \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n \end{bmatrix}, h_\theta (x) = \theta^T x​$</li>
</ul>
</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="Gradient-Descent-for-Multiple-Variables"><a href="#Gradient-Descent-for-Multiple-Variables" class="headerlink" title="Gradient Descent for Multiple Variables"></a>Gradient Descent for Multiple Variables</h2><ul>
<li><p>Hypothesis: $h_\theta(x)=\theta^Tx=\theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n$</p>
</li>
<li><p>Parameters: $\theta_0, \theta_1, \dots ,\theta_n$</p>
</li>
<li><p>Cost function: $J(\theta_0, \theta_1, \dots, \theta_n) = \frac{1}{2m} \sum^m_{i=1}\left(h_\theta (x^{(i)})-y^{(i)}\right)^2$</p>
<ul>
<li>or $J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(\theta^T x^{(i)} - y^{(i)})^2$</li>
</ul>
</li>
<li><p>Gradient descent</p>
<blockquote>
<p>repeat {</p>
<p>  $\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \dots ,\theta_n)$</p>
<p>  (simultaneously update for every $j=0,\dots,n$)</p>
<p>}</p>
</blockquote>
<p>or</p>
<blockquote>
<p>repeat {</p>
<p>  $\theta_j := \theta_j - \alpha \frac{1}{m} \sum^m_{i=1}\left(h_\theta(x^{(i)})-y^{(i)}\right) x^{(i)}_j$ </p>
<p>  (simultaneously update for every $j=0,\dots,n$)</p>
<p>}</p>
</blockquote>
</li>
</ul>
<h2 id="Gradient-Descent-in-Practice"><a href="#Gradient-Descent-in-Practice" class="headerlink" title="Gradient Descent in Practice"></a>Gradient Descent in Practice</h2><h3 id="Feature-Normalization"><a href="#Feature-Normalization" class="headerlink" title="Feature Normalization"></a>Feature Normalization</h3><p>Idea: Make sure featueres are on a similar scale.</p>
<ul>
<li>We can speed up gradient descent by having each of our input values in roughly the same range. This is because $\theta$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</li>
<li>The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same.</li>
</ul>
<h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><ul>
<li>$x_1$ = size (0-2000 $feet^2$)</li>
<li>$x_2$ = number of bedrooms (1-5)</li>
</ul>
<p>$x_1$ has a much larger range of values than $x_2$. So the $J(\theta_1, \theta_2)$ can be a very very skewed elliptical shape. And if you run gradient descents on this cost function, your gradients may end up taking a long time and can oscillate back and forth and take a long time before it can finally find its way to the global minimum. </p>
<h4 id="Feature-scaling"><a href="#Feature-scaling" class="headerlink" title="Feature scaling"></a>Feature scaling</h4><p>Get every feature into approcimately $-1\le x_i \le 1$ range.</p>
<ul>
<li>Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1.</li>
<li>These aren’t exact requirements; we are only trying to speed things up.</li>
</ul>
<ul>
<li>$-3\le x_i \le 3$ or $ -\frac{1}{3} \le x_i \le \frac{1}{3}$ just is fine.</li>
</ul>
<h4 id="Mean-normalization"><a href="#Mean-normalization" class="headerlink" title="Mean normalization"></a>Mean normalization</h4><p>Replace $x_i$ with $x_i - \mu _i$ to make features have approximately zero mean (Do not apply to $x_0 = 1$)</p>
<ul>
<li>E.g. $x_1 = \frac{size -1000}{2000}, x_2 = \frac{bedrooms - 2}{5}$</li>
<li>$x_i = \frac{x_i - \mu _i}{s_i}$<ul>
<li>$\mu _i$ is the average value of $x_i$ in training set.</li>
<li>$s_i$ is the range ($x_{imax}-x_{imin}$) or standard deviation ($\sigma$)</li>
</ul>
</li>
</ul>
<h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><ul>
<li>“Debugging”: <strong>How to make sure gradient descent is working correctly</strong><ul>
<li>Make a plot with <em>number of iterations</em> on the x-axis. Now plot the cost function, $J(\theta)$ over the number of iterations of gradient descent.<ul>
<li>For sufficient small $\alpha$, $J(\theta)$ should decreases on every iteration.</li>
<li>But if $\alpha$ is too small, gradient descent can be slow to converge.</li>
<li>If $J(\theta)$ ever increases, then you probably need to use smaller $\alpha$.</li>
</ul>
</li>
<li>Example automatic convergence test<ul>
<li>Declare convergence if $J(\theta)$ decreases by less than $\epsilon$ (e.g., $10^{-3})$ in one iteration.</li>
</ul>
</li>
</ul>
</li>
<li><strong>How to choose learing rate $\alpha$</strong><ul>
<li>So just try running gradient descent with a range of values for $\alpha$, like 0.001 and 0.01. And for these different values of $\alpha$ are just plot $J(\theta)$ as a function of number of iterations, and then pick the value of $\alpha$ that seems to be causing $J(\theta)$to decrease rapidly.</li>
<li>Andrew Ng recommends decreasing $\alpha$ by multiples of 3. And then try to pick the largest possible value, or just something slightly smaller than the largest reasonable value.</li>
<li>E.g. $\dots, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, \dots$</li>
</ul>
</li>
</ul>
<h2 id="Features-and-Polynomial-Regression"><a href="#Features-and-Polynomial-Regression" class="headerlink" title="Features and Polynomial Regression"></a>Features and Polynomial Regression</h2><h3 id="Choice-of-features"><a href="#Choice-of-features" class="headerlink" title="Choice of features"></a>Choice of features</h3><ul>
<li>We can improve our features and the form of our hypothesis function in a couple different ways.</li>
<li>We can <strong>combine</strong> multiple features into one. For example, we can combine $x_1​$ and $x_2​$ into a new feature $x_3​$ by taking $x_1\cdot x_2​$. (E.g. $House Area = Frontage \times Depth​$)</li>
</ul>
<h3 id="Polynomial-Regression"><a href="#Polynomial-Regression" class="headerlink" title="Polynomial Regression"></a>Polynomial Regression</h3><p>Our hypothesis function need not be linear (a straight line) if that does not fit the data well.</p>
<ul>
<li><p>We can <strong>change the behavior or curve</strong> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</p>
<ul>
<li><p>For example, if our hypothesis function is $h_\theta(x) = \theta_0 + \theta_1 x_1$ then we can create additional features based on $x_1$, to get the quadratic function $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2$ or the cubic function $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2 + \theta_3 x_1^3$.</p>
</li>
<li><p>In the cubic version, we have created new features $x_2$ and $x_3$ where $x_2 = x_1^2$ and $x_3=x^3_1$.</p>
</li>
<li><p>To make it a square root function, we could do: $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 \sqrt{x_1}$</p>
</li>
<li><p>Note that at 2:52 and through 6:22 in the “Features and Polynomial Regression” video, the curve that Prof Ng discusses about “doesn’t ever come back down” is in reference to the hypothesis function that uses the <code>sqrt()</code> function (shown by the solid purple line), not the one that uses $size^2$ (shown with the dotted blue line). The quadratic form of the hypothesis function would have the shape shown with the blue dotted line if $\theta _2$ was negative.</p>
<p><img src="/images/polynomial_regression.png" alt="polynomial_regression"></p>
</li>
</ul>
</li>
<li>One important thing to keep in mind is, if you choose your features this way then <strong>feature scaling becomes very important</strong>.<ul>
<li>E.g. if $x_1$ has range $1 - 1000$ then range of $x^2_1$ becomes $1 - 1000000$ and that of $x^3_1$ becomes $1 - 1000000000$</li>
<li>So you should scale $x_1$ before using polynomial regression.</li>
</ul>
</li>
</ul>
<h2 id="Computing-Parameters-Analytically"><a href="#Computing-Parameters-Analytically" class="headerlink" title="Computing Parameters Analytically"></a>Computing Parameters Analytically</h2><h3 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a>Normal Equation</h3><p>The “Normal Equation” (正规方程) is a method of finding the optimum $\theta$ <strong>without iteration.</strong></p>
<blockquote>
<p>There is <strong>no need</strong> to do feature scaling with the normal equation.</p>
</blockquote>
<h4 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h4><ul>
<li>$\theta \in R^{n+1} , J(\theta _0, \theta _1, \dots , \theta_m) = \frac{1}{2m} \sum ^m_{i=1} \left( h_\theta (x^{(i)}) - y^{(i)} \right) ^2$</li>
<li>Set $\frac{\partial }{\partial \theta _j} J(\theta ) = \cdots = 0$ (for every $j$), solve for $\theta _0, \theta _1, \dots , \theta _m$</li>
</ul>
<h4 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h4><p>We have $m$ examples $(x^{(1)}, y^{(1)}), \dots , (x^{(m)}, y^{(m)})$ and $n$ features. (Note that $x^{(i)}_0 = 0$)</p>
<p>$$x^{(i)} = \begin{bmatrix}x^{(i)}_0 \\ x^{(i)}_1 \\ x^{(i)}_2 \\ \vdots \\ x^{(i)}_n \end{bmatrix}$$</p>
<p>And construct the $m \times (n+1)$ matrix $X$</p>
<p>$$X = \begin{bmatrix} (x^{(1)})^T \\ (x^{(2)})^T \\ \vdots \\ (x^{(m)})^T \end{bmatrix}$$</p>
<p>And the $m$-dimension vector $y$</p>
<p>$$y = \begin{bmatrix}y^{(i)} \\ y^{(i)} \\ y^{(i)} \\ \vdots \\ y^{(m)} \end{bmatrix}$$</p>
<p>Finally, we can get</p>
<p>$$ \theta = (X^T X)^{-1}X^T y $$</p>
<h4 id="Example-1"><a href="#Example-1" class="headerlink" title="Example"></a>Example</h4><p>Suppose you have the training in the table below:</p>
<table>
<thead>
<tr>
<th style="text-align:center">age ($x_1$)</th>
<th style="text-align:center">height in cm ($x_2$)</th>
<th style="text-align:center">weight in kg ($y$)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">89</td>
<td style="text-align:center">16</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">124</td>
<td style="text-align:center">28</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">103</td>
<td style="text-align:center">20</td>
</tr>
</tbody>
</table>
<p>You would like to predict a child’s weight as a function of his age and height with the model</p>
<p>$$weight = \theta _0 + \theta _1 age + \theta _2 height$$</p>
<p>Then you can construct $X$ and $y$</p>
<p>$$X = \begin{bmatrix} 1 &amp; 4 &amp; 89 \\ 1 &amp; 9 &amp; 124 \\ 1 &amp; 5 &amp; 103 \end{bmatrix}$$</p>
<p>$$Y = \begin{bmatrix} 16 \\ 28 \\ 20 \end{bmatrix}$$</p>
<h4 id="Usage-in-Octave"><a href="#Usage-in-Octave" class="headerlink" title="Usage in Octave"></a>Usage in Octave</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pinv (X&apos;*X)*X&apos;*y</div></pre></td></tr></table></figure>
<h4 id="Comparison-of-gradient-descent-and-the-normal-equation"><a href="#Comparison-of-gradient-descent-and-the-normal-equation" class="headerlink" title="Comparison of gradient descent and the normal equation"></a>Comparison of gradient descent and the normal equation</h4><p>$m$ training examples and $n$ features.</p>
<table>
<thead>
<tr>
<th>Gradient Descent</th>
<th>Normal Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Need to choose $\alpha$</td>
<td>No need to choose $\alpha$</td>
</tr>
<tr>
<td>Needs many iterations</td>
<td>No need to iterate</td>
</tr>
<tr>
<td>$O (kn^2)$</td>
<td>$O (n^3)$, need to calculate  $(X^TX)^{-1}$</td>
</tr>
<tr>
<td>Works well when $n$ is large</td>
<td>Slow if $n$ is very large</td>
</tr>
</tbody>
</table>
<p>With the normal equation, computing the inversion has complexity $O(n^3)$. So if we have a very large number of features, the normal equation will be slow. In practice, <strong>when $n$ exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.</strong></p>
<h3 id="Normal-Equation-Noninvertibility"><a href="#Normal-Equation-Noninvertibility" class="headerlink" title="Normal Equation Noninvertibility"></a>Normal Equation Noninvertibility</h3><p>$$ \theta = (X^T X)^{-1}X^T y $$</p>
<ul>
<li>What if $X^TX$ is non-invertible (不可逆的) ? (singular/ degenerate)</li>
<li>Octave: <code>pinv(X&#39;*X)*X&quot;*y</code><ul>
<li>There’s two functions in Octave for inverting matrices, <code>pinv</code> (pseudo-inverse, 伪逆) and <code>inv</code> (inverse).</li>
<li>As long as you use the <code>pinv</code> function then this will actually compute the value of data that you want even if X transpose X is non-invertible.</li>
<li>So when implementing the normal equation in octave we want to use the <code>pinv</code> function rather than <code>inv</code>.</li>
</ul>
</li>
<li>$X^TX$ may be <strong>noninvertible</strong>. The common causes are:<ul>
<li>Redundant features, where two features are very closely related (i.e. they are linearly dependent)<ul>
<li>E.g. $x_1$ = size in $feet^2$, and $x_2$ = size in $m^2$. So you’ll always have $x_1 = (3.28)^2 x_2$</li>
</ul>
</li>
<li>Too many features (e.g. $m\le n$). <ul>
<li>In this case, delete some features or use “regularization”.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="Octave-Matlab-Tutorial"><a href="#Octave-Matlab-Tutorial" class="headerlink" title="Octave/Matlab Tutorial"></a>Octave/Matlab Tutorial</h1><h2 id="Basic-Operations"><a href="#Basic-Operations" class="headerlink" title="Basic Operations"></a>Basic Operations</h2><ul>
<li>Print specific decimals: <code>disp(sprintf(&#39;6 decimals: %0.6f&#39;, a)) // 6 decimals: 3.141593</code></li>
<li><code>v = 1:0.2:2 // [1.0 1.2 1.4 1.6 1.8 2.0]</code></li>
<li><code>ones</code>, <code>zeros</code>, <code>rand</code>, <code>randn</code> (生成正态分布的随机数矩阵), <code>eye</code> (生成单位矩阵)</li>
<li><code>hist</code> (直方图，第二个参数课自定义条数)</li>
<li><code>size</code> (返回矩阵的行数与列数 [m n] )</li>
<li><code>length</code> (返回向量的维数)</li>
</ul>
<h2 id="Moving-Data-Around"><a href="#Moving-Data-Around" class="headerlink" title="Moving Data Around"></a>Moving Data Around</h2><ul>
<li><p>Use <code>load</code> to load data set.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">load featureX.dat</div><div class="line">load(&apos;priceY.dat&apos;)</div></pre></td></tr></table></figure>
</li>
<li><p>Use <code>who</code> to show all variables in Octave workspace</p>
<ul>
<li><p><code>whos</code> for detail information</p>
</li>
<li><p><code>clear</code> to delete a variable</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">clear featureX</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Get first ten elements of a matrix</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">v = priceY(1:10)</div></pre></td></tr></table></figure>
</li>
<li><p>Use <code>save</code> to save your variable</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">save hello.mat v</div></pre></td></tr></table></figure>
<ul>
<li><p>By default the data is saved in binary. You can save it to ASCII by</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">save hello.txt v -ascii</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Use <code>A(3, 2)</code> to get $A_{32}$, or <code>A(2, :)</code> to get every element along the second row</p>
<ul>
<li><code>A([1, 3], :)</code> to get everything in the first and third rows</li>
<li><code>A(:, 2) = [10; 11; 12]</code> to change the value of elements in second column.</li>
<li><code>A = [A, [100; 101; 102]]</code> to append another column vector to right</li>
<li><code>A(:)</code> to put all elements of $A$ into a single vector</li>
</ul>
</li>
</ul>
<h2 id="Computing-on-Data"><a href="#Computing-on-Data" class="headerlink" title="Computing on Data"></a>Computing on Data</h2><ul>
<li><p>Use <code>max</code> to get the largest element in a vector</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a = [1 15 2 0.5];</div><div class="line">[val, ind] = max(a); // val = 15, ind = 2</div></pre></td></tr></table></figure>
<ul>
<li><p>If you do <code>max(A)</code>, where $A$ is a matrix, what this does is this actually does the column wise maximum.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">A = [1 2; 3 4; 5 6];</div><div class="line">max(A) // [5 6]</div><div class="line"></div><div class="line">A = [8 1 6; 3 5 7; 4 9 2];</div><div class="line">max(A, [], 1) // [8 9 7] (get the column wise maximum)</div><div class="line">max(A, [], 2) // [8 7 9] (get the row wise maximum)</div><div class="line">max(max(A)) // 9</div><div class="line">max(A(:)) // 9</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><code>a &lt; 3</code>  does the element wise operation, you’ll get <code>[1 0 1 1]</code></p>
<ul>
<li><code>find(a&lt;3)</code> gets <code>[1 3 4]</code></li>
</ul>
</li>
<li><p><code>magic(3)</code> gets a 3x3 magic matrix</p>
</li>
<li><p><code>sum</code>, <code>prod</code>, <code>floor</code>, <code>ceil</code>, <code>flipud</code></p>
</li>
</ul>
<h2 id="Plotting-Data"><a href="#Plotting-Data" class="headerlink" title="Plotting Data"></a>Plotting Data</h2><ul>
<li><code>plot</code></li>
<li><code>hold on</code>, <code>figure</code>, <code>subplot</code></li>
<li><code>xlabel</code>, <code>ylabel</code>, <code>legend</code>, <code>title</code>, <code>axis</code></li>
<li><code>print -dpng &#39;myPlot.png&#39;</code></li>
<li><code>imagesc(A)</code> to visualize a matrix<ul>
<li><code>imagesc(A), colorer, colormap gray</code> to be in gray scale.</li>
</ul>
</li>
</ul>
<h2 id="Control-Statements-for-while-if-statement"><a href="#Control-Statements-for-while-if-statement" class="headerlink" title="Control Statements: for, while, if statement"></a>Control Statements: for, while, if statement</h2><h2 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h2><p>Vectorization is the process of taking code that relies on <strong>loops</strong> and converting it into <strong>matrix operations</strong>. It is more efficient, more elegant, and more concise.</p>
<p>As an example, let’s compute our prediction from a hypothesis. Theta is the vector of fields for the hypothesis and x is a vector of variables.</p>
<p>With loops ($h_\theta (x) =\theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n$):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">prediction = 0.0;</div><div class="line">for j = 1:n+1,</div><div class="line">  prediction += theta(j) * x(j);</div><div class="line">end;</div></pre></td></tr></table></figure>
<p>With vectorization ($h_\theta (x) = \theta^T x$):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">prediction = theta&apos; * x;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Linear-Regression-with-Multiple-Variables&quot;&gt;&lt;a href=&quot;#Linear-Regression-with-Multiple-Variables&quot; class=&quot;headerlink&quot; title=&quot;Linear Regression with Multiple Variables&quot;&gt;&lt;/a&gt;Linear Regression with Multiple Variables&lt;/h1&gt;&lt;h2 id=&quot;Multivariate-Linear-Regression&quot;&gt;&lt;a href=&quot;#Multivariate-Linear-Regression&quot; class=&quot;headerlink&quot; title=&quot;Multivariate Linear Regression&quot;&gt;&lt;/a&gt;Multivariate Linear Regression&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Multiple features (variables)&lt;ul&gt;
&lt;li&gt;$n$ = number of features&lt;/li&gt;
&lt;li&gt;$x^{(i)}$ = input (features) of $i^{th}$ training example.&lt;/li&gt;
&lt;li&gt;$x^{(i)}_j$ = value of feature $j$ in $i^{th}$ training example.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hypotesis&lt;ul&gt;
&lt;li&gt;Previously: $h_\theta (x) = \theta_0 + \theta_1 x$&lt;/li&gt;
&lt;li&gt;$h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n$&lt;ul&gt;
&lt;li&gt;For convenience of notation, define $x_0=1$&lt;/li&gt;
&lt;li&gt;$x=\begin{bmatrix}x_0 \\ x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}, \theta = \begin{bmatrix}\theta_0 \\ \theta_1 \\ \theta_2 \\ \vdots \\ \theta_n \end{bmatrix}, h_\theta (x) = \theta^T x​$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Coursera" scheme="http://www.yuthon.com/tags/Coursera/"/>
    
      <category term="Machine Learning" scheme="http://www.yuthon.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Notes for Machine Learning - Week 1</title>
    <link href="http://www.yuthon.com/2016/07/26/Coursera-Machine-Learning-Week-1/"/>
    <id>http://www.yuthon.com/2016/07/26/Coursera-Machine-Learning-Week-1/</id>
    <published>2016-07-26T03:55:36.000Z</published>
    <updated>2016-07-29T08:21:27.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linear-Regression-with-One-Variable"><a href="#Linear-Regression-with-One-Variable" class="headerlink" title="Linear Regression with One Variable"></a>Linear Regression with One Variable</h1><h2 id="Model-and-Cost-Function"><a href="#Model-and-Cost-Function" class="headerlink" title="Model and Cost Function"></a>Model and Cost Function</h2><h3 id="Model-Representation"><a href="#Model-Representation" class="headerlink" title="Model Representation"></a>Model Representation</h3><ul>
<li><strong>Supervised Learning (监督学习)</strong>: Given the “right answer” for each example in the data.<ul>
<li><strong>Regression Problem (回归问题)</strong>: Predict real-valued output. </li>
<li><strong>Classification Problem (分类问题)</strong>: Predict discrete-valued output.</li>
</ul>
</li>
<li><strong>Training set (训练集)</strong><ul>
<li><strong>m</strong>: number of training examples</li>
<li><strong>x</strong>‘s: “input” variable / features</li>
<li><strong>y</strong>‘s: “output” variable / “target” variable</li>
<li><strong>$(x, y)$</strong>: one training example</li>
<li><strong>$(x^i, y^i)$</strong>: $i^{th}$ training example</li>
</ul>
</li>
</ul>
<ul>
<li>Training Set -&gt; Learning Algorithm -&gt; <strong>h(hypothesis, 假设)</strong><ul>
<li>h is a function maps from x’s to y’s</li>
<li>e.g. Size of house -&gt; h -&gt; Estimated price</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Linear regression with one variable</strong><ul>
<li>$h_\theta (x) = \theta_0 + \theta_1 x$<ul>
<li>Shorthand: $h(x)$</li>
</ul>
</li>
<li>Or named Univariate linear regression (单变量线性回归)</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><ul>
<li><p><strong>Hypothesis</strong>: $h_\theta (x) = \theta_0 + \theta_1 x$</p>
<ul>
<li>$\theta_i$’s: Parameters (模型参数)</li>
<li>How to choose $\theta_i$’s ?<ul>
<li>Idea: Choose $\theta_0, \theta_1$ so that $h_\theta (x)$ is close to $y$ for our training example $(x,y)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Cost function (代价函数)</strong><ul>
<li>$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m \left(h_\theta(x^{(i)})-y^{(i)}\right)^2$</li>
<li>Sometimes called Square error function (平方误差代价函数)</li>
</ul>
</li>
</ul>
<ul>
<li>Goal: minimise $J(\theta_0, \theta_1)$</li>
</ul>
<h2 id="Parameter-Learning"><a href="#Parameter-Learning" class="headerlink" title="Parameter Learning"></a>Parameter Learning</h2><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><ul>
<li><p><strong>Gradient Descent (梯度下降)</strong></p>
<ul>
<li>Goal<ul>
<li>Have some function $J(\theta_0, \theta_1)$</li>
<li>Want $\theta_0, \theta_1$ of $min J(\theta_0, \theta_1)$</li>
</ul>
</li>
<li>Outline<ul>
<li>Start with some $\theta_0, \theta_1$, usually all set to $0$.</li>
<li>Keep changing $\theta_0, \theta_1$ to reduce $J(\theta_0, \theta_1)$ until we hopefully end up at minimum</li>
</ul>
</li>
</ul>
</li>
<li><p>Gradient descent algorithm</p>
<blockquote>
<p>repeat until convergence (收敛) {<br>​    $\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_j)$  (for $j=0$ and $j=1$)<br>}</p>
</blockquote>
<ul>
<li><p><code>:=</code> denotes assignment</p>
</li>
<li><p>$\alpha$ denotes learning rate</p>
<ul>
<li>if too small, gradient descent can be slow</li>
<li>If too large, gradient descent can overshoot the minimum. It may fail to converge or even diverge.</li>
</ul>
</li>
<li><p>You should <u>simultaneously</u> update $\theta_0$ and $\theta_1$</p>
<ul>
<li><p>That is, you should compute the right-hand sides of $\theta_0$ and $\theta_1$, then save them to temporary variables, and finally update $\theta_0$ and $\theta_1$.</p>
<blockquote>
<p>$temp0 := \theta_0 - \alpha \frac{\partial}{\partial \theta_0} J(\theta_0, \theta_j)$</p>
<p>$temp1 := \theta_1 - \alpha \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_j)$</p>
<p>$\theta_0 := temp0$</p>
<p>$\theta_1 :=temp1$</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h3><ul>
<li><p>If $\theta_1$ at local optima, it leaves $\theta_1$ unchanged.</p>
</li>
<li><p><u>gradient descent can converge to a local minimum, even with the learning rate $\alpha$ fixed.</u></p>
<ul>
<li>As we approach a local minimum, gradient descent will automatically take smaller steps. So, no need to decrease $\alpha$ over time.</li>
</ul>
</li>
</ul>
<h3 id="Gradient-Descent-For-Linear-Regression"><a href="#Gradient-Descent-For-Linear-Regression" class="headerlink" title="Gradient Descent For Linear Regression"></a>Gradient Descent For Linear Regression</h3><p>We can compute that</p>
<p>$\frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1) = \frac{1}{m} \sum^m_{i=1}\left(h_\theta(x^{(i)})-y^{(i)}\right)$</p>
<p>$\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) = \frac{1}{m} \sum^m_{i=1}\left(h_\theta(x^{(i)})-y^{(i)}\right) \cdot x^{(i)}$</p>
<p>Thus the Gradient descent algorithm can be expressed as</p>
<blockquote>
<p> repeat until convergence {<br>   $\theta_0 := \theta_0 - \alpha \frac{1}{m} \sum^m_{i=1}\left(h_\theta(x^{(i)})-y^{(i)}\right)​$</p>
<p>   $\theta_1 := \theta_1 - \alpha \frac{1}{m} \sum^m_{i=1}\left(h_\theta(x^{(i)})-y^{(i)}\right) \cdot x^{(i)}$<br> }</p>
</blockquote>
<p>And the cost funciton of linear refression is always a convex function (凸函数), or called Bowl-shaped function (弓形函数). <u>It doesn’t have any local optima except for the one global optimum.</u></p>
<h4 id="“Batch”-Gradient-Descent"><a href="#“Batch”-Gradient-Descent" class="headerlink" title="“Batch” Gradient Descent"></a>“Batch” Gradient Descent</h4><ul>
<li>The algorithm that we just went over is sometimes called <strong>Batch Gradient Descent (批量梯度下降)</strong>.</li>
<li>“Batch”: Each step of gradient descent uses all th etraining examples.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Linear-Regression-with-One-Variable&quot;&gt;&lt;a href=&quot;#Linear-Regression-with-One-Variable&quot; class=&quot;headerlink&quot; title=&quot;Linear Regression with One Variable&quot;&gt;&lt;/a&gt;Linear Regression with One Variable&lt;/h1&gt;&lt;h2 id=&quot;Model-and-Cost-Function&quot;&gt;&lt;a href=&quot;#Model-and-Cost-Function&quot; class=&quot;headerlink&quot; title=&quot;Model and Cost Function&quot;&gt;&lt;/a&gt;Model and Cost Function&lt;/h2&gt;&lt;h3 id=&quot;Model-Representation&quot;&gt;&lt;a href=&quot;#Model-Representation&quot; class=&quot;headerlink&quot; title=&quot;Model Representation&quot;&gt;&lt;/a&gt;Model Representation&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Supervised Learning (监督学习)&lt;/strong&gt;: Given the “right answer” for each example in the data.&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Regression Problem (回归问题)&lt;/strong&gt;: Predict real-valued output. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Classification Problem (分类问题)&lt;/strong&gt;: Predict discrete-valued output.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training set (训练集)&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;m&lt;/strong&gt;: number of training examples&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;x&lt;/strong&gt;‘s: “input” variable / features&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;y&lt;/strong&gt;‘s: “output” variable / “target” variable&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$(x, y)$&lt;/strong&gt;: one training example&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$(x^i, y^i)$&lt;/strong&gt;: $i^{th}$ training example&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Training Set -&amp;gt; Learning Algorithm -&amp;gt; &lt;strong&gt;h(hypothesis, 假设)&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;h is a function maps from x’s to y’s&lt;/li&gt;
&lt;li&gt;e.g. Size of house -&amp;gt; h -&amp;gt; Estimated price&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear regression with one variable&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;$h_\theta (x) = \theta_0 + \theta_1 x$&lt;ul&gt;
&lt;li&gt;Shorthand: $h(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Or named Univariate linear regression (单变量线性回归)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Coursera" scheme="http://www.yuthon.com/tags/Coursera/"/>
    
      <category term="Machine Learning" scheme="http://www.yuthon.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>OS X 10.11.4 on XPS 15 9550</title>
    <link href="http://www.yuthon.com/2016/05/16/OS-X-10-11-4-on-XPS-15-9550/"/>
    <id>http://www.yuthon.com/2016/05/16/OS-X-10-11-4-on-XPS-15-9550/</id>
    <published>2016-05-16T12:20:08.000Z</published>
    <updated>2016-07-26T03:52:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>Thanks to the <a href="http://www.tonymacx86.com/threads/guide-wip-dell-xps-15-9550-skylake-gtx960m-ssd-via-clover-uefi.192598/" target="_blank" rel="external">guide</a> and its participants, I successfully installed OS X 10.11.4 on my XPS 15 9550.</p>
<p> <img src="/images/OS-X-10-11-4-on-XPS-15-9550.jpg" alt="OS-X-10-11-4-on-XPS-15-9550"></p>
<p>The guide is considerably perfect. Just follow the steps and you’ll get a well-done Hackintosh.</p>
<p>My Clover directory, DSDT &amp; SSDT and Kexts are uploaded to <a href="https://github.com/corenel/XPS9550-OSX" target="_blank" rel="external">Github</a>, for reference only. </p>
<p><strong>Update</strong>: Now I upgrade to 10.11.5, and everything is fine.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Thanks to the &lt;a href=&quot;http://www.tonymacx86.com/threads/guide-wip-dell-xps-15-9550-skylake-gtx960m-ssd-via-clover-uefi.192598/&quot; target=&quot;
    
    </summary>
    
    
      <category term="OS X" scheme="http://www.yuthon.com/tags/OS-X/"/>
    
      <category term="XPS" scheme="http://www.yuthon.com/tags/XPS/"/>
    
  </entry>
  
  <entry>
    <title>Notes for Code School - Shaping up with Angular.js</title>
    <link href="http://www.yuthon.com/2016/04/14/CodeSchool-Shaping-up-with-Angular.js/"/>
    <id>http://www.yuthon.com/2016/04/14/CodeSchool-Shaping-up-with-Angular.js/</id>
    <published>2016-04-14T13:19:22.000Z</published>
    <updated>2016-05-16T11:51:43.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flatlander’s-Gem-Store"><a href="#Flatlander’s-Gem-Store" class="headerlink" title="Flatlander’s Gem Store"></a>Flatlander’s Gem Store</h1><h2 id="Ramp-up"><a href="#Ramp-up" class="headerlink" title="Ramp up"></a>Ramp up</h2><p><strong>Why Angular?</strong></p>
<p>If you?re using JavaScript to create a dynamic website, Angular is a good choice.</p>
<ul>
<li>Angular helps you organize your JavaScript</li>
<li>Angular helps create responsive (as in fast) websites.</li>
<li>Angular plays well with jQuery.</li>
<li>Angular is easy to test.</li>
</ul>
<a id="more"></a>
<p><strong>Traditional Page-Refresh</strong></p>
<p> <img src="/images/traditional-page-refresh.png" alt="traditional-page-refresh"></p>
<p><strong>A “responsive “website using Angular</strong></p>
<p> <img src="/images/a-responsive-website-using-angular.png" alt="a-responsive-website-using-angular"></p>
<p><strong>What is Angular JS?</strong></p>
<p>A client-side JavaScript Framework for adding interactivity to HTML.</p>
<h3 id="Directives"><a href="#Directives" class="headerlink" title="Directives"></a>Directives</h3><p>A Directive is a marker on a HTML tag that tells Angular to run or reference some JavaScript code.</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!-- index.html --&gt;</span></div><div class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">body</span> <span class="attr">ng-controller</span>=<span class="string">"StoreController"</span>&gt;</span></div><div class="line">...</div><div class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></div></pre></td></tr></table></figure>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// app.js</span></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">StoreController</span>(<span class="params"></span>)</span>&#123;</div><div class="line">  alert(<span class="string">'Welcome, Gregg!'</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Modules"><a href="#Modules" class="headerlink" title="Modules"></a>Modules</h3><ul>
<li>Where we write pieces of our Angular application.</li>
</ul>
<ul>
<li>Makes our code more maintainable, testable, and readable.</li>
</ul>
<ul>
<li>Where we define dependencies for our app.</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!-- index.html --&gt;</span></div><div class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">ng-app</span>=<span class="string">"store"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"bootstrap.min.css"</span> /&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"angular.min.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"app.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></div></pre></td></tr></table></figure>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// app.js</span></div><div class="line"><span class="comment">// Application Name and Dependencies</span></div><div class="line"><span class="keyword">var</span> app = angular.module(<span class="string">'store'</span>, [ ]);</div></pre></td></tr></table></figure>
<h3 id="Expressions"><a href="#Expressions" class="headerlink" title="Expressions"></a>Expressions</h3><p>Allow you to insert dynamic values into your HTML.</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!-- Numerical Operations --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span></div><div class="line">  I am &#123;&#123;4 + 6&#125;&#125;</div><div class="line"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line"></div><div class="line"><span class="comment">&lt;!-- String Operations --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span></div><div class="line">  &#123;&#123;"hello" + " you"&#125;&#125;</div><div class="line"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></div></pre></td></tr></table></figure>
<h2 id="Index-HTML-Setup"><a href="#Index-HTML-Setup" class="headerlink" title="Index HTML Setup"></a>Index HTML Setup</h2><h3 id="Controllers"><a href="#Controllers" class="headerlink" title="Controllers"></a>Controllers</h3><p>Controllers are where we define our app?s behavior by defining functions and values.</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// app.js</span></div><div class="line"><span class="comment">// Wrapping your Javascript in a closure is a good habit!</span></div><div class="line">(<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">  <span class="keyword">var</span> app = angular.module(<span class="string">'store'</span>, [ ]);</div><div class="line">  <span class="comment">// Notice that controller is attached to (inside) our app.</span></div><div class="line">  app.controller(<span class="string">'StoreController'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">    <span class="keyword">this</span>.product = gem;</div><div class="line">  &#125;);</div><div class="line">  </div><div class="line">  <span class="keyword">var</span> gem = &#123;</div><div class="line">    name: <span class="string">'Dodecahedron'</span>,</div><div class="line">    price: <span class="number">2.95</span>,</div><div class="line">    description: <span class="string">'. . .'</span>,</div><div class="line">  &#125;.</div><div class="line">&#125;)();</div></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!-- index.html --&gt;</span></div><div class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">ng-app</span>=<span class="string">"store"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"bootstrap.min.css"</span> /&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">ng-controller</span>=<span class="string">"StoreController as store"</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">h1</span>&gt;</span> &#123;&#123;ore.product.name&#125;&#125; <span class="tag">&lt;/<span class="name">h1</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">h2</span>&gt;</span> $&#123;&#123;store.product.price&#125;&#125; <span class="tag">&lt;/<span class="name">h2</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">p</span>&gt;</span> &#123;&#123;store.product.description&#125;&#125; <span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"angular.min.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"app.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></div></pre></td></tr></table></figure>
<h2 id="Using-Built-in-Directives"><a href="#Using-Built-in-Directives" class="headerlink" title="Using Built-in Directives"></a>Using Built-in Directives</h2><h3 id="ng-show-Directive"><a href="#ng-show-Directive" class="headerlink" title="ng-show Directive"></a>ng-show Directive</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!-- index.html --&gt;</span></div><div class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">ng-app</span>=<span class="string">"store"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">head</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"bootstrap.min.css"</span> /&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">head</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">ng-controller</span>=<span class="string">"StoreController as store"</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">h1</span>&gt;</span> &#123;&#123;store.product.name&#125;&#125; <span class="tag">&lt;/<span class="name">h1</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">h2</span>&gt;</span> $&#123;&#123;store.product.price&#125;&#125; <span class="tag">&lt;/<span class="name">h2</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">p</span>&gt;</span> &#123;&#123;store.product.description&#125;&#125; <span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line">      <span class="comment">&lt;!-- Will only show the element if the value of the Expression is</span></div><div class="line">true --&gt;</div><div class="line">      <span class="tag">&lt;<span class="name">button</span> <span class="attr">ng-show</span>=<span class="string">"store.product.canPurchase"</span>&gt;</span> Add to Cart <span class="tag">&lt;/<span class="name">button</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"angular.min.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"app.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></div></pre></td></tr></table></figure>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// app.js</span></div><div class="line"><span class="keyword">var</span> gem = &#123;</div><div class="line">  name: <span class="string">'Dodecahedron'</span>,</div><div class="line">  price: <span class="number">2.95</span>,</div><div class="line">  description: <span class="string">'. . .'</span>,</div><div class="line">&#125;.</div></pre></td></tr></table></figure>
<h3 id="ng-hide-Directive"><a href="#ng-hide-Directive" class="headerlink" title="ng-hide Directive"></a>ng-hide Directive</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!-- index.html --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">body</span> <span class="attr">ng-controller</span>=<span class="string">"StoreController as store"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">ng-hide</span>=<span class="string">"store.product.soldOut"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">h1</span>&gt;</span> &#123;&#123;store.product.name&#125;&#125; <span class="tag">&lt;/<span class="name">h1</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">h2</span>&gt;</span> $&#123;&#123;store.product.price&#125;&#125; <span class="tag">&lt;/<span class="name">h2</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span> &#123;&#123;store.product.description&#125;&#125; <span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">button</span> <span class="attr">ng-show</span>=<span class="string">"store.product.canPurchase"</span>&gt;</span> Add to Cart <span class="tag">&lt;/<span class="name">button</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line">  . . .</div><div class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></div></pre></td></tr></table></figure>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// app.js</span></div><div class="line"><span class="keyword">var</span> gem = &#123;</div><div class="line">  name: <span class="string">'Dodecahedron'</span>,</div><div class="line">  price: <span class="number">2.95</span>,</div><div class="line">  description: <span class="string">'. . .'</span>,</div><div class="line">  canPurchase: <span class="literal">true</span>,</div><div class="line">  <span class="comment">// If the product is sold out, we want to hide it.</span></div><div class="line">  soldOut: <span class="literal">true</span>,</div><div class="line">&#125;.</div></pre></td></tr></table></figure>
<h3 id="ng-repeat-Directive"><a href="#ng-repeat-Directive" class="headerlink" title="ng-repeat Directive"></a>ng-repeat Directive</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!-- index.html --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">body</span> <span class="attr">ng-controller</span>=<span class="string">"StoreController as store"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">ng-repeat</span>=<span class="string">"product in store.products"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">h1</span>&gt;</span> &#123;&#123;product.name&#125;&#125; <span class="tag">&lt;/<span class="name">h1</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">h2</span>&gt;</span> $&#123;&#123;product.price&#125;&#125; <span class="tag">&lt;/<span class="name">h2</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span> &#123;&#123;product.description&#125;&#125; <span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">button</span> <span class="attr">ng-show</span>=<span class="string">"product.canPurchase"</span>&gt;</span>Add to Cart<span class="tag">&lt;/<span class="name">button</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line">  . . .</div><div class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></div></pre></td></tr></table></figure>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// app.js</span></div><div class="line">app.controller(<span class="string">'StoreController'</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">  <span class="keyword">this</span>.products = gems;</div><div class="line">&#125;);</div><div class="line"></div><div class="line"><span class="keyword">var</span> gems = [</div><div class="line">  &#123;</div><div class="line">    name: <span class="string">"Dodecahedron"</span>,</div><div class="line">    price: <span class="number">2.95</span>,</div><div class="line">    description: <span class="string">". . ."</span>,</div><div class="line">    canPurchase: <span class="literal">true</span>,</div><div class="line">  &#125;,</div><div class="line">  &#123;</div><div class="line">    name: <span class="string">"Pentagonal Gem"</span>,</div><div class="line">    price: <span class="number">5.95</span>,</div><div class="line">    description: <span class="string">". . ."</span>,</div><div class="line">    canPurchase: <span class="literal">false</span>,</div><div class="line">  &#125;</div><div class="line">];</div></pre></td></tr></table></figure>
<h1 id="Build-in-Directives"><a href="#Build-in-Directives" class="headerlink" title="Build-in Directives"></a>Build-in Directives</h1><h2 id="Filters-and-a-new-Directive"><a href="#Filters-and-a-new-Directive" class="headerlink" title="Filters and a new Directive"></a>Filters and a new Directive</h2><h3 id="Filters"><a href="#Filters" class="headerlink" title="Filters"></a>Filters</h3>
{{ data | filter:options }}

<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// date</span></div><div class="line">&#123;&#123;<span class="string">'1388123412323'</span> | date:<span class="string">'MM/dd/yyyy @ h:mma'</span>&#125;&#125; <span class="comment">// 12/27/2013 @ 12:50AM</span></div><div class="line"></div><div class="line"><span class="comment">// uppercase &amp; lowercase</span></div><div class="line">&#123;&#123;<span class="string">'octagon gem'</span> | uppercase&#125;&#125; <span class="comment">// OCTAGON GEM</span></div><div class="line"></div><div class="line"><span class="comment">// limitTo</span></div><div class="line">&#123;&#123;<span class="string">'My Description'</span> | limitTo:<span class="number">8</span>&#125;&#125; <span class="comment">// My Descr</span></div><div class="line">&lt;li ng-repeat=<span class="string">"product in store.products | limitTo:3"</span>&gt;</div><div class="line">  </div><div class="line"><span class="comment">// orderBy</span></div><div class="line"><span class="comment">// Will list products by descending price.</span></div><div class="line"><span class="comment">// Without the  - products would list in ascending order.</span></div><div class="line">&lt;li ng-repeat=<span class="string">"product in store.products | orderBy:'-price'"</span>&gt;</div></pre></td></tr></table></figure>
<h3 id="Using-ng-src-for-Images"><a href="#Using-ng-src-for-Images" class="headerlink" title="Using ng-src for Images"></a>Using ng-src for Images</h3><p>Using Angular Expressions inside a src attribute causes an error! Because the browser tries to load the image before the Expression evaluates.</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&lt;body ng-controller=<span class="string">"StoreController as store"</span>&gt;</div><div class="line">  <span class="xml"><span class="tag">&lt;<span class="name">ul</span> <span class="attr">class</span>=<span class="string">"list-group"</span>&gt;</span></span></div><div class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">"list-group-item"</span> <span class="attr">ng-repeat</span>=<span class="string">"product in store.products"</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">h3</span>&gt;</span> </div><div class="line">        &#123;&#123;product.name&#125;&#125;</div><div class="line">        <span class="tag">&lt;<span class="name">em</span> <span class="attr">class</span>=<span class="string">"pull-right"</span>&gt;</span> &#123;&#123;product.price | currency&#125;&#125; <span class="tag">&lt;/<span class="name">em</span>&gt;</span></div><div class="line">        // NG-SOURCEto the rescue!</div><div class="line">        <span class="tag">&lt;<span class="name">img</span> <span class="attr">ng-src</span>=<span class="string">"&#123;&#123;product.images[0].full&#125;&#125;"</span>/&gt;</span></div><div class="line">      <span class="tag">&lt;/<span class="name">h3</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">li</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></div></pre></td></tr></table></figure>
<h2 id="Tabs-Inside-Out"><a href="#Tabs-Inside-Out" class="headerlink" title="Tabs Inside Out"></a>Tabs Inside Out</h2><ul>
<li><code>ng-click</code></li>
<li><code>ng-init</code></li>
<li><code>ng-class</code></li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">section</span> <span class="attr">ng-conrtoller</span>=<span class="string">"PanelController as panel"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">ul</span> <span class="attr">class</span>=<span class="string">"nav nav-pills"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">ng-class</span>=<span class="string">"&#123;active:panel.isSelected(1)&#125;"</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">a</span> <span class="attr">herf</span> <span class="attr">ng-click</span>=<span class="string">"panel.selectTab(1)"</span>&gt;</span>Description<span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">li</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">ng-class</span>=<span class="string">"&#123;active:panel.isSelected(3)&#125;"</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">a</span> <span class="attr">herf</span> <span class="attr">ng-click</span>=<span class="string">"panel.selectTab(1)"</span>&gt;</span>Specifications<span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">li</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">li</span> <span class="attr">ng-class</span>=<span class="string">"&#123;active:panel.isSelected(3)&#125;"</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">a</span> <span class="attr">herf</span> <span class="attr">ng-click</span>=<span class="string">"panel.selectTab(3)"</span>&gt;</span>Reviews<span class="tag">&lt;/<span class="name">a</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">li</span>&gt;</span> </div><div class="line">  <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"panel"</span> <span class="attr">ng-show</span>=<span class="string">"panel.isSelected(1)"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">h4</span>&gt;</span>Description <span class="tag">&lt;/<span class="name">h4</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">p</span>&gt;</span>&#123;&#123;product.description&#125;&#125;<span class="tag">&lt;/<span class="name">p</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">section</span>&gt;</span></div></pre></td></tr></table></figure>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">app.controller(<span class="string">"PanelController"</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">  <span class="keyword">this</span>.tab = <span class="number">1</span>;</div><div class="line">  <span class="keyword">this</span>.selectTab = <span class="function"><span class="keyword">function</span>(<span class="params">setTab</span>) </span>&#123;</div><div class="line">    <span class="keyword">this</span>.tab = setTab;</div><div class="line">  &#125;;</div><div class="line">  <span class="keyword">this</span>.isSelected = <span class="function"><span class="keyword">function</span>(<span class="params">checkTab</span>)</span>&#123;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.tab === checkTab;</div><div class="line">  &#125;;</div><div class="line">&#125;);</div></pre></td></tr></table></figure>
<h1 id="Forms-Models-and-Validations"><a href="#Forms-Models-and-Validations" class="headerlink" title="Forms, Models, and Validations"></a>Forms, Models, and Validations</h1><h2 id="Forms-and-Models"><a href="#Forms-and-Models" class="headerlink" title="Forms and Models"></a>Forms and Models</h2><h3 id="Introducing-ng-model"><a href="#Introducing-ng-model" class="headerlink" title="Introducing ng-model"></a>Introducing ng-model</h3><p><code>ng-model</code> binds the form element value to the property.</p>
<p>So we can have live previrew.</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">name</span>=<span class="string">"reviewForm"</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">blockquote</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">b</span>&gt;</span>Stars: &#123;&#123;review.stars&#125;&#125;<span class="tag">&lt;/<span class="name">b</span>&gt;</span></div><div class="line">    &#123;&#123;review.body&#125;&#125; </div><div class="line">    <span class="tag">&lt;<span class="name">cite</span>&gt;</span>by: &#123;&#123;review.author&#125;&#125;<span class="tag">&lt;/<span class="name">cite</span>&gt;</span> </div><div class="line">  <span class="tag">&lt;/<span class="name">blockquote</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">select</span> <span class="attr">ng-model</span>=<span class="string">"review.stars"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">option</span> <span class="attr">value</span>=<span class="string">"1"</span>&gt;</span>1 star<span class="tag">&lt;/<span class="name">option</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">option</span> <span class="attr">value</span>=<span class="string">"2"</span>&gt;</span>2 stars<span class="tag">&lt;/<span class="name">option</span>&gt;</span></div><div class="line">    . . .</div><div class="line">  <span class="tag">&lt;/<span class="name">select</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">textarea</span> &gt;</span><span class="tag">&lt;/<span class="name">textarea</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">label</span>&gt;</span>by:<span class="tag">&lt;/<span class="name">label</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">input</span> <span class="attr">ng-model</span>=<span class="string">"review.body"</span> <span class="attr">type</span>=<span class="string">"email"</span> /&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">input</span> <span class="attr">ng-model</span>=<span class="string">"review.author"</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"Submit"</span> /&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></div></pre></td></tr></table></figure>
<p><strong>Two More Binding Examples</strong></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!-- With a Checkbox --&gt;</span></div><div class="line"><span class="comment">&lt;!-- Sets value to true or false --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">ng-model</span>=<span class="string">"review.terms"</span> <span class="attr">type</span>=<span class="string">"checkbox"</span> /&gt;</span> I agree to the terms</div><div class="line"></div><div class="line"><span class="comment">&lt;!-- With Radio Buttons --&gt;</span></div><div class="line"><span class="comment">&lt;!-- Sets the proper value based on which is selected --&gt;</span></div><div class="line">What color would you like? </div><div class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">ng-model</span>=<span class="string">"review.color"</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">value</span>=<span class="string">"red"</span> /&gt;</span> Red </div><div class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">ng-model</span>=<span class="string">"review.color"</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">value</span>=<span class="string">"blue"</span> /&gt;</span> Blue </div><div class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">ng-model</span>=<span class="string">"review.color"</span> <span class="attr">type</span>=<span class="string">"radio"</span> <span class="attr">value</span>=<span class="string">"green"</span> /&gt;</span> Green</div></pre></td></tr></table></figure>
<h2 id="Accepting-Submissions"><a href="#Accepting-Submissions" class="headerlink" title="Accepting Submissions"></a>Accepting Submissions</h2><p><code>ng-submit</code> directive.</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">app.controller(<span class="string">"ReviewController"</span>, <span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;</div><div class="line">  <span class="keyword">this</span>.review = &#123;&#125;;</div><div class="line">  <span class="keyword">this</span>.addReview = <span class="function"><span class="keyword">function</span>(<span class="params">product</span>) </span>&#123;</div><div class="line">    product.reviews.push(<span class="keyword">this</span>.review);</div><div class="line">    <span class="keyword">this</span>.review = &#123;&#125;;</div><div class="line">  &#125;;</div><div class="line">&#125;);</div></pre></td></tr></table></figure>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">name</span>=<span class="string">"reviewForm"</span> <span class="attr">ng-controller</span>=<span class="string">"ReviewController as reviewCtrl"</span></span></div><div class="line"><span class="attr">ng-submit</span>=<span class="string">"reviewCtrl.addReview(product)"</span>&gt;</div><div class="line">  <span class="tag">&lt;<span class="name">blockquote</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">b</span>&gt;</span>Stars: &#123;&#123;reviewCtrl.review.stars&#125;&#125;<span class="tag">&lt;/<span class="name">b</span>&gt;</span></div><div class="line">    &#123;&#123;reviewCtrl.review.body&#125;&#125;</div><div class="line">    <span class="tag">&lt;<span class="name">cite</span>&gt;</span>by: &#123;&#123;reviewCtrl.review.author&#125;&#125;<span class="tag">&lt;/<span class="name">cite</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">blockquote</span>&gt;</span></div></pre></td></tr></table></figure>
<h2 id="Form-Validations-101"><a href="#Form-Validations-101" class="headerlink" title="Form Validations 101"></a>Form Validations 101</h2><p>We don?t want the form to submit when it?s invalid.</p>
<p><strong>Turn Off Default HTML Validation</strong></p>
<ul>
<li><code>novalidate</code>: Turn Off Default HTML Validation</li>
<li><code>required</code>: Mark Required Fields</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">name</span>=<span class="string">"reviewForm"</span> <span class="attr">ng-controller</span>=<span class="string">"ReviewController as reviewCtrl"</span></span></div><div class="line"><span class="attr">ng-submit</span>=<span class="string">"reviewCtrl.addReview(product)"</span> &gt;</div><div class="line">  <span class="tag">&lt;<span class="name">select</span> <span class="attr">ng-model</span>=<span class="string">"reviewCtrl.review.stars"</span> <span class="attr">required</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">option</span> <span class="attr">value</span>=<span class="string">"1"</span>&gt;</span>1 star<span class="tag">&lt;/<span class="name">option</span>&gt;</span></div><div class="line">    ...</div><div class="line">  <span class="tag">&lt;/<span class="name">select</span>&gt;</span></div><div class="line">  </div><div class="line">  <span class="tag">&lt;<span class="name">textarea</span> <span class="attr">name</span>=<span class="string">"body"</span> <span class="attr">ng-model</span>=<span class="string">"reviewCtrl.review.body"</span> <span class="attr">required</span>&gt;</span><span class="tag">&lt;/<span class="name">textarea</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">label</span>&gt;</span>by:<span class="tag">&lt;/<span class="name">label</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">input</span> <span class="attr">name</span>=<span class="string">"author"</span> <span class="attr">ng-model</span>=<span class="string">"reviewCtrl.review.author"</span> <span class="attr">type</span>=<span class="string">"email"</span> <span class="attr">required</span>/&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">div</span>&gt;</span> reviewForm is &#123;&#123;reviewForm.$valid&#125;&#125; <span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"Submit"</span> /&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></div></pre></td></tr></table></figure>
<p><strong>Preventing the Submit</strong></p>
<p>If valid is <code>false</code> , then <code>addReview</code> is never called.</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">name</span>=<span class="string">"reviewForm"</span> <span class="attr">ng-controller</span>=<span class="string">"ReviewController as reviewCtrl"</span></span></div><div class="line"><span class="attr">ng-submit</span>=<span class="string">"reviewForm.$valid &amp;&amp; reviewCtrl.addReview(product)"</span> <span class="attr">novalidate</span>&gt;</div></pre></td></tr></table></figure>
<p><strong>Doesn?t Submit an Invalid Form</strong></p>
<p>How might we give a hint to the user why their form is invalid?</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">name</span>=<span class="string">"author"</span> <span class="attr">ng-model</span>=<span class="string">"reviewCtrl.review.author"</span> <span class="attr">type</span>=<span class="string">"email"</span> <span class="attr">required</span> /&gt;</span></div></pre></td></tr></table></figure>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="selector-class">.ng-invalid</span><span class="selector-class">.ng-dirty</span> &#123;</div><div class="line">  <span class="attribute">border-color</span>: <span class="number">#FA787E</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="selector-class">.ng-valid</span><span class="selector-class">.ng-dirty</span> &#123;</div><div class="line">  <span class="attribute">border-color</span>: <span class="number">#78FA89</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li><p>Source before typing email</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">name</span>=<span class="string">"author"</span> <span class="attr">.</span> <span class="attr">.</span> <span class="attr">.</span> <span class="attr">class</span>=<span class="string">"ng-pristine ng-invalid"</span>&gt;</span></div></pre></td></tr></table></figure>
</li>
<li><p>Source after typing, with invalid email</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">name</span>=<span class="string">"author"</span><span class="attr">.</span> <span class="attr">.</span> <span class="attr">.</span> <span class="attr">class</span>=<span class="string">"ng-dirty ng-invalid"</span>&gt;</span></div></pre></td></tr></table></figure>
</li>
<li><p>Source after typing, with valid email</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">name</span>=<span class="string">"author"</span> <span class="attr">.</span> <span class="attr">.</span> <span class="attr">.</span> <span class="attr">class</span>=<span class="string">"ng-dirty ng-valid"</span>&gt;</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>HTML5-based type validations</strong></p>
<p>Web forms usually have rules around valid input:</p>
<ul>
<li>Angular JS has built-in validations for common input types:</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"email"</span> <span class="attr">name</span>=<span class="string">"email"</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"url"</span> <span class="attr">name</span>=<span class="string">"homepage"</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"number"</span> <span class="attr">min</span>=<span class="string">1</span> <span class="attr">max</span>=<span class="string">10</span> <span class="attr">name</span>=<span class="string">"quantity"</span>&gt;</span></div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Flatlander’s-Gem-Store&quot;&gt;&lt;a href=&quot;#Flatlander’s-Gem-Store&quot; class=&quot;headerlink&quot; title=&quot;Flatlander’s Gem Store&quot;&gt;&lt;/a&gt;Flatlander’s Gem Store&lt;/h1&gt;&lt;h2 id=&quot;Ramp-up&quot;&gt;&lt;a href=&quot;#Ramp-up&quot; class=&quot;headerlink&quot; title=&quot;Ramp up&quot;&gt;&lt;/a&gt;Ramp up&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Why Angular?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you?re using JavaScript to create a dynamic website, Angular is a good choice.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Angular helps you organize your JavaScript&lt;/li&gt;
&lt;li&gt;Angular helps create responsive (as in fast) websites.&lt;/li&gt;
&lt;li&gt;Angular plays well with jQuery.&lt;/li&gt;
&lt;li&gt;Angular is easy to test.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="CodeSchool" scheme="http://www.yuthon.com/tags/CodeSchool/"/>
    
      <category term="JavaScript" scheme="http://www.yuthon.com/tags/JavaScript/"/>
    
  </entry>
  
</feed>
