<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yuthon&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.yuthon.com/"/>
  <updated>2017-10-10T14:31:39.000Z</updated>
  <id>http://www.yuthon.com/</id>
  
  <author>
    <name>Yusu Pan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TorchSharp - A Sharp Knife for PyTorch</title>
    <link href="http://www.yuthon.com/2017/10/10/TorchSharp-A-Sharp-Knife-for-PyTorch/"/>
    <id>http://www.yuthon.com/2017/10/10/TorchSharp-A-Sharp-Knife-for-PyTorch/</id>
    <published>2017-10-10T13:53:36.000Z</published>
    <updated>2017-10-10T14:31:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>Recently I code several projects with PyTorch, and find it really a light-weight and easy-to-use deep learning framework. After coding thousands of lines, a thought emerges in my mind about what we can do for improving code re-use and accelerating programming. That’s why I try to introduce <a href="https://github.com/corenel/torchsharp" target="_blank" rel="external"><em>torchsharp</em></a>, a sharp knife for PyTorch.</p>
<p>Note that this package is still <strong>under early development</strong>, and I’ll add features continously in future. Issues ans Pull Requests are extremely welcomed.</p>
<a id="more"></a>
<h2 id="TorchSharp"><a href="#TorchSharp" class="headerlink" title="TorchSharp"></a>TorchSharp</h2><p><a href="https://github.com/corenel/torchsharp" target="_blank" rel="external"><em>torchsharp</em></a> is a framework for PyTorch which provides a set of sharp utilities aiming at speeding up programming and encouraging code re-use. The repository consists of:</p>
<ul>
<li><a href="https://github.com/corenel/torchsharp#data" target="_blank" rel="external">torchsharp.data</a> : Useful stuff about data operation such as dummy datasets and  image tranforms for data argumentation, etc.</li>
<li><a href="https://github.com/corenel/torchsharp#data" target="_blank" rel="external">torchsharp.model</a> : Helpful fucntions about model training process such as initializer and metrics, etc.</li>
<li><a href="https://github.com/corenel/torchsharp#data" target="_blank" rel="external">torchsharp.utils</a> : Other tools like logger and timer, etc.</li>
</ul>
<p>Apart from <em>torchsharp</em>, there’re also two auxiliary libraries for PyTorch - <code>torchzoo</code> and <code>pytorch-starter-kit</code>.</p>
<h2 id="TorchZoo"><a href="#TorchZoo" class="headerlink" title="TorchZoo"></a>TorchZoo</h2><p><a href="https://github.com/corenel/torchzoo" target="_blank" rel="external"><em>torchzoo</em></a> is a zoo of models and datasets for PyTorch. Most of them are used frequently in my research life but not provided by <code>torchvision</code>.</p>
<p>This repository consists of:</p>
<ul>
<li><a href="#datasets">torchzoo.datasets</a> : Data loaders for popular vision datasets.</li>
<li><a href="#models">torchzoo.models</a> : Definitions for popular model architectures.</li>
</ul>
<h2 id="PyTorch-Starter-Kit"><a href="#PyTorch-Starter-Kit" class="headerlink" title="PyTorch Starter Kit"></a>PyTorch Starter Kit</h2><p><a href="https://github.com/corenel/pytorch-starter-kit" target="_blank" rel="external"><em>pytorch-starter-kit</em></a> is a demo project and quick starter kit for PyTorch.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Recently I code several projects with PyTorch, and find it really a light-weight and easy-to-use deep learning framework. After coding thousands of lines, a thought emerges in my mind about what we can do for improving code re-use and accelerating programming. That’s why I try to introduce &lt;a href=&quot;https://github.com/corenel/torchsharp&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;em&gt;torchsharp&lt;/em&gt;&lt;/a&gt;, a sharp knife for PyTorch.&lt;/p&gt;
&lt;p&gt;Note that this package is still &lt;strong&gt;under early development&lt;/strong&gt;, and I’ll add features continously in future. Issues ans Pull Requests are extremely welcomed.&lt;/p&gt;
    
    </summary>
    
      <category term="Projects" scheme="http://www.yuthon.com/categories/Projects/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="PyTorch" scheme="http://www.yuthon.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Notes for Adversarial Discriminative Domain Adaptation</title>
    <link href="http://www.yuthon.com/2017/08/15/Notes-for-Adversarial-Discriminative-Domain-Adaptation/"/>
    <id>http://www.yuthon.com/2017/08/15/Notes-for-Adversarial-Discriminative-Domain-Adaptation/</id>
    <published>2017-08-15T12:19:46.000Z</published>
    <updated>2017-08-22T07:50:56.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About-this-paper"><a href="#About-this-paper" class="headerlink" title="About this paper"></a>About this paper</h2><ul>
<li><strong>Title</strong>: Adversarial Discriminative Domain Adaptation</li>
<li><strong>Authors</strong>: Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell</li>
<li><strong>Topic</strong>: Domain Adaptation</li>
<li><strong>From</strong>: <a href="https://arxiv.org/abs/1702.05464" target="_blank" rel="external">arXiv:1702.05464</a>, appearing in CVPR 2017</li>
</ul>
<h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ul>
<li>将之前的论文里提到的一些方法，例如weight sharing、base models、adversarial loss等，归入了统一的框架之中，并进行了测试；</li>
<li>提出了一种新的框架ADDA，主要思想是不做分类器的自适应，而是设法将目标域的数据映射到域源域差不多的特征空间上，这样就能够复用源域的分类器。</li>
</ul>
<a id="more"></a>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="Generalized-architecture-for-adversarial-domain-adaptation"><a href="#Generalized-architecture-for-adversarial-domain-adaptation" class="headerlink" title="Generalized architecture for adversarial domain adaptation"></a>Generalized architecture for adversarial domain adaptation</h3><p>如下图所示，文中提出的用于对抗性领域迁移的统一框架，主要是其中有三个可以做选择的地方：</p>
<p><img src="/images/ADDA_design_choices.png" alt="ADDA_design_choices"></p>
<ul>
<li><p><strong>对于源映射（source mapping）与目标映射（target mapping）采用何种参数化的模型的选择。</strong>具体地说，是用判别式模型还是生成式模型。早期的领域自适应的方法基本上就是做判别模型的自适应。但是随着GANs兴起，有研究者开始用GANs的生成器通过随机噪声生成采样，然后用判别器中间层的特征作为分类器的输入特征，来训练特定任务的分类器。</p>
</li>
<li><p><strong>对于目标映射参数初始化与参数约束的选择。</strong>一般来说都是用源映射的参数来初始化，但是在后续训练中如何约束两者之间参数的关系，使得在映射后两个域的数据之间距离能尽可能的小，这就见仁见智了。</p>
<ul>
<li>一个常见的约束方法是期望两者逐层的参数相等，在CNN上可以用weight sharing来实现。这种对称的变换能减少模型中的参数数量，并且保证至少在源域上，目标映射是可判别的。不过也有坏处，如此一来一个映射需要能工作在两个独立的域上，这可能会在优化过程中导致病态条件（poorly conditioned）。</li>
<li>另外一个常见的约束方法是万事不管，根本不约束。</li>
<li>此外，还有种非对称变换的方法，只对某些层的参数施加约束。</li>
</ul>
</li>
<li><p><strong>对抗学习损失函数的选择。</strong>文中列举了三种不同的损失函数：</p>
<ul>
<li><p>第一个是直接取判别器的相反数，这一点在GAN论文中提到过，会导致判别器收敛已经收敛时生成器梯度消失的问题，因此不予选用。<br>$$<br>\mathcal{L} _{adv _ M} = - \mathcal{L} _{adv _ D}<br>$$</p>
</li>
<li><p>第二个是采用GAN论文中的非饱和的损失函数，采用判别器误判的期望来作为损失函数。<br>$$<br>\mathcal{L} _{adv _M} (X_s, X_t, D) = - \mathbb{E} _{x_t,\sim X_t} [\log D(M_t (x_t))]<br>$$</p>
</li>
<li><p>第三个则是作者在前一篇文章中提出来的domain confusion loss：<br>$$<br>\mathcal{L} _{adv _M} (X_s, X_t, D) = - \sum _{d\in {s, t}} \mathbb{E} _{x_d \sim X_d} \left[ \frac{1}{2} \log D (M_d (x_d)) + \frac{1}{2} \log (1 - D (M_d (x_d))) \right]<br>$$</p>
</li>
</ul>
</li>
</ul>
<h3 id="ADDA-architecture"><a href="#ADDA-architecture" class="headerlink" title="ADDA architecture"></a>ADDA architecture</h3><p>对于上述统一框架中，ADDA的选择如下：</p>
<ul>
<li><strong>首先，ADDA选择了使用判别模型</strong>，因为作者觉得用生成模型生成领域内的样本所训练出来的参数，很大一部分对于判别器的自适应任务来说并没有什么用。另外，之前的文章有很多是直接在判别的空间上做自适应，这对于两个很相似的域来说是可以的（比如MNIST与USPS），但是对于一些不那么相似的域之间（比如MNIST与SVHN），也就是所谓的“困难任务”上，这么做就不一定能够收敛了。</li>
<li><strong>其次，对于源映射域目标映射，ADDA选择不对两者的参数关系作出约束。</strong>当然，为了让目标映射快速收敛起见，ADDA使用了源映射的参数来初始化目标映射模型。</li>
<li><strong>最后，ADDA使用了非饱和的GANs损失函数。</strong></li>
</ul>
<p>如下图所示，整个ADDA的工作流程可以分为三个步骤：</p>
<p><img src="/images/ADDA_overview.png" alt="ADDA_overview"></p>
<ul>
<li>首先，使用带标记的源数据$(X_s, Y_s)$来训练源映射$M_s$与分类器$C$，优化$\mathcal{L}_{cls}$；</li>
<li>其次，固定$M_s$，使用GANs来训练$M_t$，优化$\mathcal{L}_{adv_D}$与$\mathcal{L}_{adv_M}$；</li>
<li>最后，使用$M_t$，将目标域的数据变换到特征空间，并交由分类器$C$来分类。</li>
</ul>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><ul>
<li><a href="https://github.com/erictzeng/adda" target="_blank" rel="external">code from authors (TensorFlow)</a></li>
<li><a href="https://github.com/corenel/pytorch-adda" target="_blank" rel="external">my implementation (PyTorch)</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;About-this-paper&quot;&gt;&lt;a href=&quot;#About-this-paper&quot; class=&quot;headerlink&quot; title=&quot;About this paper&quot;&gt;&lt;/a&gt;About this paper&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Title&lt;/strong&gt;: Adversarial Discriminative Domain Adaptation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Domain Adaptation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;From&lt;/strong&gt;: &lt;a href=&quot;https://arxiv.org/abs/1702.05464&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;arXiv:1702.05464&lt;/a&gt;, appearing in CVPR 2017&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Contributions&quot;&gt;&lt;a href=&quot;#Contributions&quot; class=&quot;headerlink&quot; title=&quot;Contributions&quot;&gt;&lt;/a&gt;Contributions&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;将之前的论文里提到的一些方法，例如weight sharing、base models、adversarial loss等，归入了统一的框架之中，并进行了测试；&lt;/li&gt;
&lt;li&gt;提出了一种新的框架ADDA，主要思想是不做分类器的自适应，而是设法将目标域的数据映射到域源域差不多的特征空间上，这样就能够复用源域的分类器。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Thesis Notes" scheme="http://www.yuthon.com/categories/Thesis-Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="Domain Adaptation" scheme="http://www.yuthon.com/tags/Domain-Adaptation/"/>
    
      <category term="GANs" scheme="http://www.yuthon.com/tags/GANs/"/>
    
  </entry>
  
  <entry>
    <title>Something about GAN</title>
    <link href="http://www.yuthon.com/2017/08/12/Something-about-GANs/"/>
    <id>http://www.yuthon.com/2017/08/12/Something-about-GANs/</id>
    <published>2017-08-12T11:24:14.000Z</published>
    <updated>2017-08-23T01:59:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看关于GANs的论文，并且自己动手用PyTorch写了一些经典文章的实现，想要稍微总结一下，故有此文。在最后我总结了我自己看过的有关GANs的一些比较好的资源，希望对读者有所帮助。</p>
<a id="more"></a>
<h2 id="Before-Reading-PyTorch"><a href="#Before-Reading-PyTorch" class="headerlink" title="Before Reading: PyTorch"></a>Before Reading: PyTorch</h2><p>在讲GANs之前，首先推一波PyTorch。就我的使用体验来说，PyTorch是远远超过TensorFlow的。PyTorch作为一个动态图计算的框架，与Python结合得非常好，写出来的代码非常<em>Pythonic</em>（反例就是TF的<code>tf.while_loop</code>）。同时PyTorch与NumPy结合得非常好，不用在<code>Tensor</code>与<code>ndarray</code>之间转换来转换去。</p>
<p>总而言之，PyTorch非常适合我这样需要快速开发与快速验证，并且对于运行速度要求并不高的DL研究（讲真，TF的速度也不怎么快，还是吃内存大户）。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><strong>GANs（Generative Adversarial Networks，生成对抗网络）是Generative Models（生成模型）的一种。</strong>所谓生成模型，就是能在一个含有真实数据分布$p_{data}$样本（samples）的训练集上，学习到真实数据分布的估计的表示$p_{model}$的模型。学习到的这个表示可以是显式的（explicit），比如说直接就给出了$p_{model}$；也可以是隐式的（implicit），比如说能够生成符合$p_{model}$分布的样本。<strong>一般来说，GANs属于第二种，能够进行样本生成。</strong>不过在设计上，GANs其实是两者皆可的。</p>
<p><strong>那为什么要搞这个生成模型呢？</strong>原因也是有很多的。比如说，训练生成样本是一个用来测试我们表示高维概率分布的能力的好方法，而现实世界的物体往往是具有高维概率分布的，这就对我们用DL来表示与解释现实世界有帮助。同时，很多DL相关的任务也是很需要样本生成的，比如说超分辨率（super resolution）、风格迁移（style transfer）之类的。此外，生成模型对于某些标签甚至是数据缺失的数据集也很有用，比如说可以用在半监督学习（semi-supervised learning）上，不过我倒是没见到过相关的论文。总之，生成模型是很有用的，也是目前DL领域的一大热门方向。</p>
<p>那么问题又来了，<strong>还有哪些生成模型，GANs相比于其他的生成模型有什么优势</strong>，特别是在去年和它几乎同时火起来的VAE？首先谈谈生成模型的分类。说起这个，就不得不谈<strong>极大似然估计（maximum likelihood estimation）</strong>，几乎所有的生成模型都使用了或者可以转换为使用极大似然来进行估计。极大似然估计的基本思想是，定义一个含有参数$\theta$的模型，用它来提供对于一个概率分布的估计，然后寻找一组最优的参数$\theta$来使得模型的概率分布$p_{model}$最贴合实际数据的概率分布$p_{data}$。具体地说，对于一个具有$m$个样本$x^{(i)}$的数据集来说，可以用似然（likelihood）来表示模型与数据集中数据的契合概率$\prod^m_{i=1} p_{model} (x^{(i)};\theta)$，而我们的最终目的就是找到使得似然最大的参数$\theta$。由于对数函数的优良性质，我们可以将似然取对数。<br>$$<br>\begin{align}<br>\theta ^* &amp;= \underset{\theta}{\operatorname{argmax}} \prod ^m _{i=1} p_{model} (x^{(i)};\theta) \\<br>&amp;= \underset{\theta}{\operatorname{argmax}} \log \prod ^m _{i=1} p_{model} (x^{(i)};\theta) \\<br>&amp;= \underset{\theta}{\operatorname{argmax}} \sum ^m _{i=1} \log p_{model} (x^{(i)};\theta) \\<br>\end{align}<br>$$</p>
<p>另外一方面，我们也可以将极大似然估计看成是一种最小化真实概率分布与模型概率分布之间的<strong>KL散度（Kullback–Leibler divergence）</strong>的方法。虽然我们通常在实践中并不能直接接触到$p_{data}$，而是只能够获得服从其分布的$m$个采样。我们可以用这些采样组成的数据集来定义$\hat{p} _{data}$这么一个经验分布（empirical distribution）来近似$p_{data}$。可以证明，最小化$\hat{p} _{data}$与$p _{model}$之间的KL散度，等价于在数据集上最大化对数似然。<br>$$<br>\theta ^*= \underset{\theta}{\operatorname{argmin}} D_{KL} (p_{data} (x) \parallel p_{model} (x;\theta))<br>$$</p>
<p>好了，说了这么多，让我们再把话题转向生成模型的分类。生成模型可以根据其计算似然及其梯度，或者近似估计这些值的方法来进行分类。其中值得注意的有FBVNs、VAE以及GANs。GANs属于右边的那一类，能够隐式地得到概率密度，并直接从中生成样本。</p>
<p><img src="/images/taxonomy_for_generative_models.png" alt="taxonomy_for_generative_models"></p>
<p>GANs相对于其他的生成模型，其优点主要在于：</p>
<ul>
<li>GANs能够并行地生成样本，而非FBVNs那样只能串行；</li>
<li>GANs在设计上的约束很少，不像玻尔兹曼机（Boltzmann Machine）那样只能用少数几种概率分布，也不像非线性ICA那样，要求生成器必须可逆并且隐式编码（latent code）$z$必须与数据集中的样本$x$具有相同的维度；</li>
<li>GANs不需要马尔科夫链（Markov chains），这点不同于玻尔兹曼机以及GSNs；</li>
<li>GANs不需要使用微分边界（variational bound），并且GANs里面用到的模型早已被证明是万能逼近器（universial approximators），因此GANs能够保证渐进一致（asymptotically consistent）。相比而言，某些VAEs虽然推测是渐进一致的，但是没有得到证明；</li>
<li>最后一点，GANs就目前的效果来说，其生成出来的样本的质量比用其他生成模型得到的要好。</li>
</ul>
<p>当然，原始的GANs有一点是非常让人头疼的，就是它的训练过程本质上是寻找一场比赛的纳什均衡（Nash equilibrium）的过程，这导致GANs很难稳定的训练。当然，之后要提到的WGAN在一定程度上解决了这问题。</p>
<h2 id="How-do-GANs-Work"><a href="#How-do-GANs-Work" class="headerlink" title="How do GANs Work?"></a>How do GANs Work?</h2><h3 id="The-GAN-framework"><a href="#The-GAN-framework" class="headerlink" title="The GAN framework"></a>The GAN framework</h3><p>GANs由两个部分组成：一个是<strong>生成器（generator）</strong>，负责生成样本，并且尽力与原始数据集中的分布一致；另一个是<strong>判别器（discriminator）</strong>，负责检验输入的样本是来自真实数据分布还是生成器生成的。这两者都可以表现为可微的函数（其实最后就是表现为神经网络），生成器是以$z$为输入，$\theta^{(G)}$为参数的函数$G$，而判别器是以$x$为输入，$\theta^{(D)}$为参数的函数$D$。生成器的目的是在只能控制$\theta^{(G)}$的情况下，最小化$J^{(G)} (\theta ^{(D)}, \theta ^{(G)})$；而判别器则是在只能改变$\theta^{(D)}$的情况下，最小化$J^{(D)} (\theta ^{(D)}, \theta ^{(G)})$。通俗地说就是，生成器想要生成出的样本能够让判别器区分不出这是来自真实数据还是生成的（$D(G(z))=1$），而判别器则是想要尽可能地将这些区分开来（$D(G(z))=0$）。这么一来，这场比赛的<strong>解就是一个纳什均衡</strong>。也就是说，这个解$(\theta ^{(D)}, \theta ^{(G)})$不但能在$J^{(G)}$上取到局部最小值（local minimum），而且在$J^{(D)}$上也取到局部最小值。如果两者均具有足够的容量（capacity），则$\forall x, D(x)=D(G(z))=\frac{1}{2}$。</p>
<p><img src="/images/the-GAN-framework.png" alt="the-GAN-framework"></p>
<p>生成器就是一个简单的可微分的函数$G$，一般来说我们用DNN来表示。生成器接受一个来自先验分布（比如说高斯分布）的采样$z$，然后对其进行处理，得到一个来自$p_{model}$分布的采样$G(z)$。值得注意的是，我们并不一定要把$z$作为DNN第一层的输入。比如说我们可以把$z$一刀切成两部分，$z^{(1)}$和$z^{(2)}$，$z^{(1)}$作为DNN首层的输入，而$z^{(2)}$则加在DNN末层的输出上。还有一种操作是，我们可以在DNN的隐藏层上搞加性噪声（additive noise）或者乘性噪声（multiplicative noise），或者直接在隐藏层输出上串（concatenate）一个噪声。总而言之，生成器网络在设计上的约束是很少的，可以任意开脑洞，只要<strong>保证$z$的维度不低于$x$，并且整个网络是可微的</strong>就可以了。</p>
<p>判别器就是一个简单的二分类的DNN，在此就不赘述了。</p>
<h3 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h3><p><strong>判别器的代价函数就是简单的交叉熵（cross-entropy cost）</strong>，如下所示。唯一的区别是，判别器的一次训练由两个mini-batches组成，一部分是来自于数据集的真实样本，标签为1；另一部分是来自生成器所生成的样本，标签为0。一般来说，所有的GANs的判别器都是用下述公式作为代价函数的。<br>$$<br>J^{(D)} (\theta ^{(D)}, \theta ^{(G)}) = -\frac{1}{2} \mathbb{E}_{x\sim p_{data}} \log D(x) - \frac{1}{2} \mathbb{E}_{z} \log (1-D(G(z)))<br>$$<br>生成器的代价函数的可选择范围就稍微多了一些，主要有minimax、heuristic以及maximum likelihood三种，其中前两种比较常见。</p>
<ul>
<li><p><strong>Minimax</strong>：其实就是零和游戏（zero-sum game），生成器的代价函数等于判别器代价函数的负值。可以证明，这等价于最小化数据与模型之间的JS散度（Jenson-Shannon divergence）。但是这个代价函数其实是存在着隐患的。一般来说，判别器训练的收敛速度比生成器快得多，因此判别器很快就能以较高的置信度将生成器生成的假样本给拒绝掉，从而造成生成器的梯度消失的问题。这一缺陷可以用下面的方法来解决。<br>$$<br>J^{(G)} = -J^{(D)} \\<br>V (\theta ^{(D)}, \theta ^{(G)}) = J^{(D)} (\theta ^{(D)}, \theta ^{(G)}) \\<br>\theta ^{(G)*} = \underset{\theta ^{(G)}}{\operatorname{argmin}} \underset{\theta ^{(D)}}{\operatorname{max}} V (\theta ^{(D)}, \theta ^{(G)})<br>$$</p>
</li>
<li><p><strong>Heuristic, non-saturating game</strong>：我们可以换种方式来思考问题：在minimax game中，我们是让生成器最小化判别器判对的对数概率，这会导致一些问题；那么可不可以让生成器来最大化判别器判错的对数概率呢？显然也是可以的，并且这也能避免生成器梯度消失的问题。这是一种启发式的方法，其动机是为了让玩家（也就是生成器）在“输掉”游戏的时候能得到比较强的梯度。<br>$$<br>J^{(G)} = - \frac{1}{2} \mathbb{E}_{z} \log (D(G(z)))<br>$$</p>
</li>
</ul>
<p>从下图可以看出，heuristicly designed non-saturating cost在$D(G(z))$变化的时候，其方差较小，因此是比较合适作为生成器代价函数的选择的。</p>
<p><img src="/images/cost_functions_of_GANs.png" alt="cost_functions_of_GANs"></p>
<h2 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h2><p>DCGAN即使用了全卷积网络的GANs，一般特指<a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="external">这篇paper</a>中的网络结构。目前几乎所有的GANs都或多或少地借鉴了DCGAN的架构。DCGAN的主要创新点在于：</p>
<ul>
<li><strong>同时在判别器与生成器网络中使用了Batch Normalization层。</strong>当然，为了能够学到真实数据分布正确的均值（mean）与规模（scale），判别器的首层与生成器的末层没有加BN层。</li>
<li><strong>整个网络架构借鉴了the all-convolutional net</strong>（不是FCN），不含pooling和“unpooling”层，增加表示维度是靠<code>stride=1</code>的转置卷积（transposed convolition）实现的。</li>
</ul>
<p>总而言之，就是换了原始的GAN中的网络架构，把FC层都换成了带BN层的卷积层。</p>
<h2 id="WGAN-amp-WGAN-GP"><a href="#WGAN-amp-WGAN-GP" class="headerlink" title="WGAN &amp; WGAN-GP"></a>WGAN &amp; WGAN-GP</h2><h3 id="What-does-it-mean-to-learn-a-probability-distribution"><a href="#What-does-it-mean-to-learn-a-probability-distribution" class="headerlink" title="What does it mean to learn a probability distribution?"></a>What does it mean to learn a probability distribution?</h3><p>WGAN在<a href="https://arxiv.org/abs/1701.07875" target="_blank" rel="external">paper</a>开头就直截了当地提出了一个问题，<strong>我们怎么样才算是学到了一个概率分布呢？</strong>如果按照本文开头的说法，我们的做法是先定义一个参数化的概率密度族$(P_\theta )_{\theta \in \mathbb{R}}$，然后通过在已有的数据集${x^{(i)}} ^m_{i=1}$上最大化似然的方法来找到一个最佳的参数，并将这个参数对应的那个概率密度$P_{\theta}$视为我们所学习到的模型。<br>$$<br>\underset{\theta \in \mathbb{R}}{\max} \frac{1}{m} \sum ^m_{i=1} \log P_{\theta} (x^{(i)})<br>$$<br>如果真实的数据分布$\mathbb{P}_r$存在密度，而我们学习到的概率密度$P_{\theta}$所对应的概率分布为$\mathbb{P}_{\theta}$，则<strong>上述学习过程实际上等价于最小化$\mathbb{P}_r$与$\mathbb{P}_{\theta}$之间的KL散度$KL(\mathbb{P}_r \parallel \mathbb{P}_{\theta})$。</strong>这也是上文提到过的。</p>
<p>是不是看起来有理有据令人信服？但是，这上面的推理过程有一个很重要很显而易见但是又常常为人所忽视的条件，那就是<strong>首先$\mathbb{P}_r$与$\mathbb{P}_{\theta}$之间的KL散度必须存在，然后我们才能去最小化这个KL散度，来获取最优$\theta$</strong>。你TM在逗我，KL散度还能不存在？事实上，这是很常见的。比如说我们用GANs生成样本的时候，输入的随机噪声通常就是一个64维的向量，然后经过生成器网络来生成一个$64\times64=4096$维的图片，其本质还是决定于开始的那个64维的随机向量。64维相对于4096维来说实在是微不足道，<strong>也就是说，生成样本分布的支撑集（support）构成了高维空间上的低维流形（manifold），撑不满整个高维空间。那么该模型的流形与真实分布的支撑集之间很有可能就没有不可忽视的交集（non-negligible intersection），以致两者间的KL散度不存在，或者说是$\infty$。</strong></p>
<p>KL散度都不存在了，那还优化个毛？不过这难不倒千千万万机智的研究者。<strong>你不是要让这两个概率分布有交集吗，那我就往$\mathbb{P}_{\theta}​$上加噪声，加个大点的噪声总是能让这两个分布碰在一起的，那KL散度不是就有了？</strong>这也是几乎所有的生成模型都包含了噪声项的原因。一般来说，在训练开始的时候加的噪声要大一点，让含有噪声的两个分布能够够得着。随着训练过程的深入，两个分布的主体开始慢慢靠近，这时候噪声就需要慢慢减小了。最后两个分布开始真正有了不可忽视的交集，那么此时加不加噪声也没什么关系了。</p>
<p>不过这种方法也是存在着一些问题的，加噪声很可能使得生成出来的样本比较模糊（blurry）。如果加的是<code>mean=0.1</code>的高斯噪声，而像素值本身又已经归一化到了$[0,1]$，那么很显然，这个噪声相对于像素值来说太大了。于是机智的研究者在论文里展示生成的样本的时候，不像他们在训练过程中干的那样，是不怎么加噪声的。换句话说，加噪声虽然在一定程度上保证了KL散度的存在，也就是使得极大似然方法能够奏效，但是这对于问题本质上来说并没有改善，并非解决问题的正确方法（走上了邪路^_^）。</p>
<p>与其显式地估计有可能并不存在的$\mathbb{P}_r$的密度，不如<strong>直接将来自先验分布$p(z)$的随机变量$\mathcal{Z}$通过参数化的映射$g_\theta : \mathcal{Z} \to \mathcal{X}$来生成样本$\mathcal{X}$，只要这个映射出来的样本$\mathcal{X}$的分布服从或者接近$\mathbb{P}_r$</strong>，那不就是学到了真实的概率分布？VAEs和GANs就是这么做的。这么做有两个好处，首先其与直接估计密度的方法不同，能够表示被约束在低维流形的概率分布；同时直接生成样本有时候比得到一个干巴巴的概率密度更加有用。</p>
<p>那么问题又转移到了，<strong>如何衡量生成样本分布$\mathbb{P}_{\theta}$与真实样本分布$\mathbb{P}_r$之间的相似性或者说距离$\rho(\mathbb{P}_{\theta}, \mathbb{P}_r)$？</strong>这个距离度量需要有比较好的性质，不能两个分布没有交集就直接歇菜了（说的就是KL散度你这个大坑货）。WGAN论文之后的部分就在讲如何定义一个好的距离度量，并将其应用在GANs中。</p>
<p>各个距离度量的一个基本的差异在于它们对于成序列的概率分布的收敛性的影响。一个概率分布的序列$(\mathbb{P}_t)_{t\in\mathbb{R}}$能够收敛，当且仅当存在一个$\mathbb{P}_\infty$使得$\rho(\mathbb{P}_{\theta}, \mathbb{P}_r)$趋向于零，也就是说取决于$\rho$的定义。一般来说，如果$\rho$在拓扑（topology）上越弱，则序列越容易收敛。</p>
<p>为了优化生成模型的参数$\theta$，我们希望定义的模型能够使得映射$\theta \mapsto \mathbb{P_\theta}$连续。这里的连续指的是当一连串的参数$\theta_t$收敛于一个值$\theta$，概率$\mathbb{P}_{\theta_t}$也能收敛于$\mathbb{P}_\theta$。不过这种连续性取决于我们选择的距离度量。距离度量越弱，概率分布越容易瘦脸，则越容易定义一个从$\theta$空间到$\mathbb{P}_\theta$空间的连续的映射。我们这里关注映射$\theta \mapsto \mathbb{P_\theta}$连续性的原因是，<strong>我们希望$\theta \mapsto \rho(\mathbb{P}_{\theta}, \mathbb{P}_r)$的损失函数是连续的（方便梯度下降训练），而这等价于在使用距离度量$\rho$的情况下映射$\theta \mapsto \mathbb{P_\theta}$连续。</strong></p>
<h3 id="Different-Disrances"><a href="#Different-Disrances" class="headerlink" title="Different Disrances"></a>Different Disrances</h3><p>令$\mathcal{X}$为紧致的度量集合（例如图像空间$[0,1]^d$），$\Sigma$为$\mathcal{X}$的Borel子集，$\operatorname{Prob}(\mathcal{X})$为$\mathcal{X}$上的概率度量空间，则可以定义两个分布$\mathbb{P}_r, \mathbb{P}_g \in \operatorname{Prob}(\mathcal{X})$之间的距离或是散度如下：</p>
<ul>
<li><p><strong>TV距离（The Total Variation distance）</strong><br>$$<br>\delta (\mathbb{P}_r, \mathbb{P}_g) = \underset{A\in\Sigma}{\sup} |\mathbb{P}_r (A) - \mathbb{P}_g(A)|<br>$$</p>
</li>
<li><p><strong>KL散度（The Kullback-Leibler divergence）</strong>：注意KL散度是不对称的，而且在例如$P_g (x) = 0$且$P_r (x) &gt; 0$的情况下会变成无穷。<br>$$<br>KL(\mathbb{P}_r \parallel \mathbb{P}_g) = \int \log \left( \frac{P_r (x)}{P_g (x)} \right) P_r (x) d \mu (x)<br>$$</p>
</li>
<li><p><strong>JS散度（The Jensen-Shannon divergence）</strong>：其中$\mathbb{P}_m= (\mathbb{P}_r + \mathbb{P}_g) / 2$。JS散度是对称的，并且始终是有定义的。DCGAN的生成器的损失函数就是JS散度形式的，但是JS散度也会导致一些问题。<br>$$<br>JS(\mathbb{P}_r, \mathbb{P}_g) = KL(\mathbb{P}_r \parallel \mathbb{P}_m) + KL(\mathbb{P}_g \parallel \mathbb{P}_m)<br>$$</p>
</li>
<li><p><strong>EM距离（The Earth-Mover distance or Wasserstein-1）</strong>：其中$\prod(\mathbb{P}_r, \mathbb{P}_g)$表示所有边际概率为$\mathbb{P}_r$与$\mathbb{P}_g$的联合分布$\gamma (x,y)$的集合。直观地说，$\gamma (x,y)$表示了为了将分布$\mathbb{P}_r$变换成$\mathbb{P}_g$所需要的从$x$移到$y$的”质量“的多少。而EM距离则代表了最优搬运方案的代价。<br>$$<br>W(\mathbb{P}_r, \mathbb{P}_g) = \underset{\gamma\in\prod(\mathbb{P}_r, \mathbb{P}_g)}{\inf} \mathbb{E} _{(x,y)\sim \gamma} \left[ \parallel x- y\parallel \right]<br>$$</p>
</li>
</ul>
<p>为了比较这几个距离度量的优劣，WGAN设计了一个例子：令$Z \sim U[0,1]$，$\mathbb{P}_0$为$(0, Z) \in \mathbb{R}^2$的分布（$x$坐标为$0$，$y$坐标为$Z$，实际上就是分布在点$(0,0)$到点$(0,1)$这条线段上）。令生成样本分布为$g_\theta (z)=(\theta, z)$，其中以$\theta$作为唯一的实值参数（实际上就就是沿着$x$轴左右移动之前提到的线段）。</p>
<p><img src="/images/WGAN_example_1.png" alt="WGAN_example_1"></p>
<p><img src="/images/WGAN_figure_1.png" alt="WGAN_figure_1"></p>
<p>则前文所述的各个距离度量的表达式为：</p>
<ul>
<li>$W(\mathbb{P}_\theta, \mathbb{P}_0) = |\theta|$</li>
<li>$JS(\mathbb{P}_\theta, \mathbb{P}_0) =  \begin{cases} \log 2, &amp; \text{if } \theta \ne 0 \\  0, &amp; \text{if } \theta = 0 \end{cases}$</li>
<li>$KL(\mathbb{P}_\theta, \mathbb{P}_0) = (\mathbb{P}_0, \mathbb{P}_\theta) = \begin{cases} +\infty, &amp; \text{if } \theta \ne 0 \\  0, &amp; \text{if } \theta = 0 \end{cases}$</li>
<li>$\delta(\mathbb{P}_\theta, \mathbb{P}_0) = \begin{cases} 1, &amp; \text{if } \theta \ne 0 \\  0, &amp; \text{if } \theta = 0 \end{cases}$</li>
</ul>
<p>可以看出，当$\theta _t \to 0$，只有EM距离能够使得序列$(\mathbb{P}_{\theta _t}) _{t \in N}$收敛于$\mathbb{P}_0$，而其他的JS散度、KL散度、KL散度取反或者TV距离都不能。这说明<strong>在EM距离下，我们能够通过梯度下降的方式习得低维流形上的概率分布，而在其他距离度量下甚至连损失函数的连续性都不能保证，更遑论习得概率分布了。</strong>一句话，EM距离大法好！因此，WGAN将EM距离（又称Wasserstein-1距离）作为两个概率分布之间的距离度量，从而推导损失函数的表达式。</p>
<h3 id="Wasserstein-GAN"><a href="#Wasserstein-GAN" class="headerlink" title="Wasserstein GAN"></a>Wasserstein GAN</h3><p>从上面的例子可以看出，<strong>Wasserstein距离比JS散度具有更优良的性质</strong>。（当然，WGAN原论文里面还有一大堆数学推导来证明，这里就不列了。）但是，Wasserstein距离的原始公式是很难计算的（intractable）。WGAN里使用了一个Kantorovich-Rubinstein duality的定理，将EM距离的式子转换为：<br>$$<br>W(\mathbb{P}_\theta, \mathbb{P}_0) = \frac{1}{K} \underset{\parallel f \parallel _L \le K}{\sup} \mathbb{E}_{x\sim \mathbb{P}_r} - \mathbb{E}_{x\sim \mathbb{P}_\theta} [f(x)]<br>$$<br>这里的$f(x)$需要是K-Lipschitz函数，也就是说要求$\exists K\in \mathbb{R},K\ge0$，使得对于实值函数$f(x)$定义域内的任意$x_1$与$x_2$，都满足下式：<br>$$<br>|f(x_1)-f(x_2)| \le K |x_1-x_2|<br>$$<br>假设我们有一个符合K-Lipschitz条件的函数族${f_w}_{w\in \mathcal{W}}$，那么就变成了解决以下问题：<br>$$<br>\underset{w\in\mathcal{W}}{\max} \mathbb{E} _{x\sim \mathbb{P}_r} [f_w (x)] - \mathbb{E} _{z\sim p(z)} [f_w (g_\theta (z))]<br>$$<br>这个函数族自然是可以用一个含参$w\in \mathcal{W}$的神经网络来表示，但是如何满足K-Lipschitz条件呢？需要注意的是，整个函数族${f_w}_{w\in \mathcal{W}}$需要符合条件，但我们并不关心$K$的具体值。因此<strong>WGAN论文中提出了一种简单粗暴的做法，把权重的值域$\mathcal{W}$限制在一个非常小的范围内</strong>，比如说$\mathcal{W} = [-0.01, 0.01]$，也就是$w \in [-0.01, 0.01]$。这么一来，$\partial f_w / \partial x$也会被限制在一定范围内，则$\exists K\in \mathbb{R},K\ge0$使得$f_w$满足K-Lipschitz条件。当然这种做法很粗糙，也会导致某些问题，这些问题与解决放将在之后的WGAN-GP小节中详述。</p>
<p>WGAN又做了一个实验，如下图所示，判别器学习区分真假两个高斯分布。可以看出用了JS散度的判别器虽然能很快就把这两个样本分布判别出来，但是在左右两端都处于饱和状态，根本提供不了梯度信息。而<strong>用了Wasserstein距离的判别器（文中称作critic）则几乎能在任何位置都提供线性的梯度信息，这对于生成器的训练来说是非常重要的。</strong></p>
<p><img src="/images/WGAN_figure_2.png" alt="WGAN_figure_2"></p>
<p>到此为止，WGAN所使用的Wasserstein距离就介绍完了。损失函数定义如下：<br>$$<br>\mathcal{L}_D = \mathbb{E} _{z\sim p(z)} [f_w (g_\theta (z))] - \mathbb{E} _{x\sim \mathbb{P}_r} [f_w (x)]   \\<br>\mathcal{L}_G = - \mathbb{E} _{z\sim p(z)} [f_w (g_\theta (z))]<br>$$<br>具体到网络训练上，相对于DCGAN的主要改进有：</p>
<ul>
<li>判别器与生成器的loss不用log</li>
<li>判别器不用Sigmoid层</li>
<li>优化器更新参数之后将权重限制在一个非常小的区间$\mathcal{W}$内</li>
<li>使用RMSprop而非基于动量的优化器</li>
<li>在初始阶段多训练判别器几次（$n_{critic}=100$），保证判别器在初始的时候就已经达到一个比较好的水准，增加训练的稳定性</li>
</ul>
<p>WGAN训练过程的伪代码如下所示：</p>
<p><img src="/images/WGAN_algorithm.png" alt="WGAN_algorithm"></p>
<p><strong>除了改善网络训练的稳定性之外，WGAN所使用的Wasserstein距离还是一个非常好的训练程度的指示器。</strong>但是JS散度就没有这个作用。</p>
<p><img src="/images/WGAN_figure_3.png" alt="WGAN_figure_3"></p>
<p><img src="/images/WGAN_figure_4.png" alt="WGAN_figure_4"></p>
<p>使用Wasserstein距离与使用JS散度训练出来的DCGAN，在正常情况下生成样本质量是差不多的：</p>
<p><img src="/images/WGAN_figure_5.png" alt="WGAN_figure_5"></p>
<p>但是，WGAN还能在生成器去掉BN层之后依然得到比较好的生成样本，原始DCGAN就直接崩了。</p>
<p><img src="/images/WGAN_figure_6.png" alt="WGAN_figure_6"></p>
<p>此外，如果都用MLP而不是CNN，WGAN的质量会稍差一些，但是使用JS散度训练出来的DCGAN还会有多样性不足（mode collapse）的缺陷。</p>
<p><img src="/images/WGAN_figure_7.png" alt="WGAN_figure_7"></p>
<h3 id="WGAN-GP"><a href="#WGAN-GP" class="headerlink" title="WGAN-GP"></a>WGAN-GP</h3><p>上面提到，WGAN让$f_w$满足K-Lipschitz条件的方法很粗暴，直接将整个网络的权重都限制在了一个很小的区间内。这样做其实是会导致一些问题的。</p>
<ul>
<li>第一个问题是<strong>weight clipping不能充分利用神经网络的容量（capacity underuse）</strong>。如下图所示，左图第一排是WGAN估计的分布（value surface），第二排是WGAN-GP估计的分布。可以明显看出，WGAN基本上都是用矩形来近似，没有像WGAN-GP那样学到数据样本更高层的变化趋势。从右图的第二幅图可以看出，经过充分训练的WGAN网络的权值基本上就分布在了$-c$与$c$两个端点上，中间几乎没有权值分布。这简直就像二值神经网络一样了，难怪网络的容量不能得到充分利用。</li>
<li>第二个问题是<strong>梯度消失与梯度爆炸（exploding and vanishing gradients）</strong>。从右图的第一幅图可以看到，在不用BatchNorm的时候，weight clipping的常数$c$稍有不同，就会使得梯度随着网络一层层传播而指数增长或者衰减，从而导致梯度消失或者爆炸。</li>
</ul>
<p><img src="/images/WGAN-GP_figure_1.png" alt="WGAN-GP_figure_1"></p>
<p>针对这两个问题，WGAN的作者又想出了新的操作——gradient decay。新的操作能够解决上述的两个问题，让WGAN的训练过程更加稳定（好像上次WGAN对DCGAN的时候也是这么讲的:cry:）。</p>
<p>上面说了，weight clipping是为了让网络函数$D(x)$满足Lipschitz条件才用的，并不是非它不可。既然表现这么差，那我们能不能换一种方法来做Lipschitz约束呢？gradient decay就是这样被想出来的。我们回顾一下Lipschitz条件的定义，是要求函数$D(x)$在它的值域上满足$\forall x_1, x_2, \parallel \nabla D(x) \parallel _p \le K$。于是作者们<strong>在判别器的损失函数上加了一个梯度的惩罚项，鼓励梯度的范数趋近于K</strong>（或者说是1，反正1-Lipschitz与K-Lipschitz条件只是差了常数倍）<br>$$<br>L = \underset{\tilde{x} \sim \mathbb{P}_g}{\mathbb{E}} [D(\tilde{x})] - \underset{x \sim \mathbb{P}_r}{\mathbb{E}} [D(x)] + \lambda \underset{\hat{x} \sim \mathbb{P}_{\hat{x}}}{\mathbb{E}} [(\parallel \nabla _{\hat{x}} D(\hat{x})\parallel _2 -1)^2]<br>$$<br>这里有几个需要注意的地方：</p>
<ul>
<li><strong>最优判别器的一个特性（properties of the optimal WGAN critic）</strong>：为什么惩罚项是鼓励梯度绝对值趋向于$K$，而不是只是让梯度在$[-K, K]$之间呢？这就牵扯到WGAN训练出来的最优判别器的一个性质。假设我们随机采到了两个样本$x_r \sim \mathbb{P} _ r$和$x_g \sim \mathbb{P} _ g$，那么对所有在其连线$x_t = (1-t)x_g + t x_r$上的点的梯度就是$\nabla D(x_t) = \frac{x_r - x_t}{\parallel y - x_t \parallel}$。换句话说，在$\mathbb{P}_r$与$\mathbb{P}_g$内，以及两者的中间地带，最优判别器的梯度基本上都是1（或者说$K$）。</li>
</ul>
<ul>
<li><strong>$\mathbb{P}_{\hat{x}}$的采样分布（sampling distribution）</strong>：在计算$\lambda \underset{\hat{x} \sim \mathbb{P}_{\hat{x}}}{\mathbb{E}} [(\parallel \nabla _{\hat{x}} D(\hat{x})\parallel _2 -1)^2]$的时候，需要把期望$\mathbb{E}$转换为采样然后取平均。然而这个$\mathbb{P}_{\hat{x}}$要求我们在整个样本空间$\mathcal{X}$上采样，这种在高维空间指望用采样来估计期望的方法显然是不可实现的，会导致维度灾难。于是文中将$\mathbb{P}_{\hat{x}}$的采样定义为在沿着真实分布$\mathbb{P}_r$的一堆点与生成分布$\mathbb{P}_g$的一堆点的直线上均匀采样。从实验效果上来说，这是很有效的。</li>
<li><strong>惩罚系数（penalty coefficient）</strong>：取$\lambda=10$，作者称无论在Toy数据集还是大规模的ImageNet数据及上都是work的。</li>
<li><strong>不要用Batch Normalization</strong>：gradient penalty惩罚的是单个对单个的样本，而BN会把这样单个的映射变成整个batch的所有输入到整个batch的输出，引入了不同采样之间的依赖关系，这样GP就不能用了。作者推荐用Layer Normalization。</li>
<li><strong>双边惩罚（two-sided penalty）</strong>：惩罚项鼓励梯度从正负两边来趋向1（或者说$K$），作者称这样能收敛到更好的最优点，速度更快，而且对于判别器来说不会造成太大的约束。</li>
</ul>
<p>WGAN-GP相对于WGAN的改进就是上面这些了，其算法的伪代码如下。</p>
<p><img src="/images/WGAN-GP_algorithm.png" alt="WGAN-GP_algorithm"></p>
<p>接下来看看实验结果：</p>
<ul>
<li><p>WGAN-GP的收敛速度明显比WGAN快，虽然在wall time上还是比不过DCGAN，但是WGAN-GP在训练的稳定性上犹有过之。</p>
<p><img src="/images/WGAN-GP_figure_3.png" alt="WGAN-GP_figure_3"></p>
</li>
<li><p>同时，在面对之前的一些困难的训练条件（比如说不用BN层甚至不用任何normalization），WGAN-GP都能够稳定训练并且结果良好。特别是面对ResNet-101这么高层的网络，只有WGAN-GP能训练出结果。</p>
<p><img src="/images/WGAN-GP_figure_2.png" alt="WGAN-GP_figure_2"></p>
</li>
</ul>
<h2 id="GANs-in-Practice"><a href="#GANs-in-Practice" class="headerlink" title="GANs in Practice"></a>GANs in Practice</h2><h3 id="MLP-GAN"><a href="#MLP-GAN" class="headerlink" title="MLP-GAN"></a>MLP-GAN</h3><p>使用多层感知机（MLP）来构建判别器与生成器的网络，可以说得上是最为简单的GANs了。可以根据这个来摸清楚GANs自身的一套工作流程到底是怎么样的，为之后实现复杂的GANs网络做个铺垫。本小节相关代码在<a href="https://github.com/corenel/GAN-Zoo/tree/master/GAN" target="_blank" rel="external">GAN-Zoo/GAN</a>中。</p>
<p>首先我们需要训练集，比如说我们这里用到的MNIST数据集。PyTorch在这点上做得很好，对于一些常用的数据集都自带有loader，不用自己写了。相关代码见<a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/data_loader.py" target="_blank" rel="external">GAN-Zoo/GAN/data_loader.py</a>。</p>
<p>有了数据集之后就需要自己定义模型的网络结构了，具体到GANs就是<a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/models.py#L6-L22" target="_blank" rel="external">判别器</a>与<a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/models.py#L25-L41" target="_blank" rel="external">生成器</a>的定义。这里贴一段判别器的定义，可以看出PyTorch在网络定义方面还是很方便的（和Keras差不多）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">"""Model for Discriminator."""</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></div><div class="line">        <span class="string">"""Init for Discriminator model."""</span></div><div class="line">        super(Discriminator, self).__init__()</div><div class="line">        self.layer = nn.Sequential(nn.Linear(input_size, hidden_size),</div><div class="line">                                   nn.LeakyReLU(<span class="number">0.2</span>),</div><div class="line">                                   nn.Linear(hidden_size, hidden_size),</div><div class="line">                                   nn.LeakyReLU(<span class="number">0.2</span>),</div><div class="line">                                   nn.Linear(hidden_size, output_size),</div><div class="line">                                   nn.Sigmoid())</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">"""Forward step for Discriminator model."""</span></div><div class="line">        out = self.layer(x)</div><div class="line">        <span class="keyword">return</span> out</div></pre></td></tr></table></figure>
<p>定义完模型，之后就是整个网络的训练过程了：</p>
<ul>
<li><a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/main.py#L17-L37" target="_blank" rel="external">初始化阶段</a>：初始化models、criterion（<code>nn.BCELoss()</code>）以及optimizer（<code>nn.optim.Adam()</code>），检查cuda是否可用（<code>nn.cuda.is_available()</code>），能用的话就上GPU跑。</li>
<li>网络训练阶段：<ul>
<li><a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/main.py#L49-L78" target="_blank" rel="external">训练判别器</a>：主要分为两个步骤，首先从数据集中读取样本，判别器forward一遍，然后和真实标签（<code>1</code>）做loss并backward；其次，生成随机噪声而后经过生成器的forward得到生成样本，再喂给判别器，与虚假标签（<code>0</code>）做loss并backward。最后由optimizer更新判别器网络的参数。</li>
<li><a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/main.py#L80-L105" target="_blank" rel="external">训练生成器</a>：首先生成随机噪声，而后通过生成器网络生成虚假样本，再通过判别器网络得到loss，并更新生成器网络。值得注意的是，<a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/main.py#L91" target="_blank" rel="external">生成器的loss的计算</a>用的是真实标签（<code>1</code>），也就是上述的heuristicly designed non-saturating cost。</li>
<li><a href="https://github.com/corenel/GAN-Zoo/blob/master/GAN/main.py#L107-L131" target="_blank" rel="external">输出log并保存model</a></li>
</ul>
</li>
</ul>
<p>非常简单的代码，但是生成出来的数字的效果还是很不错的。</p>
<p><img src="/images/GAN_real_images.png" alt="GAN_real_images"></p>
<p><img src="/images/GAN_fake_images-300.png" alt="GAN_fake_images-300"></p>
<p>第一张是MNIST数据集中的，第二张是通过GANs生成的（300次迭代）。虽然第二张还有些不尽如人意之处（迭代次数太少），但是总体上来说，已经非常接近真实的数字图片了。这就是GANs的威力！</p>
<h3 id="DCGAN-1"><a href="#DCGAN-1" class="headerlink" title="DCGAN"></a>DCGAN</h3><p>DCGAN与MLP-GAN的代码相差不多，基本上就是重新写一遍model的事。为了测试DCGAN的capacity，我将MNIST数据集换成了CIFAR-10数据集。相关代码见<a href="https://github.com/corenel/GAN-Zoo/tree/master/DCGAN" target="_blank" rel="external">GAN-Zoo/DCGAN</a>。</p>
<p>不过这次的结果只能说是差强人意，20次迭代后生成的图像还算不错（毕竟CIFAR-10的分辨率是<code>28*28</code>）：</p>
<p><img src="/images/DCGAN-fake-20-700.png" alt="DCGAN-fake-20-700"></p>
<p>但是继续训练的话，GANs训练不稳定的问题就出现了。到24次迭代的时候，由于判别器已经非常精准，导致生成器的loss固定在了27左右动弹不得，从而生成的图像变成了一团噪声：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Epoch [24/25] Step [200/782]:d_loss=2.160674767992532e-07 g_loss=27.614051818847656 D(x)=2.160674767992532e-07 D(G(z))=0.0</div><div class="line">Epoch [24/25] Step [210/782]:d_loss=6.410500191122992e-06 g_loss=27.623014450073242 D(x)=6.410500191122992e-06 D(G(z))=0.0</div><div class="line">Epoch [24/25] Step [220/782]:d_loss=1.5441528375959024e-06 g_loss=27.62175750732422 D(x)=1.5441528375959024e-06 D(G(z))=0.0</div><div class="line">Epoch [24/25] Step [230/782]:d_loss=3.24100881243794e-07 g_loss=27.62472152709961 D(x)=3.24100881243794e-07 D(G(z))=0.0</div><div class="line">...</div></pre></td></tr></table></figure>
<p><img src="/images/DCGAN-fake-24-300.png" alt="DCGAN-fake-24-300"></p>
<p>不过到了第25次迭代，DCGAN似乎又略微恢复了正常：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Epoch [25/25] Step [10/782]:d_loss=0.32297325134277344 g_loss=8.964262962341309 D(x)=0.3229268193244934 D(G(z))=4.6418874262599275e-05</div><div class="line">Epoch [25/25] Step [20/782]:d_loss=0.006471103988587856 g_loss=7.038626194000244 D(x)=0.0035153746139258146 D(G(z))=0.002955729141831398</div><div class="line">Epoch [25/25] Step [30/782]:d_loss=0.17143061757087708 g_loss=12.035135269165039 D(x)=0.17115993797779083 D(G(z))=0.0002706760715227574</div><div class="line">Epoch [25/25] Step [40/782]:d_loss=0.21678031980991364 g_loss=11.419050216674805 D(x)=0.004731819964945316 D(G(z))=0.2120485007762909</div></pre></td></tr></table></figure>
<p><a href="/images/DCGAN-fake-25-700.png">DCGAN-fake-25-700</a></p>
<p>当然，让GANs训练变得稳定的方法不是没有，<a href="https://github.com/soumith/ganhacks" target="_blank" rel="external">这里</a>就列举了不少tricks：</p>
<ul>
<li>避免稀疏的梯度：不要使用ReLU或者Max Pooling，尽量用LeakyReLU；</li>
<li>使用软标签（soft and noisy labels），也就是说真实标签与虚假标签不要是固定的<code>1</code>或者<code>0</code>，最好加点噪声上去；</li>
<li>在真实样本的输入上也加点随时间衰减的噪声</li>
<li>给生成器加Dropout层</li>
<li>……</li>
</ul>
<p>这样的trick是还有很多，有些我试过确实有用，还有些则不是很确定，属于玄学范畴。不过与其用一大堆tricks，还不如直接简单粗暴地上WGAN吧！</p>
<h3 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h3><p>相关代码见<a href="https://github.com/corenel/GAN-Zoo/tree/master/WGAN" target="_blank" rel="external">GAN-Zoo/WGAN</a>。WGAN在实现上主要有以下几点需要注意：</p>
<ul>
<li>判别器模型的输出不过Sigmoid层，而是取平均之后flatten到1维，输出的是Wasserstein距离而非分类结果。（<a href="https://github.com/corenel/GAN-Zoo/blob/master/WGAN/models.py#L81-L82" target="_blank" rel="external">code</a>）</li>
<li>由于判别器器在一个epoch中需要训练多次，因此不能用<code>for..in..</code>来loop整个书记，而是用<code>iterator</code>来迭代循环。（<a href="https://github.com/corenel/GAN-Zoo/blob/master/WGAN/main.py#L68-L69" target="_blank" rel="external">code</a>）</li>
<li>在生成器训练过25次之前，或者是之后每500次的时候，判别器在每个epoch内都需要训练100次而非默认的5次。这是为了让判别器在一开始就达到差不多最优的状态。（<a href="https://github.com/corenel/GAN-Zoo/blob/master/WGAN/main.py#L58-L64" target="_blank" rel="external">code</a>）</li>
<li>为了提升效率，优化生成器的时候用判别器得到loss不更新判别器的梯度（<a href="https://github.com/corenel/GAN-Zoo/blob/master/WGAN/main.py#L95-L97" target="_blank" rel="external">code</a>），优化判别器用生成器生成虚假样本时也不更新生成器的梯度（<a href="https://github.com/corenel/GAN-Zoo/blob/master/WGAN/main.py#L78-L83" target="_blank" rel="external">code</a>）。</li>
<li>在生成器的optimizer更新权值之后clamp所有的权值，使其在一定范围内，以满足K-Lipschitz条件。（<a href="https://github.com/corenel/GAN-Zoo/blob/master/WGAN/main.py#L88-L90" target="_blank" rel="external">code</a>）</li>
</ul>
<h3 id="WGAN-GP-1"><a href="#WGAN-GP-1" class="headerlink" title="WGAN-GP"></a>WGAN-GP</h3><p>相关代码见<a href="https://github.com/corenel/GAN-Zoo/tree/master/WGAN-GP" target="_blank" rel="external">GAN-Zoo/WGAN-GP</a>。WGAN-GP在实现上与WGAN的主要区别在于：</p>
<ul>
<li>用gradient penalty代替了weight clipping。这里面涉及到计算梯度的梯度，这在PyTorch 0.2.0版本前是无法做到的，我猜大概也是因为这个原因，原作者选择了用TensorFlow来实现WGAN-GP。关于在低版本的PyTorch上实现gradient penalty的讨论<a href="https://discuss.pytorch.org/t/how-to-implement-gradient-penalty-in-pytorch/1656" target="_blank" rel="external">见此</a>。PyTorch版本更新之后增加了<code>torch.autograd.grad</code>这么一个函数，能够满足我们的需求。</li>
<li>在判别器中不使用Batch Normalization，详细原因在前文已经给出。可以选择的替代有Layer Normaliztion，Weight Normalization等。</li>
</ul>
<p>在代码与实验中，有以下几点需要注意：</p>
<ul>
<li>在训练的初始阶段，由于增大了判别器的训练次数（1个epoch训练100次），判别器接近最优，导致刚开始的loss非常大（~ -150000）。在判别器训练次数恢复正常后（1个epoch训练5次），loss的绝对值逐渐下降到几千几百的级别。迭代10000次左右即收敛到比较好的效果。</li>
<li>我的代码实现里，对于判别器模型没有使用任何Normalization，加上的话收敛速度应该会有一定提升。</li>
</ul>
<!-- more -->
<h2 id="Further-Reading"><a href="#Further-Reading" class="headerlink" title="Further Reading"></a>Further Reading</h2><p>最后总结一些我在学习过程中看过的比较好的资料以及代码实现：</p>
<ul>
<li><a href="https://github.com/yunjey/pytorch-tutorial" target="_blank" rel="external">pytorch-tutorial</a>: 我见到的最好的PyTorch入门教程，简洁清晰明了，有其他DL框架使用经验以及Python基础的朋友适用。一上来看不懂的话，可以先看官方的60分钟入门教程之后，再看这个。</li>
<li><a href="http://arxiv.org/abs/1701.00160" target="_blank" rel="external">NIPS 2016 Tutorial: Generative Adversarial Networks</a>: Iran Goodfellow在NIPS2016上的教程演讲，很好地介绍了GANs的基本思想和应用。前面的数学推导看不懂的话，可以结合<a href="http://www.deeplearningbook.org/" target="_blank" rel="external">DeepLearningBook</a>（<a href="http://www.epubit.com.cn/book/details/4278" target="_blank" rel="external">中译版</a>已经上市，本人忝为校对之一）。同时，还可以结合slides看，slides上的都写得很简练，tutorial中则做了详细的说明。可惜的是，当时WGAN及其变种还没有出来，因此在这篇tutorial中没有提到。</li>
<li>几篇代表论文以及相关的代码实现：<ul>
<li>GAN: <a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="external">paper</a>, <a href="https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/generative_adversarial_network/main.py#L34-L50" target="_blank" rel="external">code (pytorch-tutorial)</a></li>
<li>DCGAN: <a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="external">paper</a>, <a href="https://github.com/pytorch/examples/tree/master/dcgan" target="_blank" rel="external">code (PyTorch official example)</a>, <a href="https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/deep_convolutional_gan" target="_blank" rel="external">code (pytorch-tutorial)</a></li>
<li>WGAN: <a href="https://arxiv.org/abs/1701.07875" target="_blank" rel="external">paper</a>, <a href="https://github.com/martinarjovsky/WassersteinGAN" target="_blank" rel="external">code (PyTorch)</a></li>
<li>WGAN-GP: <a href="https://arxiv.org/abs/1704.00028" target="_blank" rel="external">https://arxiv.org/abs/1704.00028</a>, <a href="https://github.com/caogang/wgan-gp" target="_blank" rel="external">code (PyTorch)</a></li>
</ul>
</li>
<li>我自己对上述论文的代码实现，欢迎指正：<a href="https://github.com/corenel/GAN-Zoo" target="_blank" rel="external">GAN-Zoo</a></li>
<li>一些有趣的GANs应用<ul>
<li><a href="http://make.girls.moe/technical_report.pdf" target="_blank" rel="external">Create Anime Characters with A.I. !</a>：一篇非常有意思的技术文章，生成的头像插图质量非常高。（<a href="http://make.girls.moe/" target="_blank" rel="external">online demo</a>）</li>
</ul>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在看关于GANs的论文，并且自己动手用PyTorch写了一些经典文章的实现，想要稍微总结一下，故有此文。在最后我总结了我自己看过的有关GANs的一些比较好的资源，希望对读者有所帮助。&lt;/p&gt;
    
    </summary>
    
      <category term="Thesis Notes" scheme="http://www.yuthon.com/categories/Thesis-Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="GANs" scheme="http://www.yuthon.com/tags/GANs/"/>
    
      <category term="Generative Adversarial Networks" scheme="http://www.yuthon.com/tags/Generative-Adversarial-Networks/"/>
    
      <category term="DCGAN" scheme="http://www.yuthon.com/tags/DCGAN/"/>
    
      <category term="WGAN" scheme="http://www.yuthon.com/tags/WGAN/"/>
    
      <category term="WGAN-GP" scheme="http://www.yuthon.com/tags/WGAN-GP/"/>
    
  </entry>
  
  <entry>
    <title>Let&#39;s talk about Zero-Shot Learning.</title>
    <link href="http://www.yuthon.com/2017/06/14/Let-s-talk-about-zero-shot-learning/"/>
    <id>http://www.yuthon.com/2017/06/14/Let-s-talk-about-zero-shot-learning/</id>
    <published>2017-06-14T06:20:01.000Z</published>
    <updated>2017-06-14T06:43:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看Zero-Shot learning方面的文章，有些想要记录备忘的东西，就写在这儿吧。</p>
<p>诸位如果对此有兴趣，可以参考以下的讲座或者文章：</p>
<ul>
<li><a href="https://staff.fnwi.uva.nl/t.e.j.mensink/zsl2017/" target="_blank" rel="external">Zero-Shot Learning Tutorial | CVPR 2017</a> （未开始）</li>
<li><a href="https://staff.fnwi.uva.nl/t.e.j.mensink/zsl2016/#home" target="_blank" rel="external">Zero-Shot Learning Tutorial | ECCV 2016</a> （上一讲座16年的版本，<a href="http://isis-data.science.uva.nl/tmensink/docs/zsl16.web.pdf" target="_blank" rel="external">Slides</a>）</li>
<li><a href="https://arxiv.org/abs/1704.08345" target="_blank" rel="external">Semantic Autoencoder for Zero-Shot Learning</a> （CVPR2017）</li>
<li><a href="https://arxiv.org/abs/1703.04394" target="_blank" rel="external">Zero-Shot Learning-The Good, the Bad and the Ugly</a>（CVPR2017）</li>
<li><a href="http://www.deeplearningbook.org/contents/representation.html" target="_blank" rel="external">DeepLearningBook Chapter 15 Representation Learning</a> （中文版见<a href="https://github.com/exacity/deeplearningbook-chinese" target="_blank" rel="external">Github</a>）</li>
</ul>
<a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在看Zero-Shot learning方面的文章，有些想要记录备忘的东西，就写在这儿吧。&lt;/p&gt;
&lt;p&gt;诸位如果对此有兴趣，可以参考以下的讲座或者文章：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://staff.fnwi.uva.nl/t.e.j.mensink/zsl2017/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Zero-Shot Learning Tutorial | CVPR 2017&lt;/a&gt; （未开始）&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://staff.fnwi.uva.nl/t.e.j.mensink/zsl2016/#home&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Zero-Shot Learning Tutorial | ECCV 2016&lt;/a&gt; （上一讲座16年的版本，&lt;a href=&quot;http://isis-data.science.uva.nl/tmensink/docs/zsl16.web.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Slides&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.08345&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Semantic Autoencoder for Zero-Shot Learning&lt;/a&gt; （CVPR2017）&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.04394&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Zero-Shot Learning-The Good, the Bad and the Ugly&lt;/a&gt;（CVPR2017）&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.deeplearningbook.org/contents/representation.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;DeepLearningBook Chapter 15 Representation Learning&lt;/a&gt; （中文版见&lt;a href=&quot;https://github.com/exacity/deeplearningbook-chinese&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Github&lt;/a&gt;）&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Thesis Notes" scheme="http://www.yuthon.com/categories/Thesis-Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="Semantic Segmentation" scheme="http://www.yuthon.com/tags/Semantic-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>Thesis Notes for Amortized Inference and Learning in Latent CRF</title>
    <link href="http://www.yuthon.com/2017/05/10/Thesis-Notes-for-Amortized-Inference-and-Learning-in-Latent-CRF/"/>
    <id>http://www.yuthon.com/2017/05/10/Thesis-Notes-for-Amortized-Inference-and-Learning-in-Latent-CRF/</id>
    <published>2017-05-10T14:05:31.000Z</published>
    <updated>2017-05-30T14:05:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>This is my notes for <strong>Amortized Inference and Learning in Latent Conditional Random Fields for Weakly-Supervised Semantic Image Segmentation</strong>.</p>
<ul>
<li><a href="https://arxiv.org/abs/1705.01262" target="_blank" rel="external">arXiv:1705.01262</a></li>
<li><a href="http://www.ece.iisc.ernet.in/~divsymposium/EECS2017/slides_posters/EECS_2017_paper_31.pdf" target="_blank" rel="external">Poster &amp; Slides</a></li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>CRF属于判别式的图模型，通常被用来标注或分析序列资料。2014年左右的时候，CRF也被用来做图像分割。但是随着深度学习的兴起，CRF慢慢就沦为了进行post-precessing的工具，以优化结果。一般的做法就是用分割网络输出的pixel-level的各类的概率分布作为CRF的unary potential，然后用<em>Efficient inference in fully connected CRFs with Gaussian edge potentials</em>里面的方法来设置pairwise potential。</p>
<p>强监督的语义分割用这个方法当然很好，但是弱监督的话，因为没有pixel-level的标签，要把image-level的标签有选择地广播到所有像素还是一个很艰苦的工作。通常用的方法是基于localization cues的，比如说用saliency maps、objectness priors或者bounding boxes等等，就像之前介绍的<a href="http://www.yuthon.com/2017/04/28/Thesis-Notes-for-SEC/">SEC</a>一样。</p>
<p>不过这篇文章里面用的不是localization cues的那一套，而是<strong>把pixel-level的标签作为CRF的latent variables（隐变量），然后把图像本身以及image-level的标签作为observed variables（观测变量）</strong>。然后想通过训练inference network以估计在给定observed variables的情况下latent variables的后验分布（posterior distribution）这样的方法，把latent variables的inference cost分摊到整个数据集中。值得注意的是，这种方法并没有学习CRF的uanry potential，而是要训练inference network，使其输出的pixel-level标签在全局上与image-level标签一致，在局部上与附近的标签一致。</p>
<blockquote>
<p>讲真上边这一句话我也读着不是很懂，慢慢看之后的说明就能理解了。</p>
</blockquote>
<p>这种方法的好处在于，inference network（也就是分割网络）的训练可以是end-to-end的，并不需要另外去生成localization cues。此外，用这种方法训练出来的模型也不需要post-processing步骤。</p>
<p>这种方法的预测精度与使用了saliency masks的方法（比如说SEC）相近，也算是一种新的研究方向。</p>
<h2 id="The-proposed-model"><a href="#The-proposed-model" class="headerlink" title="The proposed model"></a>The proposed model</h2><table>
<thead>
<tr>
<th style="text-align:center">Symbol</th>
<th style="text-align:left">Description</th>
<th style="text-align:left">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$x^{(i)}$</td>
<td style="text-align:left">第$i$幅图像</td>
<td style="text-align:left">$1\le i \le n$</td>
</tr>
<tr>
<td style="text-align:center">$y^{(i)}$</td>
<td style="text-align:left">第$i$幅图像对应的image-level标签</td>
<td style="text-align:left">Each $y^{(i)}$ is a boolean vector whose length equals the number of classes used for training.</td>
</tr>
<tr>
<td style="text-align:center">$z^{(i)}=(z^{(i)}_j)$</td>
<td style="text-align:left">第$i$幅图像对应pixel-level标签</td>
<td style="text-align:left">$1\le j\le m$, and we use one-hot encoding for $z^{(i)}_j$</td>
</tr>
</tbody>
</table>
<p>本文的目标是，给定$(x^{(i)}, y^{(i)}), 1\le i \le n$的情况下，本算法希望能够学习到一个从$x^{(i)}$到$y^{(i)}$的映射关系，其能够对pixel-level的标注$z^{(i)}$做出推断。</p>
<p><img src="/images/the_latent_CRF.png" alt="the_latent_CRF"></p>
<p>当给定图像$x$的时候，image-level的标签$y$的条件分布如下所示，目标则是能最大化$\sum^n_{i=1} log p(y^{(i)}|x^{(i)})$。<br>$$<br>p(y|x) = \sum_z p(z|x)p(y|z,x)<br>$$<br>定义$p(z|x)$为CRF的pairwise项，同时也是<strong>先验概率（Prior）</strong>：<br>$$<br>p(z|x) \propto exp \left(-\sum_{j&lt;j’} k(t_j, t_{j’})\mu (z_j, z_{j’}))\right)<br>$$<br>其中$t_j$为位置$j$处像素的feature vector，$\mu (z_j, z_{j’})$为两个标签之间的compatibility。$p(z|x)$的意义在于<strong>使得相邻且色彩相似的像素具有同一标签。</strong>需要注意的是$p(z|x)$并不用训练，相反，需要学习的是作为CRF的unary potential的$p(y|z,x)$。</p>
<p>不过由于从$p(z|x,y)$进行采样在计算上十分困难，因此不能用EM法来最大化$\log p(y|z,x)$，具体分析可见原论文。因此需要用其他方法来达成目的。该文章使用的方法是最大化$p(y|x)$的下界，来使得$p(y|x)$最大。</p>
<h3 id="Variational-Lower-Bound"><a href="#Variational-Lower-Bound" class="headerlink" title="Variational Lower Bound"></a>Variational Lower Bound</h3><p>用了一个变分分布$q(z|x,y)$，通过以下变换得到了$\log p(y|x)$的一个下界：<br>$$<br>\begin{align}<br>\log p(y|x) &amp; = \log \sum _z p(z|x) p(y|z,x) \\<br>&amp; = \log \sum _z q(z|y,x) \frac{p(y|z,x)p(z|x)}{q(z|y,x)} \\<br>&amp; \ge \sum _z q(z|y,x) \log \frac{p(y|z,x)p(z|x)}{q(z|y,x)} \\<br>&amp; = -KL(q(z|y,x)||p(z|x)) + E_{q(z|x,y)} \log p(y|z,x)<br>\end{align}<br>$$<br>下界的第一项使得变分分布$q(z|y,x)$趋向于与图像$x$的先验分布$p(z|x)$接近，也就是说，使得$q所产生的分割遮罩在一定程度上遵循图像中的位置信息与色彩信息。也就是保持了局部的一致性。下界的第二项则能够使得变分分布所产生的遮罩能够提升图像分类的得分，即确保了pixel-level的标签与image-level标签一致。</p>
<p>假设变分分布$q(z|y,x)$可以被完全地因式分解，其具体定义如下：<br>$$<br>q(z|x,y)= \prod _{j=1}^m q(z_j | y,x) \<br>$$</p>
<p>$$<br>q(z_{jk}=1|x,y) = \frac{\exp (g_{jk}(x))}{\sum ^K_{k’=1} \exp (g_{jk’}(x))} \equiv \varphi_{jk} (x)<br>$$</p>
<p>其中$g$是全卷积网络或者说是分割网络，${g_{jk}(x), 1 \le j \le m, 1\le k \le K }$则是$g$对$x$的输出。</p>
<p><img src="/images/AI-LCRF_inference_network.png" alt="AI-LCRF_inference_network"></p>
<h3 id="Gradient-of-the-Lower-Bound"><a href="#Gradient-of-the-Lower-Bound" class="headerlink" title="Gradient of the Lower Bound"></a>Gradient of the Lower Bound</h3><p>下界的第一项其实是基于$g$的输出的KL散度损失，该损失的梯度可以精确计算出来的。第二项的梯度可以使用MCMC采样来估计。</p>
<p><img src="/images/AI-LCRF_total_network.png" alt="AI-LCRF_total_network"></p>
<p>具体的推导过程可以看原论文的2.2节。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在PASCAL VOC 2012上的表现很不错，能够与SEC等方法相媲美了。</p>
<p><img src="/images/AI-LCRF_results.png" alt="AI-LCRF_results"></p>
<p><img src="/images/AI-LCRF_quantitative_results.png" alt="AI-LCRF_quantitative_results"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>这是语义分割领域里面，第一篇在CRF中使用CNN来做inference network的文章，效果还很不错；</li>
<li>该方法相比所有的不用saliency maps的方法来说，结果都要好；</li>
<li>该方法的预测精度和用了saliency maps的方法相差无几</li>
<li>该方法表明传统的概率模型与CNN结合之后能得到很好的效果。</li>
</ul>
<h2 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h2><p>个人感觉这篇文章所用的方法还是很有借鉴意义的，不用很多tricks，不靠saliency maps就能达到这么高的mIoU。可以改进的地方我想应该有两个，一个是$p(z|x,y)$的形式可以修改，比如说参考ScribbleSup的，把纹理、色彩、空间之类的低阶信息都考虑上。另外，$p(y_k |x,z)$的计算方式也可以修改。总而言之，这还是一片比较粗糙的文章，还有很多需要精雕细琢的地方。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This is my notes for &lt;strong&gt;Amortized Inference and Learning in Latent Conditional Random Fields for Weakly-Supervised Semantic Image Segmentation&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1705.01262&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;arXiv:1705.01262&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.ece.iisc.ernet.in/~divsymposium/EECS2017/slides_posters/EECS_2017_paper_31.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Poster &amp;amp; Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Thesis Notes" scheme="http://www.yuthon.com/categories/Thesis-Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="Semantic Segmentation" scheme="http://www.yuthon.com/tags/Semantic-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>Thesis Notes for SEC</title>
    <link href="http://www.yuthon.com/2017/04/28/Thesis-Notes-for-SEC/"/>
    <id>http://www.yuthon.com/2017/04/28/Thesis-Notes-for-SEC/</id>
    <published>2017-04-28T01:13:33.000Z</published>
    <updated>2017-05-10T14:06:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>This is my notes for <strong>Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation</strong>.</p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1603.06098" target="_blank" rel="external">https://arxiv.org/abs/1603.06098</a></li>
<li>github: <a href="https://github.com/kolesman/SEC" target="_blank" rel="external">https://github.com/kolesman/SEC</a></li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本篇论文主要是介绍了针对 image-level 弱监督语义分割的一种新的 loss function。这个 loss function 由三部分组成：</p>
<ul>
<li><strong>seeding loss</strong>：针对使用 Image calssification CNN 来进行 object localization 的问题。传统的分类网络（比如 AlexNet 或者说 VGG）能够生成可靠的定位线索（即 <strong>seeds</strong> ），然是在预测物体确切的空间范围上就不怎么样了。这个 seeding loss 主要就是来使得分割网络能够准确地匹配到物体的定位线索（localization cues），但是不要扩展得太开，基本上就是只标出物体的中心位置，而无视图像中的其他不相关的部分。</li>
<li><strong>expansion loss</strong>：因为是要用 image-level 的标注来训练份额网络，所以需要考虑用 global pooling 来将预测出的 mask 转换成 image-level label scrores。这个 pooling 层的选择很大程度上决定了最终训练得到的分割网络的预测质量。一般来说，global max-pooling 倾向于低估物体的尺寸，而global average-pooling 则倾向于高估。本文使用了一种叫做 <strong>global weighted rank pooling</strong> 的方法来将物体的 seeds 扩展到一个尺寸比较合适的区域来计分，从而计算 expansion loss。这种方法是 max-pooling 和 average-pooling 的一种泛化版本，并且性能比这两者要高。</li>
<li><strong>constrain-to-boundary loss</strong>：根据 image-level label 训练出来的网络很少能够捕捉到图像中物体的精确边缘，而且仅仅在测试的时候在网络后面套一层全连接的 CRF 也往往难以完全克服这个缺陷。这是因为按照传统方法训练出来的网络，即便是对于未标记的区域，也倾向于有比较高的置信度。于是本文提出了一种新的 loss， 能够使得网络在训练的时候就减轻不精确的边缘预测。这一 loss 主要是对预测得到的 mask 进行约束，使其能在一定程度上遵循低层次的图像信息，特别是物体的边缘这样的重要信息。</li>
</ul>
<p>文中将这一方法称为<strong>SEC</strong>，即 <strong>S</strong>eed，<strong>E</strong>xpand和<strong>C</strong>onstrain。之后将分块介绍其具体实现。</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>现有的 image-level 弱监督语义分割的方法主要可以分为三类：</p>
<ul>
<li><strong>基于图的方法（graph-based Model）</strong>：根据图像内或图像间的部分（segments）或是超像素（superpixels）的相似性来推断其标签。</li>
<li><strong>多实例学习（multiple instance learning）</strong>：使用 per-image loss function 来进行训练，本质上是保持了图像中能够被用来生成 mask 的一种空间表示。<ul>
<li>例如 MIL-FCN 以及 MIL-ILP，其区别主要在于 pooling 策略，也就是说它们如何将其内在的空间表示转换到 per-image labels。</li>
</ul>
</li>
<li><strong>自学习（self-training）</strong>：模型本身使用基于 EM-like 的方法来生成 pixel-level 的标注，然后再来训练 fully-supervised 的分割网络。<ul>
<li>例如 EM-Adapt 以及 CCNN，其区别在于如何将 per-image annotation 转换到 mask 并保持其一致性。</li>
<li>SN_B 又增加了额外的步骤来创建于合并多个目标的proposal。</li>
</ul>
</li>
</ul>
<p>文中介绍的 SEC 包含了后两类方法，即其既使用了 per-image 的 loss，也使用了 per-pixel 的 loss 形式。</p>
<h2 id="Weakly-supervised-segmentation-from-image-level-labels"><a href="#Weakly-supervised-segmentation-from-image-level-labels" class="headerlink" title="Weakly supervised segmentation from image-level labels"></a>Weakly supervised segmentation from image-level labels</h2><h3 id="Table-of-Symbols"><a href="#Table-of-Symbols" class="headerlink" title="Table of Symbols"></a>Table of Symbols</h3><table>
<thead>
<tr>
<th style="text-align:center">Symbol</th>
<th style="text-align:center">Description</th>
<th style="text-align:center">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$\chi$</td>
<td style="text-align:center">the space of images</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">$X_i$</td>
<td style="text-align:center">an image</td>
<td style="text-align:center">$X_i \in \chi$</td>
</tr>
<tr>
<td style="text-align:center">$N$</td>
<td style="text-align:center">number of images</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">$Y$</td>
<td style="text-align:center">a segmentation mask</td>
<td style="text-align:center">$Y = (y_1 \dots y_n)$</td>
</tr>
<tr>
<td style="text-align:center">$y_i$</td>
<td style="text-align:center">a semantic label at $i$ spatial location</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">$n$</td>
<td style="text-align:center">number of spatial locations</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">$u$</td>
<td style="text-align:center">a location</td>
<td style="text-align:center">$u\in {1,2,\dots n}$</td>
</tr>
<tr>
<td style="text-align:center">$C$</td>
<td style="text-align:center">a set of all labels</td>
<td style="text-align:center">$C=C’ \cup {c^{bg}}$, size is $k$</td>
</tr>
<tr>
<td style="text-align:center">$C’$</td>
<td style="text-align:center">a set of all foreground labels</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">$c^{bg}$</td>
<td style="text-align:center">a background label</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">$k$</td>
<td style="text-align:center">number of kinds of labels</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">$c$</td>
<td style="text-align:center">a label</td>
<td style="text-align:center">$c\in C$</td>
</tr>
<tr>
<td style="text-align:center">$T_i$</td>
<td style="text-align:center">a set of weakly annotated foreground labels in an image</td>
<td style="text-align:center">$T_i \in C’$</td>
</tr>
<tr>
<td style="text-align:center">$S_c$</td>
<td style="text-align:center">a set of locations that are labeled with class $c$ by the weak localizatio procdure</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">$f(X;\theta)$</td>
<td style="text-align:center">segment network, briefly written as $f(X)$</td>
<td style="text-align:center">$f_{u,c}(X;\theta)=p(y_u=c \mid X)$</td>
</tr>
</tbody>
</table>
<h3 id="The-SEC-loss-for-weakly-supervised-image-segmentation"><a href="#The-SEC-loss-for-weakly-supervised-image-segmentation" class="headerlink" title="The SEC loss for weakly supervised image segmentation"></a>The SEC loss for weakly supervised image segmentation</h3><p>本节将介绍 SEC 的 loss function 的三个组成部分，其各自的功用如下：</p>
<ul>
<li>$L_{seed}$: 为网络预测提供提示（hint）</li>
<li>$L_{expand}$: 惩罚太小或者是搞错对象的预测 mask</li>
<li>$L_{constrain}$: 鼓励分割能够与图像空间以及色彩结构相适应</li>
</ul>
<p>本文的目标是训练一个参数为$\theta$的一个深度卷积神经网络$f(X;\theta)$，其能够预测在任一位置$u\in {1,2, \dots , n}$上任一标签$c\in C$的条件概率，也就是说，$f_{u,c}(X;\theta)=p(y_u=c \mid X)$。</p>
<p>该网络的训练即为下式的优化问题：<br>$$<br>min_{\theta} \sum_{(X,T)\in D}[L_{seed}(f(X;\theta), T) + L_{expand}(f(X;\theta), T) + L_{constrain}(X, f(X;\theta))]<br>$$</p>
<p>整个网络的结构如下图所示：</p>
<p><img src="/images/illustratio_of_SEC.png" alt="A schematic illustration of SEC"></p>
<h4 id="Seeding-loss-with-localization-cues"><a href="#Seeding-loss-with-localization-cues" class="headerlink" title="Seeding loss with localization cues"></a>Seeding loss with localization cues</h4><p>Image-level 的标签本身是不能提供语义目标的定位信息的，但是以 CNN 为基础的 Classification Network 却能够提供一个比较弱的定位信息（<strong>weak localization</strong>），如下图所示。</p>
<p><img src="/images/illustration_of_weak_localization.png" alt="The schematic illustration of the weak localization procedure"></p>
<p>当然，这种 weak location 或者说 location cues 并没有精确到能直接拿来当 full and accurate segmentation masks 的地步。不过还是可以用来指导弱监督分割网络的。文中提到的 <em>seeding loss</em> 主要是用来使得网络只匹配 weak localization 的 landmark，而无视图像中的其他部分。<em>seeding loss</em> $L_{seed}$ 的定义如下：<br>$$<br>L_{seed}(f(X), T, S_c) = - \frac{1}{\sum_{c\in T} |S_c|} \sum_{c\in T} \sum_{u\in S_c} log f_{u,c}(X)<br>$$<br>需要注意的是，文章中的 weak location 是预先计算好的，并非在训练过程中生成。前景用的是 <em>Learning deep features for discriminative localization</em>，背景用的是 <em>Deep inside convolutional networks: Visualising image classification models and saliency maps</em> 中的方法</p>
<h4 id="Expansion-loss-with-global-weighted-rank-pooling"><a href="#Expansion-loss-with-global-weighted-rank-pooling" class="headerlink" title="Expansion loss with global weighted rank pooling"></a>Expansion loss with global weighted rank pooling</h4><p>要度量分割后的 mask 与原来的 image-level 标签的一致程度的话，可以把每个像素的分割的得分合起来形成一个总的分类得分，然后再套上个 loss function 就能用来训练多标签的图像分类了。一般来说，有两种比较常用的方法：</p>
<ul>
<li>一个是 <strong>GMP (global max-pooling)</strong>，对于一张图像$X$，每个类$c$的得分就是所有像素的该类得分的最大值$max_{u\in {1,\dots,n}} f_{u,c} (X)$。GMP 仅仅鼓励单个位置的响应变得很高，因此常常低估了目标的大小。</li>
<li>另外一个就是 GAP (global average-pooling) ，得分是所有像素该类得分的平均值$\frac{1}{n} \sum ^n _{u=1} f_{u,c} (X)$。而 GAP 鼓励所有的响应都变高，因此常常高估了目标的大小。</li>
</ul>
<p>文中提出了一个叫做 <strong>GWRP</strong> 的方法，具体来说是这样的：对于一个类$c\in C$，定义其预测得分的一个降序排列$I^c = {i_1, \dots, i_n}$，即$f_{i_1, c} (x) \ge f_{i_2, c} (x) \ge \dots \ge f_{i_n, c} (x)$ 。同时令$d_c$为类别$c$得分的衰减系数。那么 GWRP 的分类得分$G_c(f(X), d_c)$可定义为：<br>$$<br>G_c (f(X); d_c) = \frac{1}{Z(d_c)} \sum ^n _{j=1} (d_c)^{-1} f_{i_j, c} (X) \text{, where } Z(d_c) = \sum ^n _{j=1} (d_c) ^ {j-1}<br>$$<br>值得注意的是，当$d_c = 0$时，GWRP 即为 GMP （$0^0=1$），而当$d_c = 1$时，GWRP 为 GAP。也就是说， GWRP 是前述两种方法的一个泛化形式，通过$d_c$来控制。</p>
<p>原则上来说，可以给每个类和每张图片设置不同的衰减系数$d_c$，不过这就需要知道每个类里面的物体通常的大小这样的先验知识，显然我们用的弱监督是没有这类信息的。因此，文中只设置了三个不同的$d_c$：</p>
<ul>
<li>$d_{+}$：在图像中出现了的类别的衰减系数</li>
<li>$d_{-}$：在图像中未出现的类别的衰减系数</li>
<li>$d_{bg}$：图像背景类别的衰减系数。</li>
</ul>
<p>这三个系数的具体取值见第四节。</p>
<p>综上，<em>expansion loss</em> 的完整形式如下：<br>$$<br>\begin{align} L_{expand}(f(X), T) = &amp;-\frac{1}{|T|} \sum _{c\in T} log G_c (f(X);d_{+}) \\ &amp;-\frac{1}{|C’ \backslash T|} \sum _{c\in C’\backslash T} log G_c (f(X);d_{-}) \\ &amp;- logG_{c^{bg}} (f(X);d_{bg}) \end{align}<br>$$</p>
<h4 id="Constrain-to-boundary-loss"><a href="#Constrain-to-boundary-loss" class="headerlink" title="Constrain-to-boundary loss"></a>Constrain-to-boundary loss</h4><p>使用 <em>constrain-to-boundary loss</em> 的思想是要惩罚网络，使其不要预测出和输入图像的色彩与空间信息不连续的分割。也就是说，要<strong>鼓励网络学习到生成与目标边界相匹配的分割 mask</strong>。</p>
<p>具体来说，就是构建了一个全连接的 CRF 层，$Q(X,f(X))$，unary potentials 用的是分割网络预测出的对数概率得分，pairwise potentials 则是只取决于图像像素的定值参数形式。为了与分割 mask 的分辨率相匹配，还要对图像 $X$ 降采样。具体的方式见第四节。</p>
<p>总而言之， <em>constrain-to-boundary loss</em> 被定义为网络输出与 CRF 输出之间的 KL 散度：<br>$$<br>L_{constrain} (X, f(X)) = \frac{1}{n} \sum ^n _{u=1} \sum _{c\in C} Q_{u,c} (X, f(X)) log \frac{Q_{u,c}(X, f(X))}{f_{u,c}(X)}<br>$$<br>这一构建形式的公式能够很好地达到预期目标。其能鼓励网络输出与 CRF 的输出接近，而 CRF 的输出又被确信是能够遵循图像边缘的。</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>既然要训练，那么反向传播肯定是要的，因此各个层都要是可微的。其它层都没什么问题，全连接 CRF 层的梯度计算方法见 <em>Random field model for integration of local informationand global information</em> 。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Experiment-setup"><a href="#Experiment-setup" class="headerlink" title="Experiment setup"></a>Experiment setup</h3><h4 id="Dataset-and-evaluation-metric"><a href="#Dataset-and-evaluation-metric" class="headerlink" title="Dataset and evaluation metric"></a>Dataset and evaluation metric</h4><p>文中<strong>用的数据集是 PASCAL VOC 2012</strong>，含背景在内共21个类别。原版的 VOC 2012 的语义分割部分的图像很少，训练集、验证集、测试集分别是1464、1449、1456张图像。所以文章里还<strong>用了 SegmentationAug 的扩充数据集</strong>，里面总共有10582张弱标注的图像，这个数量基本上够了。</p>
<p>至于与其他方法的对比是在验证集与测试集上进行比较。因为验证集的 ground truth 是向公众开放的，所以主要在验证集上研究一些 SEC 中不同组件之间的相互影响。而测试集必须在 PASCAL VOC 的官方服务器上才能得到结果的，因此就只是用来对比结果。</p>
<p><strong>性能度量用的是最常见的 mIoU</strong>。</p>
<h4 id="Segmentation-network"><a href="#Segmentation-network" class="headerlink" title="Segmentation network"></a>Segmentation network</h4><p>本文所选用的分割网络是基于 VGG-16 修改而来的 DeepLab-CRF-LargeFOV，输入为321x321，输出41x41的 mask。除了最后一层预测层初始化为均值为0方差为0.01的正态分布外，其余卷积层都按照 VGG 原本的方式初始化。</p>
<h4 id="Localization-network"><a href="#Localization-network" class="headerlink" title="Localization network"></a>Localization network</h4><p>定位网络也是根据 VGG-16 改的，训练的话是用 SegmentationAug 数据集训练一个多标签分类的问题。详细的方法和参数在文章的附录里面。值得注意的是，在训练的时候，为了提高效率，降低计算复杂度，localization cues是预先生成好的。</p>
<h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><p>用的是传统的 batched SGD来训练。总计8000次迭代，batch size为15，dropout rate为0.5，weight decay为0.0005。初始 learning rate 为0.001，每2000次迭代除以10。</p>
<p>硬件设备用的是 TITAN-X，训练一次大约7到8小时。</p>
<h4 id="Decay-parameters"><a href="#Decay-parameters" class="headerlink" title="Decay parameters"></a>Decay parameters</h4><p>经验法则如下：</p>
<ul>
<li>对于那些没有出现在图像中的类别，我们希望预测的像素越少越好。所以说令$d_{-}=0$，即使用GMP。</li>
<li>对于那些出现在图像中的类别，建议前10%的得分能够占到总得分之和的50%。对于41x41的mask来说，差不多相当于$d_{+}=0.996$。</li>
<li>对于背景类别，建议前30%的得分占到总得分之和的50%，在文中是$d_{bg}=0.999$</li>
</ul>
<h4 id="Fully-connected-CRF-at-training-time"><a href="#Fully-connected-CRF-at-training-time" class="headerlink" title="Fully-connected CRF at training time"></a>Fully-connected CRF at training time</h4><p>Pairwise parameter 照搬 <em>Efficient inference in fully connected CRFs with gaus- sian edge potentials</em> 这篇文章的默认值，除了把所有的空间距离项乘了12（为了与预测出来的 mask 大小相匹配，文中把原始图像降采样了）。</p>
<h4 id="Inference-at-test-time"><a href="#Inference-at-test-time" class="headerlink" title="Inference at test time"></a>Inference at test time</h4><p>分割网络最终输出的图像大小比原始图像小，因此还需要经过上采样以及过一遍全连接 CRF 来 refine。</p>
<p><img src="/images/schematic_illustration_at_test_time.png" alt="The schematic illustration of our approach at test time"></p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>（To be continued）</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This is my notes for &lt;strong&gt;Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;arxiv: &lt;a href=&quot;https://arxiv.org/abs/1603.06098&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1603.06098&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;github: &lt;a href=&quot;https://github.com/kolesman/SEC&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/kolesman/SEC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Thesis Notes" scheme="http://www.yuthon.com/categories/Thesis-Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="Semantic Segmentation" scheme="http://www.yuthon.com/tags/Semantic-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>Notes: From Faster R-CNN to Mask R-CNN</title>
    <link href="http://www.yuthon.com/2017/04/27/Notes-From-Faster-R-CNN-to-Mask-R-CNN/"/>
    <id>http://www.yuthon.com/2017/04/27/Notes-From-Faster-R-CNN-to-Mask-R-CNN/</id>
    <published>2017-04-27T04:55:33.000Z</published>
    <updated>2017-04-27T11:15:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>That’s my notes for the talk “From Faster-RCNN to Mask-RCNN” by Shaoqing Ren on April 26th, 2017.</p>
<h2 id="Yesterday-–-background-and-pre-works-of-Mask-R-CNN"><a href="#Yesterday-–-background-and-pre-works-of-Mask-R-CNN" class="headerlink" title="Yesterday – background and pre-works of Mask R-CNN"></a>Yesterday – background and pre-works of Mask R-CNN</h2><h3 id="Key-functions"><a href="#Key-functions" class="headerlink" title="Key functions"></a>Key functions</h3><ul>
<li><strong>Classification</strong> - What are in the image?</li>
<li><strong>Localization</strong> - Where are they?</li>
<li><strong>Mask (per pixel) classification</strong> - Where+ ?<ul>
<li>More precise to bounding box</li>
</ul>
</li>
<li><strong>Landmarks localization</strong> - What+, Where+ ?<ul>
<li>Not only per-pixel mask, but also key points in the objects</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h3 id="Mask-R-CNN-Architecture"><a href="#Mask-R-CNN-Architecture" class="headerlink" title="Mask R-CNN Architecture"></a>Mask R-CNN Architecture</h3><p><img src="/images/architecture_of_mask_r-cnn.png" alt="Mask R-CNN Architecture"></p>
<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p><img src="/images/classification_cnn.png" alt="CNN for classification"></p>
<blockquote>
<p>Please ignoring the bounding box in the image</p>
</blockquote>
<p>$$<br>\text{class} = Classifier(\text{image})<br>$$</p>
<h4 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h4><ul>
<li>High-level semantic concepts</li>
<li>High efficiency</li>
</ul>
<h4 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h4><ul>
<li><strong>SIFT</strong> or <strong>HOG</strong> (about 5 or 10 years ago)<ul>
<li>Based on edge feature (low- level semantic infomations)</li>
<li>Sometimes mistake two objects which people can distinguish easily<ul>
<li>e.g.  mark the telegraph pole as a man</li>
</ul>
</li>
</ul>
</li>
<li><strong>CNN</strong> (nowadays)<ul>
<li>Based on high-level semantic concepts<ul>
<li>Rarely mistake objects. If it do so, people are likely to mix up them, too.</li>
</ul>
</li>
<li>Translation invariance</li>
<li>Scale invariance</li>
</ul>
</li>
</ul>
<h3 id="Detection"><a href="#Detection" class="headerlink" title="Detection"></a>Detection</h3><p><img src="/images/detection_concept.png" alt="Detection Concept"><br>$$<br>\text{location}=Classifier(\text{all patches of an image)} \<br>\text{precise_location}=Regressor(\text{image}, \text{rough_location})<br>$$</p>
<h4 id="Problems-1"><a href="#Problems-1" class="headerlink" title="Problems"></a>Problems</h4><ul>
<li>High efficiency</li>
</ul>
<h4 id="Solutions-1"><a href="#Solutions-1" class="headerlink" title="Solutions"></a>Solutions</h4><ul>
<li><strong>Traverse</strong> all patches of an image and apply image classifier on them, then patches with highest scores are looked as locations of objects.<ul>
<li>As long as the classifier is precise enough, and we are able to traverse millions of patches in an image, we can always get a satisfactory result.</li>
<li>But the amount of calculations is too large. (about 1 or 10 millon) </li>
</ul>
</li>
<li>Do <strong>Regression</strong> cosntantlty, starting from a rough location of an object, and finally we’ll get the precise object location.<ul>
<li>Low amount of calculations. (about 10 or 100 times)</li>
<li>Hard to locate many adjacent and similar objects</li>
</ul>
</li>
<li>The state-of-the-art methods tend to use exhaustion on large-scale, and refine the rough localtions by regression on small-scale.</li>
</ul>
<h4 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h4><p><img src="/images/detection_rcnn.png" alt="RCNN"></p>
<ul>
<li>Use <strong>region proposal</strong> to decline millions of patches into 2~10k.</li>
<li>Use classifier to determine the class of a patch</li>
<li>Use BBox regression to refine the location</li>
</ul>
<h4 id="SPP-net-Fast-R-CNN"><a href="#SPP-net-Fast-R-CNN" class="headerlink" title="SPP-net / Fast R-CNN"></a>SPP-net / Fast R-CNN</h4><p><img src="/images/SPP_net_1.png" alt="SPP-net 1"></p>
<p><img src="/images/SPP_net_2.png" alt="SPP-net 2"></p>
<ul>
<li>Use <strong>Pyramid Pooling / RoI-Pooling</strong> to generate a fixed-length representation regardless of image size/scale </li>
</ul>
<h4 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h4><p><img src="/images/Faster-RCNN_1.png" alt="Faster-RCNN"></p>
<ul>
<li>Use <strong>RPN</strong> (Region Proposal Network) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. <ul>
<li>An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.</li>
<li>The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection.</li>
<li>We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with ‘attention’ mechanisms, the RPN component tells the unified network where to look. </li>
</ul>
</li>
<li>Number of patches: $width \times height \times scales \times ratios$<ul>
<li><code>scale</code> stands for the size of image and objects</li>
<li><code>ratio</code> stands for the aspect ratio of filter</li>
</ul>
</li>
</ul>
<p><img src="/images/multiple_scales_ratios.png" alt="Multiple scales / ratios"></p>
<p>Different schemes for addressing multiple scales and sizes.</p>
<ul>
<li>Pyramids of images and feature maps are built, and the classifier is run at all scales.</li>
<li>Pyramids of filters with multiple scales/sizes are run on the feature map.</li>
<li>Faster R-CNN use pyramids of reference boxes in the regression functions, which avoids enumerating images or filters of multiple scales or aspect ratios.</li>
</ul>
<h4 id="SSD-FPN"><a href="#SSD-FPN" class="headerlink" title="SSD / FPN"></a>SSD / FPN</h4><p><img src="/images/ssd_fpn.png" alt="SSD and FPN"></p>
<ul>
<li><strong>FPN (Feature Pyramid Network)</strong> exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales.</li>
</ul>
<h3 id="Instance-Segmentation"><a href="#Instance-Segmentation" class="headerlink" title="Instance Segmentation"></a>Instance Segmentation</h3><p><img src="/images/instance_segmentation.png" alt="Instance Segmentation"></p>
<ul>
<li>Use <strong>Mask Regression</strong> to predict instance segmentation based on object bounding box.</li>
<li>Replace RoI Pooling with <strong>RoI Align</strong></li>
</ul>
<h3 id="Keypoint-Detection"><a href="#Keypoint-Detection" class="headerlink" title="Keypoint Detection"></a>Keypoint Detection</h3><p><img src="/images/keypoint_detection.png" alt="Keypoint Detection"></p>
<ul>
<li>We make minor modifications to the segmentation system when adapting it for keypoints.</li>
<li>For each of the $K$ keypoints of an instance, the training target is a one-hot $m \times m$ binary mask where only a single pixel is labeled as foreground.</li>
</ul>
<h2 id="Today-details-about-Mask-RCNN-and-comparisons"><a href="#Today-details-about-Mask-RCNN-and-comparisons" class="headerlink" title="Today - details about Mask-RCNN and comparisons"></a>Today - details about Mask-RCNN and comparisons</h2><h3 id="RoI-Align"><a href="#RoI-Align" class="headerlink" title="RoI Align"></a>RoI Align</h3><p><img src="/images/roi_align_1.png" alt="RoI Align 1"></p>
<ul>
<li>RoI pooling contains two step of coordinates quantization: from original image into feature map (divide by stride) and from feature map into roi feature (use grid). <strong>Those quantizations cause a huge loss of location precision.</strong><ul>
<li>e.g. we have two boxes whose coordinate are 1.1 and 2.2, and the stride of feature map is 16, then they’re the same in the feature map.</li>
</ul>
</li>
<li><strong>RoI Align</strong> remove those two quantizations, and <strong>manipulate coordinates on continuous domain</strong>, which increase the location accuracy greatly.</li>
</ul>
<p><img src="/images/table_of_mask_rcnn.png" alt="Ablations for Mask R-CNN"></p>
<ul>
<li>RoI Align really improves the result.</li>
<li>Moreover, note that with RoIAlign, using stride-32 C5 features (30.9 AP) is more accurate than using stride-16 C4 features (30.3 AP, Table 2c). <strong>RoIAlign largely resolves the long-standing challenge of using large-stride features for detection and segmentation.</strong><ul>
<li>Without RoIAlign, AP in ResNet-50-C4 is better than that in C5 with RoIPooling, i.e., large stride is worse.Thus many precious work try to find methods to get better results in smaller stride. Now with RoIAlign, we can consider whether to use those tricks.</li>
</ul>
</li>
</ul>
<h3 id="Multinomial-vs-Independent-Masks"><a href="#Multinomial-vs-Independent-Masks" class="headerlink" title="Multinomial vs. Independent Masks"></a>Multinomial vs. Independent Masks</h3><ul>
<li><strong>Replace softmax with sigmoid</strong>.<ul>
<li>Mask R-CNN decouples mask and class prediction: as the existing box branch predicts the class label, we generate a mask for each class without competition among classes (by a per-pixel sigmoid and a binary loss).</li>
<li>In Table 2b, we compare this to using a per-pixel softmax and a multinomial loss (as com- monly used in FCN). This alternative couples the tasks of mask and class prediction, and results in a severe loss in mask AP (5.5 points).</li>
<li>The result suggests that <strong>once the instance has been classified as a whole (by the box branch), it is sufficient to predict a binary mask without concern for the categories</strong>, which makes the model easier to train.</li>
</ul>
</li>
</ul>
<h3 id="Multi-task-Cascade-vs-Joint-Learning"><a href="#Multi-task-Cascade-vs-Joint-Learning" class="headerlink" title="Multi-task Cascade vs. Joint Learning"></a>Multi-task Cascade vs. Joint Learning</h3><p><img src="/images/multitask_cascase_vs_joint_learning.png" alt="Multi-task Cascade vs. Joint Learning"></p>
<ul>
<li>Cascading and paralleling are adopted alternately.</li>
<li>On training time, three tasks of Mask R-CNN are <strong>paralleling trained</strong>.</li>
<li>But <strong>on testing time, we do classification and bbox regression first, and then use those results to get masks</strong>.<ul>
<li>BBox regression may change the location of bbox, so we should wait it to be done.</li>
<li>After bbox regression, we may adopt NMS or other methods to reduce the number of boxes. That decreases the workload of segmenting masks. </li>
</ul>
</li>
<li><strong>Adding the mask branch</strong> to the box-only (i.e., Faster R-CNN) or keypoint-only versions consistently <strong>improves these tasks</strong>.<ul>
<li>However, adding the keypoint branch reduces the box/mask AP slightly, suggest- ing that while keypoint detection benefits from multitask training, it does not in turn help the other tasks. </li>
<li>Nevertheless, learning all three tasks jointly enables a unified system to efficiently predict all outputs simultaneously (Figure 6).</li>
</ul>
</li>
</ul>
<p><img src="/images/table_of_mask_rcnn_2.png" alt="Table for Mask R-CNN"></p>
<h3 id="Comparison-on-Human-Keypoints"><a href="#Comparison-on-Human-Keypoints" class="headerlink" title="Comparison on Human Keypoints"></a>Comparison on Human Keypoints</h3><ul>
<li>Table 4 shows that our result (62.7 APkp) is 0.9 pointshigher than the COCO 2016 keypoint detection winner [4]that uses a multi-stage processing pipeline (see caption ofTable 4). Our method is considerably simpler and faster.</li>
<li>More importantly, we have a unified model that can si-multaneously predict boxes, segments, and keypoints whilerunning at 5 fps.</li>
</ul>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/images/keypoint_detection_result.png" alt="Keypoint detection results"></p>
<p><img src="/images/mask_rcnn_results.png" alt="More results of Mask R-CNN on COCO test images"></p>
<h2 id="Future-discussion"><a href="#Future-discussion" class="headerlink" title="Future - discussion"></a>Future - discussion</h2><h3 id="Order-of-key-functions"><a href="#Order-of-key-functions" class="headerlink" title="Order of key functions?"></a>Order of key functions?</h3><ul>
<li>Order of classification, localization, mask classification and landmarks localization?</li>
<li>Top down or Buttom up?<ul>
<li>Mask R-CNN uses Top-down method.</li>
<li>the COCO 2016 keypoint detection winner CMU-Pose+++ uses Buttom-up method.<ul>
<li>Detect key points first (don’t know which keypoint belongs to which person)’</li>
<li>Then gradually stitch them together</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Precious-amp-semantic-label"><a href="#Precious-amp-semantic-label" class="headerlink" title="Precious &amp; semantic label"></a>Precious &amp; semantic label</h3><p><img src="/images/precious_and_semantic_label.png" alt="Precious &amp; semantic label"></p>
<p>box-level label -&gt; instance segmentation &amp; keypoints detection -&gt; instance seg with body parts</p>
<h3 id="Semantic-3D-reconstruction"><a href="#Semantic-3D-reconstruction" class="headerlink" title="Semantic 3D reconstruction"></a>Semantic 3D reconstruction</h3><p><img src="/images/semantic_reconstruction.png" alt="Semantic 3D reconstruction"></p>
<h3 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h3><ul>
<li>the performance &amp; system improves rapidly</li>
<li>join a team, keep going</li>
<li>always try, thinking and discussion</li>
<li>understand and structure the world</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;That’s my notes for the talk “From Faster-RCNN to Mask-RCNN” by Shaoqing Ren on April 26th, 2017.&lt;/p&gt;
&lt;h2 id=&quot;Yesterday-–-background-and-pre-works-of-Mask-R-CNN&quot;&gt;&lt;a href=&quot;#Yesterday-–-background-and-pre-works-of-Mask-R-CNN&quot; class=&quot;headerlink&quot; title=&quot;Yesterday – background and pre-works of Mask R-CNN&quot;&gt;&lt;/a&gt;Yesterday – background and pre-works of Mask R-CNN&lt;/h2&gt;&lt;h3 id=&quot;Key-functions&quot;&gt;&lt;a href=&quot;#Key-functions&quot; class=&quot;headerlink&quot; title=&quot;Key functions&quot;&gt;&lt;/a&gt;Key functions&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Classification&lt;/strong&gt; - What are in the image?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Localization&lt;/strong&gt; - Where are they?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mask (per pixel) classification&lt;/strong&gt; - Where+ ?&lt;ul&gt;
&lt;li&gt;More precise to bounding box&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Landmarks localization&lt;/strong&gt; - What+, Where+ ?&lt;ul&gt;
&lt;li&gt;Not only per-pixel mask, but also key points in the objects&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Semantic Segmentation" scheme="http://www.yuthon.com/tags/Semantic-Segmentation/"/>
    
      <category term="Object Detection" scheme="http://www.yuthon.com/tags/Object-Detection/"/>
    
      <category term="Faster R-CNN" scheme="http://www.yuthon.com/tags/Faster-R-CNN/"/>
    
      <category term="Mask R-CNN" scheme="http://www.yuthon.com/tags/Mask-R-CNN/"/>
    
  </entry>
  
  <entry>
    <title>Modern C++ Libraries: Getting Started</title>
    <link href="http://www.yuthon.com/2017/04/25/Modern-C-Libraries-Getting-Started/"/>
    <id>http://www.yuthon.com/2017/04/25/Modern-C-Libraries-Getting-Started/</id>
    <published>2017-04-25T13:06:39.000Z</published>
    <updated>2017-04-27T07:48:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post is a note for Modern C++ Libraries in Pluralsight.</p>
<h2 id="Assertions"><a href="#Assertions" class="headerlink" title="Assertions"></a>Assertions</h2><h3 id="Course-content"><a href="#Course-content" class="headerlink" title="Course content"></a>Course content</h3><p>An <strong>Assertion</strong> may be a function, but usually a macro, that brings your application to an immediate standstill if an assumption is broken.</p>
<p>Assertions <strong>document the assumptions</strong> such that those assumptions can be <strong>validated</strong> usually at run time, but also increasingly at compile time.</p>
<a id="more"></a>
<ul>
<li><p>Check pointers before using them (at run time):</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">assert(pointer);</div></pre></td></tr></table></figure>
</li>
<li><p>Confirm the  validity of some input at the boundary of application or component (at run time):</p>
</li>
<li><p>Confirm some expressions at compile time (no need to execute)</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static_assert</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) == <span class="number">4</span>, <span class="string">"I can’t float like that!"</span>);</div></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>Static or compile time Assertions</strong> are checked staticially during build. And <strong>Runtime Assertions</strong> are typically conditionally complied and included owning debug builds and stripped out of release or free builds.</p>
<blockquote>
<p>Runtime Assertions must not include expression that the application relies upon. It should not change the application state in any way.</p>
</blockquote>
<h3 id="C-documentation"><a href="#C-documentation" class="headerlink" title="C++ documentation"></a>C++ documentation</h3><h4 id="assert"><a href="#assert" class="headerlink" title="assert"></a>assert</h4><p>The definition of the macro <code>assert</code> depends on another macro, <strong><code>NDEBUG</code></strong>, which is not defined by the standard library.</p>
<p>If <code>NDEBUG</code> is defined as a macro name at the point in the source code where <code>&lt;cassert&gt;</code> is included, then <code>assert</code> does nothing.</p>
<p>If <code>NDEBUG</code> is not defined, then <code>assert</code> checks if its argument (which must have scalar type) compares equal to zero. If it does, <code>assert</code> outputs implementation-specific diagnostic information on the standard error output and calls <em>std::abort</em>.</p>
<h5 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h5><p>Because <code>assert</code> is a <em>function-like macro</em>, commas anywhere in condition that are not protected by parentheses are interpreted as macro argument separators. Such commas are often found in template argument lists:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">assert(<span class="built_in">std</span>::is_same_v&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;); <span class="comment">// error: assert does not take two arguments</span></div><div class="line">assert((<span class="built_in">std</span>::is_same_v&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;)); <span class="comment">// OK: one argument</span></div><div class="line"><span class="keyword">static_assert</span>(<span class="built_in">std</span>::is_same_v&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;); <span class="comment">// OK: not a macro</span></div></pre></td></tr></table></figure>
<h5 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h5><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="comment">// uncomment to disable assert()</span></div><div class="line"><span class="comment">// #define NDEBUG</span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cassert&gt;</span></span></div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    assert(<span class="number">2</span>+<span class="number">2</span>==<span class="number">4</span>);</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Execution continues past the first assert\n"</span>;</div><div class="line">    assert(<span class="number">2</span>+<span class="number">2</span>==<span class="number">5</span>);</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Execution continues past the second assert\n"</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Possible output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Execution continues past the first assert</div><div class="line">test: test.cc:10: int main(): Assertion `2+2==5&apos; failed.</div><div class="line">Aborted</div></pre></td></tr></table></figure>
<h4 id="Static-Assertion"><a href="#Static-Assertion" class="headerlink" title="Static Assertion"></a>Static Assertion</h4><p>Performs compile-time assertion checking</p>
<h5 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h5><p>Since message has to be a string literal, it cannot contain dynamic information or even a constant expression that is not a string literal itself. In particular, it cannot contain the name of the template type argument.</p>
<h5 id="Example-1"><a href="#Example-1" class="headerlink" title="Example"></a>Example</h5><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;type_traits&gt;</span></span></div><div class="line"> </div><div class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></div><div class="line"><span class="class"><span class="title">void</span> <span class="title">swap</span>(<span class="title">T</span>&amp; <span class="title">a</span>, <span class="title">T</span>&amp; <span class="title">b</span>)</span></div><div class="line"><span class="class">&#123;</span></div><div class="line">    <span class="keyword">static_assert</span>(<span class="built_in">std</span>::is_copy_constructible&lt;T&gt;::value,</div><div class="line">                  <span class="string">"Swap requires copying"</span>);</div><div class="line">    <span class="keyword">static_assert</span>(<span class="built_in">std</span>::is_nothrow_move_constructible&lt;T&gt;::value</div><div class="line">               &amp;&amp; <span class="built_in">std</span>::is_nothrow_move_assignable&lt;T&gt;::value,</div><div class="line">                  <span class="string">"Swap may throw"</span>);</div><div class="line">    <span class="keyword">auto</span> c = b;</div><div class="line">    b = a;</div><div class="line">    a = c;</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></div><div class="line"><span class="class"><span class="title">struct</span> <span class="title">data_structure</span></span></div><div class="line"><span class="class">&#123;</span></div><div class="line">    <span class="keyword">static_assert</span>(<span class="built_in">std</span>::is_default_constructible&lt;T&gt;::value,</div><div class="line">                  <span class="string">"Data Structure requires default-constructible elements"</span>);</div><div class="line">&#125;;</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">no_copy</span></span></div><div class="line"><span class="class">&#123;</span></div><div class="line">    no_copy ( <span class="keyword">const</span> no_copy&amp; ) = <span class="keyword">delete</span>;</div><div class="line">    no_copy () = <span class="keyword">default</span>;</div><div class="line">&#125;;</div><div class="line"> </div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">no_default</span></span></div><div class="line"><span class="class">&#123;</span></div><div class="line">    no_default () = <span class="keyword">delete</span>;</div><div class="line">&#125;;</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="keyword">int</span> a, b;</div><div class="line">    swap(a, b);</div><div class="line"> </div><div class="line">    no_copy nc_a, nc_b;</div><div class="line">    swap(nc_a, nc_b); <span class="comment">// 1</span></div><div class="line"> </div><div class="line">    data_structure&lt;<span class="keyword">int</span>&gt; ds_ok;</div><div class="line">    data_structure&lt;no_default&gt; ds_error; <span class="comment">// 2</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Possible output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1: error: static assertion failed: Swap requires copying</div><div class="line">2: error: static assertion failed: Data Structure requires default-constructible elements</div></pre></td></tr></table></figure>
<h2 id="VERIFY"><a href="#VERIFY" class="headerlink" title="VERIFY"></a>VERIFY</h2><p><code>Verify</code> behaves exactly the same as Assert in debug builds. Indeed it is an Assertion. But in release builds it drops the verification, but keeps the expression. </p>
<p>Unlike runtime Assertions the Verify macro is used for those cases where the expression is essential to the applications operation and cannot simply be stripped out of release builds.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> NDEBUG</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> VERIFY assert</span></div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> VERIFY(experssion) (expression)</span></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div></pre></td></tr></table></figure>
<h2 id="TRACE"><a href="#TRACE" class="headerlink" title="TRACE"></a>TRACE</h2><p><code>Trace</code> macro is to provide formatted output for the debugger to display, but that can be stripped out of release builds. </p>
<p>Following code works only in VC++:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;windows.h&gt;</span></span></div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> _DEBUG</span></div><div class="line">inline auto Trace(wchar_t const * format, ...) -&gt; void</div><div class="line">&#123;</div><div class="line">    va_list args;</div><div class="line">    va_start(args, format);</div><div class="line"></div><div class="line">    <span class="keyword">wchar_t</span> buffer [<span class="number">256</span>];</div><div class="line"></div><div class="line">    ASSERT(<span class="number">-1</span> != _vsnwprintf_s(buffer,</div><div class="line">                               _countof(buffer) <span class="number">-1</span>,</div><div class="line">                               format,</div><div class="line">                               args));</div><div class="line"></div><div class="line">    va_end(args);</div><div class="line"></div><div class="line">    OutputDebugString(buffer);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Tracer</span></span></div><div class="line"><span class="class">&#123;</span></div><div class="line">    <span class="keyword">char</span> <span class="keyword">const</span> * m_filename;</div><div class="line">    <span class="keyword">unsigned</span> m_line;</div><div class="line"></div><div class="line">    Tracer(<span class="keyword">char</span> <span class="keyword">const</span> * filename, <span class="keyword">unsigned</span> <span class="keyword">const</span> line) :</div><div class="line">        m_filename &#123; filename &#125;,</div><div class="line">        m_line &#123; line &#125;</div><div class="line">    &#123;</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span>... Args&gt;</div><div class="line">    auto operator()(wchar_t const * format, Args... args) const -&gt; void</div><div class="line">    &#123;</div><div class="line">        <span class="keyword">wchar_t</span> buffer [<span class="number">256</span>];</div><div class="line"></div><div class="line">        <span class="keyword">auto</span> count = swprintf_s(buffer,</div><div class="line">                                <span class="string">L"%S(%d): "</span>,</div><div class="line">                                m_filename,</div><div class="line">                                m_line);</div><div class="line"></div><div class="line">        ASSERT(<span class="number">-1</span> != count);</div><div class="line"></div><div class="line">        ASSERT(<span class="number">-1</span> != _snwprintf_s(buffer + count,</div><div class="line">                                  _countof(buffer) - count,</div><div class="line">                                  _countof(buffer) - count - <span class="number">1</span>,</div><div class="line">                                  format,</div><div class="line">                                  args...));</div><div class="line"></div><div class="line">        OutputDebugString(buffer);</div><div class="line">    &#125;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> _DEBUG</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> TRACE Tracer(__FILE__, __LINE__)</span></div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> TRACE __noop</span></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line"></div><div class="line">auto main() -&gt; int</div><div class="line">&#123;</div><div class="line">    TRACE(<span class="string">L"1 + 2 = %d\n"</span>, <span class="number">1</span> + <span class="number">2</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="My-point"><a href="#My-point" class="headerlink" title="My point"></a>My point</h2><p>Assertions are useful, however, <code>VERIFY</code> and <code>TRACE</code> can be repalced by other logging libaries.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is a note for Modern C++ Libraries in Pluralsight.&lt;/p&gt;
&lt;h2 id=&quot;Assertions&quot;&gt;&lt;a href=&quot;#Assertions&quot; class=&quot;headerlink&quot; title=&quot;Assertions&quot;&gt;&lt;/a&gt;Assertions&lt;/h2&gt;&lt;h3 id=&quot;Course-content&quot;&gt;&lt;a href=&quot;#Course-content&quot; class=&quot;headerlink&quot; title=&quot;Course content&quot;&gt;&lt;/a&gt;Course content&lt;/h3&gt;&lt;p&gt;An &lt;strong&gt;Assertion&lt;/strong&gt; may be a function, but usually a macro, that brings your application to an immediate standstill if an assumption is broken.&lt;/p&gt;
&lt;p&gt;Assertions &lt;strong&gt;document the assumptions&lt;/strong&gt; such that those assumptions can be &lt;strong&gt;validated&lt;/strong&gt; usually at run time, but also increasingly at compile time.&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Pluralsight" scheme="http://www.yuthon.com/tags/Pluralsight/"/>
    
      <category term="C++" scheme="http://www.yuthon.com/tags/C/"/>
    
      <category term="C++11" scheme="http://www.yuthon.com/tags/C-11/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow r1.0 on TX1 (now successful)</title>
    <link href="http://www.yuthon.com/2017/03/10/TensorFlow-r1-0-on-TX1/"/>
    <id>http://www.yuthon.com/2017/03/10/TensorFlow-r1-0-on-TX1/</id>
    <published>2017-03-10T10:12:18.000Z</published>
    <updated>2017-03-17T08:27:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>TensorFlow r1.0已经发布了不少时间，事实证明1.0版本在内存使用上改善了不少，以前一些在r0.11上内存满报错的程序在r1.0上能够正常运行了。同时，r1.0相较于r0.11在API上做了很大的改动，也有很多新的东西（比如Keras）将要集成进TF。</p>
<p>总而言之，r1.0是未来的方向，所以说我希望将原先在TX1上装的r0.11换成r1.0。不过网上最新的教程还是只有r0.11的。<a href="https://github.com/rwightman" target="_blank" rel="external">rwightman</a>这位仁兄编译成功了r1.0alpha版本，并且放出了<a href="https://github.com/rwightman/tensorflow/releases/tag/v1.0.0-alpha-tegra-ugly_hack" target="_blank" rel="external">whl文件</a>，不过没有编译正式版。本文将阐述如何在TX1上安装TensorFlow r1.0的正式版本<del>，不过目前由于<code>nvcc</code>的一个bug，还没有编译成功</del>。</p>
<p>Update: 做了一些非常ugly的改动之后编译成功了。</p>
<a id="more"></a>
<h2 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h2><p>Thsi article aims to install TensorFlow r1.0 on NVIDIA Jetson TX1 with JetPack 2.3.1:</p>
<ul>
<li>Ubuntu 16.04 (aarch64)</li>
<li>CUDA 8.0</li>
<li>cuDNN 5.1.5 or 5.1.10</li>
</ul>
<blockquote>
<p><del>Note that I still <strong>CAN’T</strong> build TensorFlow r1.0 yet. The reason is explained at the end of this post.</del></p>
</blockquote>
<h2 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h2><p>Before installation, you need to add swap space for TX1 since this device only has 4G memory and 16GB eMMC Storage. I use an external HDD via USB. Maybe you could use SSD for higher speed. </p>
<p>Thanks to <a href="https://github.com/jetsonhacks" target="_blank" rel="external">jetsonhacks</a>, we can simply deal this with a script:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/jetsonhacks/postFlashTX1.git</div><div class="line">$ sudo ./createSwapfile.sh -d /path/to/swap/ -s 8</div></pre></td></tr></table></figure>
<p>8G swap is enough for compilation, and ensure you have <strong>&gt;5.5GB</strong> free space on TX1.</p>
<h2 id="Install-Deps"><a href="#Install-Deps" class="headerlink" title="Install Deps"></a>Install Deps</h2><p>Thanks to <a href="https://github.com/jetsonhacks" target="_blank" rel="external">jetsonhacks</a>, we can deal with deps more convinently. I forked this repo and modify something to fit TF r1.0. You can just clone mine.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/corenel/installTensorFlowTX1.git</div><div class="line">$ <span class="built_in">cd</span> installTensorFlowTX1</div><div class="line"><span class="comment"># tell the dynamic linker to use /usr/local/lib</span></div><div class="line">$ ./setLocalLib.sh</div><div class="line"><span class="comment"># install prerequisties</span></div><div class="line">$ ./installPrerequisites.sh</div></pre></td></tr></table></figure>
<blockquote>
<p>If you meet an error that bazel can’t find <code>cudnn.h</code> in <code>/usr/lib/aarch64-linux-gnu/</code>, just download cuDNN from NVIDIA Developers website and place it into that path.</p>
<p>Or you can just edit <code>setTensorFlowEV.sh</code> and replace <code>default_cudnn_path=/usr/lib/aarch64-linux-gnu/</code> with <code>default_cudnn_path=/usr/</code> since the default <code>cudnn.h</code> is in <code>/usr/include/</code>.</p>
</blockquote>
<h2 id="Build-TensorFlow"><a href="#Build-TensorFlow" class="headerlink" title="Build TensorFlow"></a>Build TensorFlow</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># clone tensorflow r1.0</span></div><div class="line">$ ./cloneTensorFlow.sh</div><div class="line"><span class="comment"># set environment variables for tensorflow</span></div><div class="line">$ ./setTensorFlowEV.sh</div><div class="line"><span class="comment"># build tensorflow</span></div><div class="line">$ ./buildTensorFlow.sh</div><div class="line"><span class="comment"># package builds into a wheel file</span></div><div class="line">$ ./packageTensorFlow.sh</div></pre></td></tr></table></figure>
<p>Then you’ll find your wheel file in your home folder.</p>
<h2 id="Install-and-Test"><a href="#Install-and-Test" class="headerlink" title="Install and Test"></a>Install and Test</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># install tensorflow</span></div><div class="line">$ pip install ~/tensorflow-1.0.0-py2-none-any.whl</div><div class="line"><span class="comment"># test tensorflow</span></div><div class="line">$ python -c <span class="string">'import tensorflow as tf; print(tf.__version__)'</span></div></pre></td></tr></table></figure>
<h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><p>Until now, I still can’t build tensorflow successfully. An error occured:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:2498:1: output <span class="string">'tensorflow/core/kernels/_objs/softmax_op_gpu/tensorflow/core/kernels/softmax_op_gpu.cu.pic.o'</span> was not created.</div><div class="line">ERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:2498:1: not all outputs were created or valid.</div><div class="line">Target //tensorflow/tools/pip_package:build_pip_package failed to build</div></pre></td></tr></table></figure>
<p>According to <a href="https://devtalk.nvidia.com/default/topic/987306/?comment=5059105" target="_blank" rel="external">this post</a>, this may due to a bug of <code>nvcc</code>. An expert in NVIDIA says they solved it with their internal nvcc compiler, which is not yet available in JetPack. Maybe next release of JetPack (3.0 on March 14) will solve it. So I’ll update this post then.</p>
<h2 id="An-ugly-hack"><a href="#An-ugly-hack" class="headerlink" title="An ugly hack"></a>An ugly hack</h2><p>Thanks to <a href="https://github.com/rwightman/tensorflow/commit/a1cde1d55f76a1d4eb806ba81d7c63fe72466e6d" target="_blank" rel="external">rwightman’s hack</a>,  I finally compiled TF1.0 successfully. Just following hacks:</p>
<ul>
<li>Revert Eigen to revision used in Tensorflow r0.11 to avoid cuda compile error</li>
<li>Remove expm1 op that was added with new additions to Eigen</li>
</ul>
<p>My fork for <code>installTensorFlowTX1</code> has contained this hack. And my build for TensorFlow r1.0 with Python 2.7 can be find <a href="https://www.dropbox.com/s/m6bgd3sq8kggul7/tensorflow-1.0.1-cp27-cp27mu-linux_aarch64.whl?dl=0" target="_blank" rel="external">here</a>.</p>
<blockquote>
<p><strong>Update</strong>: <a href="https://github.com/barty777" target="_blank" rel="external">@barty777</a> build TF 1.0.1 with Python 3.5, and his wheel file can be found <a href="https://drive.google.com/open?id=0B2jw9AHXtUJ_OFJDV19TWTEyaWc" target="_blank" rel="external">here</a>.</p>
</blockquote>
<h2 id="Acknowledgment"><a href="#Acknowledgment" class="headerlink" title="Acknowledgment"></a>Acknowledgment</h2><p>Thanks for following posts and issues:</p>
<ul>
<li>Github issue: <a href="https://github.com/tensorflow/tensorflow/issues/851" target="_blank" rel="external">tensorflow for Nvidia TX1</a></li>
<li>NVIDIA forum post: <a href="https://devtalk.nvidia.com/default/topic/901148/jetson-tx1/tensorflow-on-jetson-tx1/" target="_blank" rel="external">TensorFlow on Jetson TX1</a></li>
<li>My earilier blog post: <a href="http://www.yuthon.com/2016/12/04/Installation-of-TensorFlow-r0-11-on-TX1/">Installation of TensorFlow r0.11 on TX1</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TensorFlow r1.0已经发布了不少时间，事实证明1.0版本在内存使用上改善了不少，以前一些在r0.11上内存满报错的程序在r1.0上能够正常运行了。同时，r1.0相较于r0.11在API上做了很大的改动，也有很多新的东西（比如Keras）将要集成进TF。&lt;/p&gt;
&lt;p&gt;总而言之，r1.0是未来的方向，所以说我希望将原先在TX1上装的r0.11换成r1.0。不过网上最新的教程还是只有r0.11的。&lt;a href=&quot;https://github.com/rwightman&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;rwightman&lt;/a&gt;这位仁兄编译成功了r1.0alpha版本，并且放出了&lt;a href=&quot;https://github.com/rwightman/tensorflow/releases/tag/v1.0.0-alpha-tegra-ugly_hack&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;whl文件&lt;/a&gt;，不过没有编译正式版。本文将阐述如何在TX1上安装TensorFlow r1.0的正式版本&lt;del&gt;，不过目前由于&lt;code&gt;nvcc&lt;/code&gt;的一个bug，还没有编译成功&lt;/del&gt;。&lt;/p&gt;
&lt;p&gt;Update: 做了一些非常ugly的改动之后编译成功了。&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="NVIDIA" scheme="http://www.yuthon.com/tags/NVIDIA/"/>
    
      <category term="Jetson TX1" scheme="http://www.yuthon.com/tags/Jetson-TX1/"/>
    
      <category term="Ubuntu" scheme="http://www.yuthon.com/tags/Ubuntu/"/>
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://www.yuthon.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Backup system partition on TX1</title>
    <link href="http://www.yuthon.com/2016/12/18/Backup-system-partition-on-TX1/"/>
    <id>http://www.yuthon.com/2016/12/18/Backup-system-partition-on-TX1/</id>
    <published>2016-12-18T10:18:59.000Z</published>
    <updated>2016-12-18T10:57:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>由于实验室只有要用到多块 TX1 开发板, 然而一个个都用 JetPack 刷机, 再用自动化脚本装软件和依赖实在是太麻烦了, 因此我和梅老板就开始研究怎么直接备份 TX1 上的 Ubuntu 系统.</p>
<a id="more"></a>
<h2 id="Failed-attempts"><a href="#Failed-attempts" class="headerlink" title="Failed attempts"></a>Failed attempts</h2><p>最开始想的是直接用<code>dd</code>来备份整块 eMMC到外置的存储上, 于是尝试了</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo dd <span class="keyword">if</span>=/dev/mmcblk0p1 of=/media/ubuntu/backup/backup.img</div></pre></td></tr></table></figure>
<p>后来还发现可以用<code>ssh</code>来远程<code>dd</code></p>
<ul>
<li><strong>run from remote computer</strong>:</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ dd <span class="keyword">if</span>=/dev/mmcblk0p1 | gzip -1 - | ssh yuthon@mac dd of=image.gz</div></pre></td></tr></table></figure>
<ul>
<li><strong>run from local computer</strong>:</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ssh ubuntu@tx1 <span class="string">"dd if=/dev/mmcblk0p1 | gzip -1 -"</span> | dd of=image.gz</div></pre></td></tr></table></figure>
<p>之后, 我们发现<code>64_TX1/Linux_for_Tegra_64_tx1/rootfs</code>目录中应该就是之后需要拷到 TX1 的<code>/</code>目录下的内容. 因此我们直接将之前备份好的<code>bakcup.img</code>解压到了此目录下, 并使用 JetPack 重新 Flash OS.</p>
<p>最后的结果是 TX1 在重启后卡在了登录界面, 经典的 login-loop.</p>
<p>此方案, 扑街.</p>
<h2 id="Using-tegraflash-py"><a href="#Using-tegraflash-py" class="headerlink" title="Using tegraflash.py"></a>Using <code>tegraflash.py</code></h2><p>Then we found a <a href="https://devtalk.nvidia.com/default/topic/898999/tx1-r23-1-new-flash-structure-how-to-clone-/" target="_blank" rel="external">post</a> on NVIDIA Developer Forums, and method in this post works for us.</p>
<blockquote>
<p>Assumed we’re in <code>64_TX1/Linux_for_Tegra_64_tx1/bootloader</code> directory.</p>
</blockquote>
<h3 id="Backup-an-image"><a href="#Backup-an-image" class="headerlink" title="Backup an image"></a>Backup an image</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo ./tegraflash.py --bl cboot.bin --applet nvtboot_recovery.bin --chip 0x21 --cmd <span class="string">"read APP my_backup_image_APP.img"</span></div></pre></td></tr></table></figure>
<h3 id="Restore-an-image"><a href="#Restore-an-image" class="headerlink" title="Restore an image"></a>Restore an image</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo ./tegraflash.py --bl cboot.bin --applet nvtboot_recovery.bin --chip 0x21 --cmd <span class="string">"write APP my_backup_image_APP.img"</span></div></pre></td></tr></table></figure>
<h3 id="One-more-thing"><a href="#One-more-thing" class="headerlink" title="One more thing"></a>One more thing</h3><p>It’s recommended in the post to use <code>flash.py</code> front-end instead of <code>tegraflash.py</code> to make sure you use the same L4T release version.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Backup</span></div><div class="line">$ sudo flash.sh -S 14580MiB jetson-tx1 mmcblk0p1</div><div class="line"><span class="comment"># Restore</span></div><div class="line">$ sudo flash.sh -r -S 14580MiB jetson-tx1 mmcblk0p1</div></pre></td></tr></table></figure>
<p>Note that the <code>-r</code>  param re-uses <code>system.img</code> in <code>bootloader</code> directory, and if a clone file is there in place, that installs the clone.<br><strong>I haven’t tried this method, maybe you could have a try and report.</strong></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于实验室只有要用到多块 TX1 开发板, 然而一个个都用 JetPack 刷机, 再用自动化脚本装软件和依赖实在是太麻烦了, 因此我和梅老板就开始研究怎么直接备份 TX1 上的 Ubuntu 系统.&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Tensorflow" scheme="http://www.yuthon.com/tags/Tensorflow/"/>
    
      <category term="NVIDIA" scheme="http://www.yuthon.com/tags/NVIDIA/"/>
    
      <category term="Jetson TX1" scheme="http://www.yuthon.com/tags/Jetson-TX1/"/>
    
      <category term="Ubuntu" scheme="http://www.yuthon.com/tags/Ubuntu/"/>
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Installation of TensorFlow r0.11 on TX1</title>
    <link href="http://www.yuthon.com/2016/12/04/Installation-of-TensorFlow-r0-11-on-TX1/"/>
    <id>http://www.yuthon.com/2016/12/04/Installation-of-TensorFlow-r0-11-on-TX1/</id>
    <published>2016-12-04T12:53:55.000Z</published>
    <updated>2016-12-14T14:09:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天折腾了一个下午, 特此记录一下其中遇到的坑, 主要还是因为 TX1 的 aarch64 架构, 以及小得可怜的内存与存储容量.</p>
<a id="more"></a>
<h2 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h2><ul>
<li><strong>Hardware</strong>: NVIDIA Jetson TX1 Developer Kit</li>
<li><strong>Software</strong>: JetPack 2.3.1<ul>
<li>Ubuntu 16.04 64-bit (aarch64)</li>
<li>CUDA 8.0</li>
<li>cuDNN 5.1</li>
</ul>
</li>
</ul>
<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>建议全程开HTTP/HTTPS代理, 否则国内下载速度堪忧.</p>
<h3 id="Install-Java"><a href="#Install-Java" class="headerlink" title="Install Java"></a>Install Java</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo add-apt-repository ppa:webupd8team/java</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install oracle-java8-installer</div></pre></td></tr></table></figure>
<h3 id="Install-dependencens"><a href="#Install-dependencens" class="headerlink" title="Install dependencens"></a>Install dependencens</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install git zip unzip autoconf automake libtool curl zlib1g-dev maven</div><div class="line">$ sudo apt-get install python-numpy swig python-dev python-wheel</div></pre></td></tr></table></figure>
<h3 id="Build-protobuf"><a href="#Build-protobuf" class="headerlink" title="Build protobuf"></a>Build protobuf</h3><p>这里测 Protobuf 要编译两份, 分别给 grpc 和 Bazel 用.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># For grpc-java build</span></div><div class="line">$ git <span class="built_in">clone</span> https://github.com/google/protobuf.git</div><div class="line">$ <span class="built_in">cd</span> protobuf</div><div class="line">$ git checkout master</div><div class="line">$ ./autogen.sh</div><div class="line">$ git checkout v3.0.0-beta-3</div><div class="line">$ ./autogen.sh</div><div class="line">$ LDFLAGS=-static ./configure --prefix=$(<span class="built_in">pwd</span>)/../</div><div class="line">$ sed -i -e <span class="string">'s/LDFLAGS = -static/LDFLAGS = -all-static/'</span> ./src/Makefile</div><div class="line">$ make -j 4</div><div class="line">$ make install</div><div class="line"></div><div class="line"><span class="comment"># For bazel build</span></div><div class="line">$ git checkout v3.0.0-beta-2</div><div class="line">$./autogen.sh</div><div class="line">$ LDFLAGS=-static ./configure --prefix=$(<span class="built_in">pwd</span>)/../</div><div class="line">$ sed -i -e <span class="string">'s/LDFLAGS = -static/LDFLAGS = -all-static/'</span> ./src/Makefile</div><div class="line">$ make -j 4</div><div class="line">$ <span class="built_in">cd</span> ..</div></pre></td></tr></table></figure>
<blockquote>
<p>注意: 给 Bazel 用的不用<code>make install</code>, 之后直接<code>cp</code>过去.</p>
</blockquote>
<h3 id="Build-grpc-java-compiler"><a href="#Build-grpc-java-compiler" class="headerlink" title="Build grpc-java compiler"></a>Build grpc-java compiler</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/neo-titans/odroid.git</div><div class="line">$ git <span class="built_in">clone</span> https://github.com/grpc/grpc-java.git</div><div class="line">$ <span class="built_in">cd</span> grpc-java/</div><div class="line">$ git checkout v0.15.0</div><div class="line">$ patch -p0 &lt; ../odroid/build_tensorflow/grpc-java.v0.15.0.patch</div><div class="line">$ CXXFLAGS=<span class="string">"-I<span class="variable">$(pwd)</span>/../include"</span> LDFLAGS=<span class="string">"-L<span class="variable">$(pwd)</span>/../lib"</span> ./gradlew java_pluginExecutable -Pprotoc=$(<span class="built_in">pwd</span>)/../bin/protoc</div><div class="line">$ <span class="built_in">cd</span> ..</div></pre></td></tr></table></figure>
<h3 id="Build-bazel"><a href="#Build-bazel" class="headerlink" title="Build bazel"></a>Build bazel</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/bazelbuild/bazel.git</div><div class="line">$ <span class="built_in">cd</span> bazel</div><div class="line">$ git checkout 0.3.2</div><div class="line">$ cp ../protobuf/src/protoc third_party/protobuf/protoc-linux-arm32.exe</div><div class="line">$ cp ../grpc-java/compiler/build/exe/java_plugin/protoc-gen-grpc-java third_party/grpc/protoc-gen-grpc-java-0.15.0-linux-arm32.exe</div></pre></td></tr></table></figure>
<p>在编译 Bazel 之前, 还需要改一些配置, 使得 Bazel 将 aarch64 认作 arm64, 以便编译成功.</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line">diff --git a/compile.sh b/compile.sh</div><div class="line">index 53fc412..11035d9 100755</div><div class="line"><span class="comment">--- a/compile.sh</span></div><div class="line"><span class="comment">+++ b/compile.sh</span></div><div class="line">@@ -27,7 +27,7 @@ cd "$(dirname "$0")"</div><div class="line"> # Set the default verbose mode in buildenv.sh so that we do not display command</div><div class="line"> # output unless there is a failure.  We do this conditionally to offer the user</div><div class="line"> # a chance of overriding this in case they want to do so.</div><div class="line"><span class="deletion">-: $&#123;VERBOSE:=no&#125;</span></div><div class="line"><span class="addition">+: $&#123;VERBOSE:=yes&#125;</span></div><div class="line"></div><div class="line"> source scripts/bootstrap/buildenv.sh</div><div class="line"></div><div class="line">diff --git a/scripts/bootstrap/compile.sh b/scripts/bootstrap/compile.sh</div><div class="line">index 77372f0..657b254 100755</div><div class="line"><span class="comment">--- a/scripts/bootstrap/compile.sh</span></div><div class="line"><span class="comment">+++ b/scripts/bootstrap/compile.sh</span></div><div class="line">@@ -48,6 +48,7 @@ linux)</div><div class="line">   else</div><div class="line">     if [ "$&#123;MACHINE_IS_ARM&#125;" = 'yes' ]; then</div><div class="line">       PROTOC=$&#123;PROTOC:-third_party/protobuf/protoc-linux-arm32.exe&#125;</div><div class="line"><span class="addition">+      GRPC_JAVA_PLUGIN=$&#123;GRPC_JAVA_PLUGIN:-third_party/grpc/protoc-gen-grpc-java-0.15.0-linux-arm32.exe&#125;</span></div><div class="line">     else</div><div class="line">       PROTOC=$&#123;PROTOC:-third_party/protobuf/protoc-linux-x86_32.exe&#125;</div><div class="line">       GRPC_JAVA_PLUGIN=$&#123;GRPC_JAVA_PLUGIN:-third_party/grpc/protoc-gen-grpc-java-0.15.0-linux-x86_32.exe&#125;</div><div class="line">@@ -150,7 +151,7 @@ function java_compilation() &#123;</div><div class="line"></div><div class="line">   run "$&#123;JAVAC&#125;" -classpath "$&#123;classpath&#125;" -sourcepath "$&#123;sourcepath&#125;" \</div><div class="line">       -d "$&#123;output&#125;/classes" -source "$JAVA_VERSION" -target "$JAVA_VERSION" \</div><div class="line"><span class="deletion">-      -encoding UTF-8 "@$&#123;paramfile&#125;"</span></div><div class="line"><span class="addition">+      -encoding UTF-8 "@$&#123;paramfile&#125;" -J-Xmx500M</span></div><div class="line"></div><div class="line">   log "Extracting helper classes for $name..."</div><div class="line">   for f in $&#123;library_jars&#125; ; do</div><div class="line">diff --git a/src/main/java/com/google/devtools/build/lib/util/CPU.java b/src/main/java/com/google/devtools/build/lib/util/CPU.java</div><div class="line">index 41af4b1..4d80610 100644</div><div class="line"><span class="comment">--- a/src/main/java/com/google/devtools/build/lib/util/CPU.java</span></div><div class="line"><span class="comment">+++ b/src/main/java/com/google/devtools/build/lib/util/CPU.java</span></div><div class="line">@@ -26,7 +26,7 @@ public enum CPU &#123;</div><div class="line">   X86_32("x86_32", ImmutableSet.of("i386", "i486", "i586", "i686", "i786", "x86")),</div><div class="line">   X86_64("x86_64", ImmutableSet.of("amd64", "x86_64", "x64")),</div><div class="line">   PPC("ppc", ImmutableSet.of("ppc", "ppc64", "ppc64le")),</div><div class="line"><span class="deletion">-  ARM("arm", ImmutableSet.of("arm", "armv7l")),</span></div><div class="line"><span class="addition">+  ARM("arm", ImmutableSet.of("arm", "armv7l", "aarch64")),</span></div><div class="line">   UNKNOWN("unknown", ImmutableSet.&lt;String&gt;of());</div><div class="line"></div><div class="line">   private final String canonicalName;</div><div class="line">diff --git a/third_party/grpc/BUILD b/third_party/grpc/BUILD</div><div class="line">index 2ba07e3..c7925ff 100644</div><div class="line"><span class="comment">--- a/third_party/grpc/BUILD</span></div><div class="line"><span class="comment">+++ b/third_party/grpc/BUILD</span></div><div class="line">@@ -29,7 +29,7 @@ filegroup(</div><div class="line">         "//third_party:darwin": ["protoc-gen-grpc-java-0.15.0-osx-x86_64.exe"],</div><div class="line">         "//third_party:k8": ["protoc-gen-grpc-java-0.15.0-linux-x86_64.exe"],</div><div class="line">         "//third_party:piii": ["protoc-gen-grpc-java-0.15.0-linux-x86_32.exe"],</div><div class="line"><span class="deletion">-        "//third_party:arm": ["protoc-gen-grpc-java-0.15.0-linux-x86_32.exe"],</span></div><div class="line"><span class="addition">+        "//third_party:arm": ["protoc-gen-grpc-java-0.15.0-linux-arm32.exe"],</span></div><div class="line">         "//third_party:freebsd": ["protoc-gen-grpc-java-0.15.0-linux-x86_32.exe"],</div><div class="line">     &#125;),</div><div class="line"> )</div><div class="line">diff --git a/third_party/protobuf/BUILD b/third_party/protobuf/BUILD</div><div class="line">index 203fe51..4c2a316 100644</div><div class="line"><span class="comment">--- a/third_party/protobuf/BUILD</span></div><div class="line"><span class="comment">+++ b/third_party/protobuf/BUILD</span></div><div class="line">@@ -28,6 +28,7 @@ filegroup(</div><div class="line">         "//third_party:darwin": ["protoc-osx-x86_32.exe"],</div><div class="line">         "//third_party:k8": ["protoc-linux-x86_64.exe"],</div><div class="line">         "//third_party:piii": ["protoc-linux-x86_32.exe"],</div><div class="line"><span class="addition">+        "//third_party:arm": ["protoc-linux-arm32.exe"],</span></div><div class="line">         "//third_party:freebsd": ["protoc-linux-x86_32.exe"],</div><div class="line">     &#125;),</div><div class="line"> )</div><div class="line">diff --git a/tools/cpp/cc_configure.bzl b/tools/cpp/cc_configure.bzl</div><div class="line">index aeb0715..688835d 100644</div><div class="line"><span class="comment">--- a/tools/cpp/cc_configure.bzl</span></div><div class="line"><span class="comment">+++ b/tools/cpp/cc_configure.bzl</span></div><div class="line">@@ -150,7 +150,12 @@ def _get_cpu_value(repository_ctx):</div><div class="line">     return "x64_windows"</div><div class="line">   # Use uname to figure out whether we are on x86_32 or x86_64</div><div class="line">   result = repository_ctx.execute(["uname", "-m"])</div><div class="line"><span class="deletion">-  return "k8" if result.stdout.strip() in ["amd64", "x86_64", "x64"] else "piii"</span></div><div class="line"><span class="addition">+  machine = result.stdout.strip()</span></div><div class="line"><span class="addition">+  if machine in ["arm", "armv7l", "aarch64"]:</span></div><div class="line"><span class="addition">+   return "arm"</span></div><div class="line"><span class="addition">+  elif machine in ["amd64", "x86_64", "x64"]:</span></div><div class="line"><span class="addition">+   return "k8"</span></div><div class="line"><span class="addition">+  return "piii"</span></div><div class="line"></div><div class="line"></div><div class="line"> _INC_DIR_MARKER_BEGIN = "#include &lt;...&gt;"</div></pre></td></tr></table></figure>
<p>之后编译安装:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ ./compile.sh </div><div class="line">$ sudo cp output/bazel /usr/<span class="built_in">local</span>/bin</div><div class="line">$ <span class="built_in">cd</span> ..</div></pre></td></tr></table></figure>
<h3 id="Build-Tensorflow"><a href="#Build-Tensorflow" class="headerlink" title="Build Tensorflow"></a>Build Tensorflow</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ git clone https://github.com/tensorflow/tensorflow.git</div><div class="line">$ git checkout r0.11</div></pre></td></tr></table></figure>
<p>同样地, 修改配置文件:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div></pre></td><td class="code"><pre><div class="line">diff --git a/tensorflow/core/kernels/BUILD b/tensorflow/core/kernels/BUILD</div><div class="line">index 2e04827..867aaca 100644</div><div class="line"><span class="comment">--- a/tensorflow/core/kernels/BUILD</span></div><div class="line"><span class="comment">+++ b/tensorflow/core/kernels/BUILD</span></div><div class="line">@@ -1184,7 +1184,7 @@ tf_kernel_libraries(</div><div class="line">         "segment_reduction_ops",</div><div class="line">         "scan_ops",</div><div class="line">         "sequence_ops",</div><div class="line"><span class="deletion">-        "sparse_matmul_op",</span></div><div class="line"><span class="addition">+       #DC "sparse_matmul_op",</span></div><div class="line">     ],</div><div class="line">     deps = [</div><div class="line">         ":bounds_check",</div><div class="line">diff --git a/tensorflow/core/kernels/cwise_op_gpu_select.cu.cc b/tensorflow/core/kernels/cwise_op_gpu_select.cu.cc</div><div class="line">index 02058a8..880252c 100644</div><div class="line"><span class="comment">--- a/tensorflow/core/kernels/cwise_op_gpu_select.cu.cc</span></div><div class="line"><span class="comment">+++ b/tensorflow/core/kernels/cwise_op_gpu_select.cu.cc</span></div><div class="line">@@ -43,8 +43,14 @@ struct BatchSelectFunctor&lt;GPUDevice, T&gt; &#123;</div><div class="line">     const int all_but_batch = then_flat_outer_dims.dimension(1);</div><div class="line"></div><div class="line"> #if !defined(EIGEN_HAS_INDEX_LIST)</div><div class="line"><span class="deletion">-    Eigen::array&lt;int, 2&gt; broadcast_dims&#123;&#123; 1, all_but_batch &#125;&#125;;</span></div><div class="line"><span class="deletion">-    Eigen::Tensor&lt;int, 2&gt;::Dimensions reshape_dims&#123;&#123; batch, 1 &#125;&#125;;</span></div><div class="line"><span class="addition">+    // Eigen::array&lt;int, 2&gt; broadcast_dims&#123;&#123; 1, all_but_batch &#125;&#125;;</span></div><div class="line"><span class="addition">+    Eigen::array&lt;int, 2&gt; broadcast_dims;</span></div><div class="line"><span class="addition">+    broadcast_dims[0] = 1;</span></div><div class="line"><span class="addition">+    broadcast_dims[1] = all_but_batch;</span></div><div class="line"><span class="addition">+    // Eigen::Tensor&lt;int, 2&gt;::Dimensions reshape_dims&#123;&#123; batch, 1 &#125;&#125;;</span></div><div class="line"><span class="addition">+    Eigen::Tensor&lt;int, 2&gt;::Dimensions reshape_dims;</span></div><div class="line"><span class="addition">+    reshape_dims[0] = batch;</span></div><div class="line"><span class="addition">+    reshape_dims[1] = 1;</span></div><div class="line"> #else</div><div class="line">     Eigen::IndexList&lt;Eigen::type2index&lt;1&gt;, int&gt; broadcast_dims;</div><div class="line">     broadcast_dims.set(1, all_but_batch);</div><div class="line">diff --git a/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc b/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc</div><div class="line">index a177696..75b67ba 100644</div><div class="line"><span class="comment">--- a/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc</span></div><div class="line"><span class="comment">+++ b/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc</span></div><div class="line">@@ -104,9 +104,17 @@ struct SparseTensorDenseMatMulFunctor&lt;GPUDevice, T, ADJ_A, ADJ_B&gt; &#123;</div><div class="line">     int n = (ADJ_B) ? b.dimension(0) : b.dimension(1);</div><div class="line"></div><div class="line"> #if !defined(EIGEN_HAS_INDEX_LIST)</div><div class="line"><span class="deletion">-    Eigen::Tensor&lt;int, 2&gt;::Dimensions matrix_1_by_nnz&#123;&#123; 1, nnz &#125;&#125;;</span></div><div class="line"><span class="deletion">-    Eigen::array&lt;int, 2&gt; n_by_1&#123;&#123; n, 1 &#125;&#125;;</span></div><div class="line"><span class="deletion">-    Eigen::array&lt;int, 1&gt; reduce_on_rows&#123;&#123; 0 &#125;&#125;;</span></div><div class="line"><span class="addition">+    // Eigen::Tensor&lt;int, 2&gt;::Dimensions matrix_1_by_nnz&#123;&#123; 1, nnz &#125;&#125;;</span></div><div class="line"><span class="addition">+    Eigen::Tensor&lt;int, 2&gt;::Dimensions matrix_1_by_nnz;</span></div><div class="line"><span class="addition">+    matrix_1_by_nnz[0] = 1;</span></div><div class="line"><span class="addition">+    matrix_1_by_nnz[1] = nnz;</span></div><div class="line"><span class="addition">+    // Eigen::array&lt;int, 2&gt; n_by_1&#123;&#123; n, 1 &#125;&#125;;</span></div><div class="line"><span class="addition">+    Eigen::Tensor&lt;int, 2&gt;::Dimensions matrix_1_by_nnz;</span></div><div class="line"><span class="addition">+    matrix_1_by_nnz[0] = 1;</span></div><div class="line"><span class="addition">+    matrix_1_by_nnz[1] = nnz;</span></div><div class="line"><span class="addition">+    // Eigen::array&lt;int, 2&gt; n_by_1&#123;&#123; n, 1 &#125;&#125;;</span></div><div class="line"><span class="addition">+    Eigen::array&lt;int, 2&gt; n_by_1;</span></div><div class="line"><span class="addition">+    n_by_1[0] = n;</span></div><div class="line"><span class="addition">+    n_by_1[1] = 1;</span></div><div class="line"><span class="addition">+    // Eigen::array&lt;int, 1&gt; reduce_on_rows&#123;&#123; 0 &#125;&#125;;</span></div><div class="line"><span class="addition">+    Eigen::array&lt;int, 1&gt; reduce_on_rows;</span></div><div class="line"><span class="addition">+    reduce_on_rows[0]= 0;</span></div><div class="line"> #else</div><div class="line">     Eigen::IndexList&lt;Eigen::type2index&lt;1&gt;, int&gt; matrix_1_by_nnz;</div><div class="line">     matrix_1_by_nnz.set(1, nnz);</div><div class="line">diff --git a/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc b/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc</div><div class="line">index 52256a7..1d027b9 100644</div><div class="line"><span class="comment">--- a/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc</span></div><div class="line"><span class="comment">+++ b/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc</span></div><div class="line">@@ -888,6 +888,9 @@ CudaContext* CUDAExecutor::cuda_context() &#123; return context_; &#125;</div><div class="line"> // For anything more complicated/prod-focused than this, you'll likely want to</div><div class="line"> // turn to gsys' topology modeling.</div><div class="line"> static int TryToReadNumaNode(const string &amp;pci_bus_id, int device_ordinal) &#123;</div><div class="line"><span class="addition">+// DC - make this clever later. ARM has no NUMA node, just return 0</span></div><div class="line"><span class="addition">+LOG(INFO) &lt;&lt; "ARM has no NUMA node, hardcoding to return zero";</span></div><div class="line"><span class="addition">+return 0;</span></div><div class="line"> #if defined(__APPLE__)</div><div class="line">   LOG(INFO) &lt;&lt; "OS X does not support NUMA - returning NUMA node zero";</div><div class="line">   return 0;</div></pre></td></tr></table></figure>
<p>之后即可编译:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ ./configure</div><div class="line">$ bazel build -c opt --<span class="built_in">jobs</span> 2 --local_resources 1024,4.0,1.0 --config=cuda //tensorflow/tools/pip_package:build_pip_package</div><div class="line">$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg</div><div class="line"><span class="comment"># The name of the .whl file will depend on your platform.</span></div><div class="line">$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.11.0-py2-none-any.whl</div></pre></td></tr></table></figure>
<p>这里有我自己编译好的 <a href="https://drive.google.com/open?id=0B0AsKkiz_kZRZG9BbFRxZ1FYWTg" target="_blank" rel="external">tensorflow_gpu-0.11.0-py2-none-aarch64.whl</a>, 可供使用.</p>
<h3 id="Start-using-Tensorflow"><a href="#Start-using-Tensorflow" class="headerlink" title="Start using Tensorflow"></a>Start using Tensorflow</h3><p>首先还得装一下 OpenCV 的 Python port:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install -y libopencv4tegra–python</div></pre></td></tr></table></figure>
<p>之后即可测试 TensorFlow 是否成功安装:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ python</div><div class="line">Python 2.7.12 (default, Nov 19 2016, 06:48:10)</div><div class="line">[GCC 5.4.0 20160609] on linux2</div><div class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> or <span class="string">"license"</span> <span class="keyword">for</span> more information.</div><div class="line">&gt;&gt;&gt; import tensorflow as tf</div><div class="line">I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally</div><div class="line">I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally</div><div class="line">I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally</div><div class="line">I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally</div><div class="line">I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure>
<p>不报错即是安装成功.</p>
<h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><h3 id="Swap-Memory"><a href="#Swap-Memory" class="headerlink" title="Swap Memory"></a>Swap Memory</h3><p>如果编译失败, 很有可能是内存不足的原因, 因此可以外接U盘或是SSD等, 并且将一部分缓存放在上面.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> /path/to/your/storage</div><div class="line">$ fallocate -l 8G swapfile</div><div class="line">$ chmod 600 swapfile</div><div class="line">$ mkswap swapfile</div><div class="line">$ sudo swapon swapfile</div><div class="line">$ swapon -s</div></pre></td></tr></table></figure>
<p>8G的 swap 空间应该是够用了, 如果还嫌不够可以再设个大点的.</p>
<p>之后再运行 Bazel 编译:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bazel build -c opt --local_resources 3072,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package</div></pre></td></tr></table></figure>
<h3 id="Build-on-external-storage"><a href="#Build-on-external-storage" class="headerlink" title="Build on external storage"></a>Build on external storage</h3><p>整个安装过程所需的空间大概是3G以上, 而 TX1 装完系统之后只剩下了 4G 的剩余空间. 所以最好将安装时的根目录选在外置的存储上, 以免因为内置存储空间不足而导致失败.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="http://stackoverflow.com/questions/39783919/tensorflow-on-nvidia-tx1/" target="_blank" rel="external">http://stackoverflow.com/questions/39783919/tensorflow-on-nvidia-tx1/</a></li>
<li><a href="https://github.com/tensorflow/tensorflow/issues/851" target="_blank" rel="external">https://github.com/tensorflow/tensorflow/issues/851</a></li>
<li><a href="https://www.neotitans.net/install-tensorflow-on-odroid-c2.html" target="_blank" rel="external">https://www.neotitans.net/install-tensorflow-on-odroid-c2.html</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天折腾了一个下午, 特此记录一下其中遇到的坑, 主要还是因为 TX1 的 aarch64 架构, 以及小得可怜的内存与存储容量.&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Tensorflow" scheme="http://www.yuthon.com/tags/Tensorflow/"/>
    
      <category term="NVIDIA" scheme="http://www.yuthon.com/tags/NVIDIA/"/>
    
      <category term="Jetson TX1" scheme="http://www.yuthon.com/tags/Jetson-TX1/"/>
    
      <category term="Ubuntu" scheme="http://www.yuthon.com/tags/Ubuntu/"/>
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Train YOLOv2 on my own dataset</title>
    <link href="http://www.yuthon.com/2016/12/03/Train-YOLOv2-on-my-own-dataset/"/>
    <id>http://www.yuthon.com/2016/12/03/Train-YOLOv2-on-my-own-dataset/</id>
    <published>2016-12-03T03:29:04.000Z</published>
    <updated>2016-12-03T10:15:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看 Darkflow 的时候, 发现连 YOLOv2 都出了, 据称 mAP 和速度都提升了不少, 立马 clone 下来试了一番.</p>
<a id="more"></a>
<h2 id="Instruction"><a href="#Instruction" class="headerlink" title="Instruction"></a>Instruction</h2><h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h3><p>下面是<a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="external">官网</a>挂出来的一个对比表, 可以看出, YOLOv2 有 76.8 的mAP, 和 SSD500 相同, 但是 FPS 不知比 SSD 高到哪里去了. YOLOv2 544x544 提升到了 78.6 mAP, 比 Faster-RCNN 的 Baseline (ResNet-101, VOC07+12) 的 76.4 mAP 高, 但是比其 Baseline+++ (ResNet-101, COCO+VOC2007+VOC2012) 的 85.6 mAP 还是逊色了不少. 不过官网没有挂出来 training on Pascal + COCO data and testing on Pascal data 的数据, 想见应该也会在 80 mAP 以上.</p>
<p>更加可喜的是, Tiny-YOLOv2比之前的Tiny-YOLO高出了52FPS, 达到了惊人的207FPS, 想必在 TX1 上跑的话应该也能上20FPS.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Train</th>
<th>Test</th>
<th>mAP</th>
<th>FLOPS</th>
<th>FPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Old YOLO</td>
<td>VOC 2007+2012</td>
<td>2007</td>
<td>63.4</td>
<td>40.19 Bn</td>
<td>45</td>
</tr>
<tr>
<td>SSD300</td>
<td>VOC 2007+2012</td>
<td>2007</td>
<td>74.3</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>SSD500</td>
<td>VOC 2007+2012</td>
<td>2007</td>
<td>76.8</td>
<td>-</td>
<td>19</td>
</tr>
<tr>
<td>YOLOv2</td>
<td>VOC 2007+2012</td>
<td>2007</td>
<td>76.8</td>
<td>34.90 Bn</td>
<td>67</td>
</tr>
<tr>
<td>YOLOv2 544x544</td>
<td>VOC 2007+2012</td>
<td>2007</td>
<td>78.6</td>
<td>59.68 Bn</td>
<td>40</td>
</tr>
<tr>
<td>Tiny YOLO</td>
<td>VOC 2007+2012</td>
<td>2007</td>
<td>57.1</td>
<td>6.97 Bn</td>
<td>207</td>
</tr>
<tr>
<td>SSD300</td>
<td>COCO trainval</td>
<td>test-dev</td>
<td>41.2</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>SSD500</td>
<td>COCO trainval</td>
<td>test-dev</td>
<td>46.5</td>
<td>-</td>
<td>19</td>
</tr>
<tr>
<td>YOLOv2 544x544</td>
<td>COCO trainval</td>
<td>test-dev</td>
<td>44.0</td>
<td>59.68 Bn</td>
<td>40</td>
</tr>
</tbody>
</table>
<h3 id="What’s-New-in-Version-2"><a href="#What’s-New-in-Version-2" class="headerlink" title="What’s New in Version 2"></a>What’s New in Version 2</h3><p>具体的文章还没在 Arxiv 上挂出来, 按照目前透露的信息, 主要是像 SSD 和 Overfeat 那样全部都用了卷积层, 而不是后面还跟着全连接层. 但是不同的是, 仍然是对整个图像进行训练. 同时还借鉴了 Faster-RCNN, 调整了 Bounding Box 的优先级, 不直接预测<code>w</code>, <code>h</code>, 但是仍然是直接预测<code>x</code>, <code>y</code>坐标.</p>
<h3 id="Video"><a href="#Video" class="headerlink" title="Video"></a>Video</h3><p>官网还挂了一个用 YOLOv2 识别过的 007 的 Trailer, 配乐+Bounding Box 使得这个视频莫名其妙地非常搞笑, 建议去<a href="https://youtu.be/VOC3huqHrss" target="_blank" rel="external">看看</a>.</p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>下面讲讲使用 YOLOv2 在我自己做的数据集 ROBdevkit 上的训练过程.</p>
<h3 id="Make"><a href="#Make" class="headerlink" title="Make"></a>Make</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/pjreddie/darknet</div><div class="line">$ <span class="built_in">cd</span> darknet</div><div class="line">$ make -j8</div></pre></td></tr></table></figure>
<p>在<code>make</code>之前, 为了最大发挥机器的性能, 还需要修改<code>Makefile</code>:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">diff --git a/Makefile b/Makefile</div><div class="line">index 3d3d5e4..dd7a33d 100644</div><div class="line"><span class="comment">--- a/Makefile</span></div><div class="line"><span class="comment">+++ b/Makefile</span></div><div class="line"><span class="meta">@@ -1,6 +1,6 @@</span></div><div class="line"><span class="deletion">-GPU=0</span></div><div class="line"><span class="deletion">-CUDNN=0</span></div><div class="line"><span class="deletion">-OPENCV=0</span></div><div class="line"><span class="addition">+GPU=1</span></div><div class="line"><span class="addition">+CUDNN=1</span></div><div class="line"><span class="addition">+OPENCV=1</span></div><div class="line"> DEBUG=0</div><div class="line"></div><div class="line"> ARCH= -gencode arch=compute_20,code=[sm_20,sm_21] \</div><div class="line">@@ -10,47 +10,47 @@ ARCH= -gencode arch=compute_20,code=[sm_20,sm_21] \</div><div class="line">       -gencode arch=compute_52,code=[sm_52,compute_52]</div><div class="line"></div><div class="line"> # This is what I use, uncomment if you know your arch and want to specify</div><div class="line"><span class="deletion">-# ARCH=  -gencode arch=compute_52,code=compute_52</span></div><div class="line"><span class="addition">+ARCH=  -gencode arch=compute_61,code=compute_61</span></div></pre></td></tr></table></figure>
<h3 id="Prepare"><a href="#Prepare" class="headerlink" title="Prepare"></a>Prepare</h3><p>YOLOv2这次不用改<code>yolo.c</code>源文件了, 只需要修改一些配置文件即可, 大大方便了我们用自己的数据集训练.</p>
<p>首先修改<code>data/voc.names</code>, 另存为<code>data/rob.names</code>:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ball</div><div class="line">goal</div><div class="line">robot</div></pre></td></tr></table></figure>
<p>修改<code>cfg/voc.data</code>, 另存为<code>cfg/rob.names</code>:</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="attr">classes</span>= <span class="number">3</span></div><div class="line"><span class="attr">train</span>  = /home/m/data/ROBdevkit/train.txt</div><div class="line"><span class="attr">valid</span>  = /home/m/data/ROBdevkit/<span class="number">2017</span>_test.txt</div><div class="line"><span class="attr">names</span> = data/rob.names</div><div class="line"><span class="attr">backup</span> = /home/m/workspace/backup/</div></pre></td></tr></table></figure>
<p>其次修改<code>script/voc_label.py</code>, 另存为<code>script/rob_label.py</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</div><div class="line"><span class="keyword">import</span> pickle</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir, getcwd</div><div class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join</div><div class="line"></div><div class="line">sets = [(<span class="string">'2017'</span>, <span class="string">'train'</span>), (<span class="string">'2017'</span>, <span class="string">'val'</span>), (<span class="string">'2017'</span>, <span class="string">'test'</span>)]</div><div class="line"></div><div class="line">classes = [<span class="string">'ball'</span>, <span class="string">'goal'</span>, <span class="string">'robot'</span>]</div><div class="line"></div><div class="line">...</div></pre></td></tr></table></figure>
<p>然后在<code>ROBdevkit</code>的根目录下执行<code>python rob_label.py</code>来生成 label 文件, 并用<code>cat 2017_* &gt; train.txt</code>生成<code>train.txt</code>. 最终目录结构为:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── ROBdevkit</div><div class="line">│   ├── 2017_test.txt</div><div class="line">│   ├── 2017_train.txt</div><div class="line">│   ├── 2017_val.txt</div><div class="line">│   ├── results</div><div class="line">│   ├── ROB2017</div><div class="line">│   ├── scripts</div><div class="line">│   ├── train.txt</div><div class="line">│   └── VOC0712</div><div class="line">├── rob_label.py</div></pre></td></tr></table></figure>
<p>最后, 修改<code>cfg/voc.data</code>, 另存为<code>cfg/rob.names</code>:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">diff --git a/cfg/tiny-yolo-voc.cfg b/cfg/tiny-yolo-rob.cfg</div><div class="line"><span class="deletion">-- a/cfg/tiny-yolo-voc.cfg</span></div><div class="line"><span class="addition">++ b/cfg/tiny-yolo-rob.cfg</span></div><div class="line">[convolutional]</div><div class="line">size=1</div><div class="line">stride=1</div><div class="line">pad=1</div><div class="line"><span class="deletion">-filters=250</span></div><div class="line"><span class="addition">+filters=40</span></div><div class="line">activation=linear</div><div class="line"></div><div class="line">[region]</div><div class="line">anchors = 1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52</div><div class="line">bias_match=1</div><div class="line"><span class="deletion">-classes=20</span></div><div class="line"><span class="addition">+classes=3</span></div><div class="line">coords=4</div><div class="line">num=5</div><div class="line">softmax=1</div><div class="line">jitter=.2</div><div class="line">rescore=1</div><div class="line"></div><div class="line">object_scale=5</div><div class="line">noobject_scale=1</div><div class="line">class_scale=1</div><div class="line">coord_scale=1</div><div class="line"></div><div class="line">absolute=1</div><div class="line">thresh = .6</div><div class="line">random=1</div></pre></td></tr></table></figure>
<blockquote>
<p>注意<code>region</code>的前一层的<code>filter</code>值的计算方法为$num \times (classes+coords+1)$.</p>
</blockquote>
<h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./darknet detector train cfg/rob.data cfg/tiny-yolo-rob.cfg darknet.conv.weights</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在看 Darkflow 的时候, 发现连 YOLOv2 都出了, 据称 mAP 和速度都提升了不少, 立马 clone 下来试了一番.&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="YOLO" scheme="http://www.yuthon.com/tags/YOLO/"/>
    
      <category term="Object detection" scheme="http://www.yuthon.com/tags/Object-detection/"/>
    
      <category term="Darknet" scheme="http://www.yuthon.com/tags/Darknet/"/>
    
  </entry>
  
  <entry>
    <title>1Password Problem browser could not be verified</title>
    <link href="http://www.yuthon.com/2016/12/02/1Password-Problem-browser-could-not-be-verified/"/>
    <id>http://www.yuthon.com/2016/12/02/1Password-Problem-browser-could-not-be-verified/</id>
    <published>2016-12-02T08:41:04.000Z</published>
    <updated>2016-12-02T08:52:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在 Chrome 上使用 1Password 总是会提示<code>Browser could not be verified</code>, 经过查阅资料解决, 特此记录.</p>
<a id="more"></a>
<p>现象如图:</p>
<p><img src="/images/1Password-Problem-browser-could-not-be-verified-1.png" alt="1Password-Problem-browser-could-not-be-verified-1"></p>
<p>根据官网<a href="https://support.1password.com/firewall-proxy/" target="_blank" rel="external">这篇文章</a>, 应该是 Surge 代理的原因. 只要将<code>127.0.0.1</code>加入到白名单里面就好了.</p>
<h2 id="Configure-your-proxy-settings"><a href="#Configure-your-proxy-settings" class="headerlink" title="Configure your proxy settings"></a>Configure your proxy settings</h2><ol>
<li><p>Choose Apple menu () &gt; System Preferences, then click the Network icon.</p>
</li>
<li><p>Select your primary network interface (typically Wi-Fi, or Ethernet if you have a wired connection).</p>
</li>
<li><p>Click Advanced, then select the Proxies tab.</p>
<p><img src="/images/1Password-Problem-browser-could-not-be-verified-2.png" alt="1Password-Problem-browser-could-not-be-verified-2"></p>
</li>
<li><p>Select Web Proxy (HTTP).</p>
</li>
<li><p>Under “Bypass proxy settings for these Hosts &amp; Domains”, click to the right of the existing text, and type a comma followed by <code>127.0.0.1</code>. Then click OK.</p>
</li>
</ol>
<blockquote>
<p>If “Secure Web Proxy (HTTPS)” is also enabled, select it, and add <code>127.0.0.1</code> as above.</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在 Chrome 上使用 1Password 总是会提示&lt;code&gt;Browser could not be verified&lt;/code&gt;, 经过查阅资料解决, 特此记录.&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="1Password" scheme="http://www.yuthon.com/tags/1Password/"/>
    
      <category term="macOS" scheme="http://www.yuthon.com/tags/macOS/"/>
    
  </entry>
  
  <entry>
    <title>Train Caffe-YOLO on our own dataset</title>
    <link href="http://www.yuthon.com/2016/11/26/Train-Caffe-YOLO-on-our-own-dataset/"/>
    <id>http://www.yuthon.com/2016/11/26/Train-Caffe-YOLO-on-our-own-dataset/</id>
    <published>2016-11-26T10:11:14.000Z</published>
    <updated>2016-11-26T11:26:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>经过这几天不断地测试, YOLO 在 TX1 上跑得还是挺不错的, 符合我们实验室的要求. 但是 YOLO 依赖的 Darknet 框架还是太原始了, 不如 TensorFlow 或者 Caffe 用着顺手. 另外, 我负责的目标检测这一块还需要和梅老板写的新框架相结合, 所以更加需要把 YOLO 移植到一个成熟的框架上去.</p>
<p>很幸运的是, YOLO 在各个框架上的移植都有前人做过了, 比如 <a href="https://github.com/thtrieu/darktf" target="_blank" rel="external">darktf</a> 和 <a href="https://github.com/yeahkun/caffe-yolo" target="_blank" rel="external">caffe-yolo</a>. 今天以 caffe-yolo 为例, 谈一下在其上使用自己的数据集来训练.</p>
<a id="more"></a>
<h2 id="Reformat-our-dataset-as-PASCAL-VOC-style"><a href="#Reformat-our-dataset-as-PASCAL-VOC-style" class="headerlink" title="Reformat our dataset as PASCAL VOC style"></a>Reformat our dataset as PASCAL VOC style</h2><p>为了之后的方便起见, 首先将我们的数据集转成 PASCAL VOC 的标准的目录格式.</p>
<h3 id="Structure-of-PASCAL-VOC-dataset"><a href="#Structure-of-PASCAL-VOC-dataset" class="headerlink" title="Structure of PASCAL VOC dataset"></a>Structure of PASCAL VOC dataset</h3><p>其目录结构如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── VOC2007</div><div class="line">│   ├── Annotations</div><div class="line">│   ├── ImageSets</div><div class="line">│   ├── JPEGImages</div><div class="line">│   ├── SegmentationClass</div><div class="line">│   └── SegmentationObject</div><div class="line">└── VOC2012</div><div class="line">    ├── Annotations</div><div class="line">    ├── ImageSets</div><div class="line">    ├── JPEGImages</div><div class="line">    ├── SegmentationClass</div><div class="line">    └── SegmentationObject</div></pre></td></tr></table></figure>
<p>其中<code>Annotations</code>目录放的是<code>.xml</code>文件, <code>JPEGImages</code>目录中存放的是对应的<code>.jpg</code>图像. 由于我们不做语义分割, 所以<code>SegmentationClass</code>与<code>SegmentationObject</code>对我们没什么用.</p>
<p> <code>ImageSets</code>目录中结构如下, 主要关注的是<code>Main</code>文件夹中的<code>trainval.txt</code>, <code>train.txt</code> , <code>val.txt</code>以及<code>test.txt</code>四个文件.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── Layout</div><div class="line">│   ├── test.txt</div><div class="line">│   ├── train.txt</div><div class="line">│   ├── trainval.txt</div><div class="line">│   └── val.txt</div><div class="line">├── Main</div><div class="line">│   ├── aeroplane_test.txt</div><div class="line">│   ├── aeroplane_train.txt</div><div class="line">│   ├── aeroplane_trainval.txt</div><div class="line">│   ├── aeroplane_val.txt</div><div class="line">│   ├── ...</div><div class="line">│   ├── test.txt</div><div class="line">│   ├── train.txt</div><div class="line">│   ├── trainval.txt</div><div class="line">│   └── val.txt</div><div class="line">└── Segmentation</div><div class="line">    ├── test.txt</div><div class="line">    ├── train.txt</div><div class="line">    ├── trainval.txt</div><div class="line">    └── val.txt</div></pre></td></tr></table></figure>
<h3 id="Reformat-our-dataset"><a href="#Reformat-our-dataset" class="headerlink" title="Reformat our dataset"></a>Reformat our dataset</h3><p>首先是把之前杂乱的图片文件名重新整理, 如下所示:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── image00001.jpg</div><div class="line">├── image00002.jpg</div><div class="line">├── image00012.jpg</div><div class="line">├── ...</div><div class="line">├── image04524.jpg</div><div class="line">├── image04525.jpg</div><div class="line">└── image04526.jpg</div></pre></td></tr></table></figure>
<p>随后用<code>labelImg</code>重新标注这些图. 标注完成后, 建立我们自己的数据集的结构, 并且将图片和标注放到对应的文件夹里:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── ROB2017</div><div class="line">│   ├── Annotations</div><div class="line">│   ├── ImageSets</div><div class="line">│   ├── JPEGImages</div><div class="line">│   └── JPEGImages_original</div><div class="line">└── scripts</div><div class="line">    ├── clean.py</div><div class="line">    ├── conf.json</div><div class="line">    ├── convert_png2jpg.py</div><div class="line">    └── split_dataset.py</div></pre></td></tr></table></figure>
<p>之后写了几个脚本, 其中<code>clean.py</code>用来清理未标注的图片; <code>split_dataset.py</code>用来分割训练集验证集测试集, 并且保存到<code>ImageSets/Main</code>中.</p>
<p>至此, 把我们的数据集转成 PASCAL VOC 标准目录的工作就完成了, 可以进行下一步的训练工作.</p>
<h2 id="Train-YOLO-on-Caffe"><a href="#Train-YOLO-on-Caffe" class="headerlink" title="Train YOLO on Caffe"></a>Train YOLO on Caffe</h2><h3 id="Clone-amp-Make"><a href="#Clone-amp-Make" class="headerlink" title="Clone &amp; Make"></a>Clone &amp; Make</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/yeahkun/caffe-yolo.git</div><div class="line">$ <span class="built_in">cd</span> caffe-yolo</div><div class="line">$ cp Makefile.config.example Makefile.config</div><div class="line">$ make -j8</div></pre></td></tr></table></figure>
<p>若是出现<code>src/caffe/net.cpp:8:18: fatal error: hdf5.h: No such file or directory</code>这一错误, 可以照下文修改<code>Makefile.config</code>文件:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">diff --git a/Makefile.config b/Makefile.config</div><div class="line">index a873502..88828cc 100644</div><div class="line"><span class="comment">--- a/Makefile.config.example</span></div><div class="line"><span class="comment">+++ b/Makefile.config.example</span></div><div class="line">@@ -69,8 +69,8 @@ PYTHON_LIB := /usr/lib</div><div class="line"> # WITH_PYTHON_LAYER := 1</div><div class="line"></div><div class="line"> # Whatever else you find you need goes here.</div><div class="line"><span class="deletion">-INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include</span></div><div class="line"><span class="deletion">-LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib</span></div><div class="line"><span class="addition">+INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/</span></div><div class="line"><span class="addition">+LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu/hdf5/serial/</span></div><div class="line"></div><div class="line"> # If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies</div><div class="line"> # INCLUDE_DIRS += $(shell brew --prefix)/include</div></pre></td></tr></table></figure>
<p>同时还可以开启 cuDNN 以及修改 compute, 充分发挥 GTX1080 的性能:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">## Refer to http://caffe.berkeleyvision.org/installation.html</div><div class="line"># Contributions simplifying and improving our build system are welcome!</div><div class="line"></div><div class="line"># cuDNN acceleration switch (uncomment to build with cuDNN).</div><div class="line"><span class="deletion">-# USE_CUDNN := 1</span></div><div class="line"><span class="addition">+USE_CUDNN := 1</span></div><div class="line"></div><div class="line"># CPU-only switch (uncomment to build without GPU support).</div><div class="line"># CPU_ONLY := 1</div><div class="line">...</div><div class="line"># CUDA architecture setting: going with all of them.</div><div class="line"># For CUDA &lt; 6.0, comment the *_50 lines for compatibility.</div><div class="line">CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \</div><div class="line">                -gencode arch=compute_20,code=sm_21 \</div><div class="line">                -gencode arch=compute_30,code=sm_30 \</div><div class="line">                -gencode arch=compute_35,code=sm_35 \</div><div class="line">                -gencode arch=compute_50,code=sm_50 \</div><div class="line"><span class="deletion">-                -gencode arch=compute_50,code=compute_50</span></div><div class="line"><span class="addition">+                -gencode arch=compute_50,code=compute_50 \</span></div><div class="line"><span class="addition">+                -gencode arch=compute_61,code=compute_61</span></div></pre></td></tr></table></figure>
<h3 id="Data-preparation"><a href="#Data-preparation" class="headerlink" title="Data preparation"></a>Data preparation</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> data/yolo</div><div class="line">$ ln -s /your/path/to/VOCdevkit/ .</div><div class="line">$ python ./get_list.py</div><div class="line"><span class="comment"># change related path in script convert.sh</span></div><div class="line">$ sudo rm -r lmdb</div><div class="line">$ mkdir lmdb</div><div class="line">$ ./convert.sh</div></pre></td></tr></table></figure>
<p>有一些注意点:</p>
<ul>
<li><p>记得将<code>ln -s /your/path/to/VOCdevkit/ .</code>中的<code>/your/path/to/VOCdevkit/</code>换成自己数据集的路径, 例如<code>ln -s ~/data/ROBdevkit/ .</code></p>
</li>
<li><p>修改<code>./get_list.py</code>:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line">diff --git a/data/yolo/get_list.py b/data/yolo/get_list.py</div><div class="line">index f519f1a..73b9858 100755</div><div class="line"><span class="comment">--- a/data/yolo/get_list.py</span></div><div class="line"><span class="comment">+++ b/data/yolo/get_list.py</span></div><div class="line">@@ -3,12 +3,15 @@ import os</div><div class="line"></div><div class="line"> trainval_jpeg_list = []</div><div class="line"> trainval_xml_list = []</div><div class="line"><span class="deletion">-test07_jpeg_list = []</span></div><div class="line"><span class="deletion">-test07_xml_list = []</span></div><div class="line"><span class="deletion">-test12_jpeg_list = []</span></div><div class="line"><span class="deletion">-</span></div><div class="line"><span class="deletion">-for name in ["VOC2007", "VOC2012"]:</span></div><div class="line"><span class="deletion">-  voc_dir = os.path.join("VOCdevkit", name)</span></div><div class="line"><span class="addition">+test_jpeg_list = []</span></div><div class="line"><span class="addition">+test_xml_list = []</span></div><div class="line"><span class="addition">+</span></div><div class="line"><span class="addition">+for name in ['ROB2017']:</span></div><div class="line"><span class="addition">+  # voc_dir = os.path.join("VOCdevkit", name)</span></div><div class="line"><span class="addition">+  voc_dir = os.path.join('ROBdevkit', name)</span></div><div class="line">   txt_fold = os.path.join(voc_dir, "ImageSets/Main")</div><div class="line">   jpeg_fold = os.path.join(voc_dir, "JPEGImages")</div><div class="line">   xml_fold = os.path.join(voc_dir, "Annotations")</div><div class="line">@@ -23,35 +26,49 @@ for name in ["VOC2007", "VOC2012"]:</div><div class="line">           print trainval_jpeg_list[-1], "not exist"</div><div class="line">         if not os.path.exists(trainval_xml_list[-1]):</div><div class="line">           print trainval_xml_list[-1], "not exist"</div><div class="line"><span class="deletion">-  if name == "VOC2007":</span></div><div class="line"><span class="deletion">-    file_path = os.path.join(txt_fold, "test.txt")</span></div><div class="line"><span class="deletion">-    with open(file_path, 'r') as fp:</span></div><div class="line"><span class="deletion">-      for line in fp:</span></div><div class="line"><span class="deletion">-        line = line.strip()</span></div><div class="line"><span class="deletion">-        test07_jpeg_list.append(os.path.join(jpeg_fold, "&#123;&#125;.jpg".format(line)))</span></div><div class="line"><span class="deletion">-        test07_xml_list.append(os.path.join(xml_fold, "&#123;&#125;.xml".format(line)))</span></div><div class="line"><span class="deletion">-        if not os.path.exists(test07_jpeg_list[-1]):</span></div><div class="line"><span class="deletion">-          print test07_jpeg_list[-1], "not exist"</span></div><div class="line"><span class="deletion">-        if not os.path.exists(test07_xml_list[-1]):</span></div><div class="line"><span class="deletion">-          print test07_xml_list[-1], "not exist"</span></div><div class="line"><span class="deletion">-  elif name == "VOC2012":</span></div><div class="line"><span class="addition">+  if name == "ROB2017":</span></div><div class="line">     file_path = os.path.join(txt_fold, "test.txt")</div><div class="line">     with open(file_path, 'r') as fp:</div><div class="line">       for line in fp:</div><div class="line">         line = line.strip()</div><div class="line"><span class="deletion">-        test12_jpeg_list.append(os.path.join(jpeg_fold, "&#123;&#125;.jpg".format(line)))</span></div><div class="line"><span class="deletion">-        if not os.path.exists(test12_jpeg_list[-1]):</span></div><div class="line"><span class="deletion">-          print test12_jpeg_list[-1], "not exist"</span></div><div class="line"><span class="addition">+        test_jpeg_list.append(os.path.join(jpeg_fold, "&#123;&#125;.jpg".format(line)))</span></div><div class="line"><span class="addition">+        test_xml_list.append(os.path.join(xml_fold, "&#123;&#125;.xml".format(line)))</span></div><div class="line"><span class="addition">+        if not os.path.exists(test_jpeg_list[-1]):</span></div><div class="line"><span class="addition">+          print test_jpeg_list[-1], "not exist"</span></div><div class="line"><span class="addition">+        if not os.path.exists(test_xml_list[-1]):</span></div><div class="line"><span class="addition">+          print test_xml_list[-1], "not exist"</span></div><div class="line"></div><div class="line"> with open("trainval.txt", "w") as wr:</div><div class="line">   for i in range(len(trainval_jpeg_list)):</div><div class="line">     wr.write("&#123;&#125; &#123;&#125;\n".format(trainval_jpeg_list[i], trainval_xml_list[i]))</div><div class="line"></div><div class="line"><span class="deletion">-with open("test_2007.txt", "w") as wr:</span></div><div class="line"><span class="deletion">-  for i in range(len(test07_jpeg_list)):</span></div><div class="line"><span class="deletion">-    wr.write("&#123;&#125; &#123;&#125;\n".format(test07_jpeg_list[i], test07_xml_list[i]))</span></div><div class="line"><span class="deletion">-</span></div><div class="line"><span class="deletion">-with open("test_2012.txt", "w") as wr:</span></div><div class="line"><span class="deletion">-  for i in range(len(test12_jpeg_list)):</span></div><div class="line"><span class="deletion">-    wr.write("&#123;&#125;\n".format(test12_jpeg_list[i]))</span></div><div class="line"><span class="addition">+with open("test.txt", "w") as wr:</span></div><div class="line"><span class="addition">+  for i in range(len(test_jpeg_list)):</span></div><div class="line"><span class="addition">+    wr.write("&#123;&#125; &#123;&#125;\n".format(test_jpeg_list[i], test_xml_list[i]))</span></div></pre></td></tr></table></figure>
</li>
<li><p>修改<code>convert.sh</code></p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">diff --git a/data/yolo/convert.sh b/data/yolo/convert.sh</div><div class="line">index 8a52525..a06eb69 100755</div><div class="line"><span class="comment">--- a/data/yolo/convert.sh</span></div><div class="line"><span class="comment">+++ b/data/yolo/convert.sh</span></div><div class="line"><span class="meta">@@ -1,7 +1,7 @@</span></div><div class="line"> #!/usr/bin/env sh</div><div class="line"></div><div class="line"> CAFFE_ROOT=../..</div><div class="line"><span class="deletion">-ROOT_DIR=/your/path/to/vocroot/</span></div><div class="line"><span class="addition">+ROOT_DIR=/home/m/data/</span></div><div class="line"> LABEL_FILE=$CAFFE_ROOT/data/yolo/label_map.txt</div><div class="line"></div><div class="line"> # 2007 + 2012 trainval</div><div class="line">@@ -10,13 +10,15 @@ LMDB_DIR=./lmdb/trainval_lmdb</div><div class="line"> SHUFFLE=true</div><div class="line"></div><div class="line"> # 2007 test</div><div class="line"><span class="deletion">-# LIST_FILE=$CAFFE_ROOT/data/yolo/test_2007.txt</span></div><div class="line"><span class="deletion">-# LMDB_DIR=./lmdb/test2007_lmdb</span></div><div class="line"><span class="addition">+# LIST_FILE=$CAFFE_ROOT/data/yolo/test.txt</span></div><div class="line"><span class="addition">+# LMDB_DIR=./lmdb/test_lmdb</span></div><div class="line"> # SHUFFLE=false</div><div class="line"></div><div class="line"> RESIZE_W=448</div><div class="line"> RESIZE_H=448</div><div class="line"></div><div class="line"> $CAFFE_ROOT/build/tools/convert_box_data --resize_width=$RESIZE_W --resize_height=$RESIZE_H \</div><div class="line">   --label_file=$LABEL_FILE $ROOT_DIR $LIST_FILE $LMDB_DIR --encoded=true --encode_type=jpg --shuffle=$SHUFFLE</div></pre></td></tr></table></figure>
</li>
<li><p>修改<code>label_map.txt</code>:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">diff --git a/data/yolo/label_map.txt b/data/yolo/label_map.txt</div><div class="line">index 1fe873a..bee8f82 100644</div><div class="line"><span class="comment">--- a/data/yolo/label_map.txt</span></div><div class="line"><span class="comment">+++ b/data/yolo/label_map.txt</span></div><div class="line"><span class="meta">@@ -1,20 +1,3 @@</span></div><div class="line"><span class="deletion">-aeroplane 0</span></div><div class="line"><span class="deletion">-bicycle 1</span></div><div class="line"><span class="deletion">-bird 2</span></div><div class="line"><span class="deletion">-boat 3</span></div><div class="line"><span class="deletion">-bottle 4</span></div><div class="line"><span class="deletion">-bus 5</span></div><div class="line"><span class="deletion">-car 6</span></div><div class="line"><span class="deletion">-cat 7</span></div><div class="line"><span class="deletion">-chair 8</span></div><div class="line"><span class="deletion">-cow 9</span></div><div class="line"><span class="deletion">-diningtable 10</span></div><div class="line"><span class="deletion">-dog 11</span></div><div class="line"><span class="deletion">-horse 12</span></div><div class="line"><span class="deletion">-motorbike 13</span></div><div class="line"><span class="deletion">-person 14</span></div><div class="line"><span class="deletion">-pottedplant 15</span></div><div class="line"><span class="deletion">-sheep 16</span></div><div class="line"><span class="deletion">-sofa 17</span></div><div class="line"><span class="deletion">-train 18</span></div><div class="line"><span class="deletion">-tvmonitor 19</span></div><div class="line"><span class="addition">+ball 0</span></div><div class="line"><span class="addition">+goal 1</span></div><div class="line"><span class="addition">+robot 2</span></div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cd examples/yolo</div><div class="line"># change related path in script train.sh</div><div class="line">mkdir models</div><div class="line">nohup ./train.sh &amp;</div></pre></td></tr></table></figure>
<p>也有一些注意点:</p>
<ul>
<li><p>修改<code>gnet_train.prototxt</code>:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">diff --git a/examples/yolo/gnet_train.prototxt b/examples/yolo/gnet_train.prototxt</div><div class="line">index 8483a32..da01daf 100644</div><div class="line"><span class="comment">--- a/examples/yolo/gnet_train.prototxt</span></div><div class="line"><span class="comment">+++ b/examples/yolo/gnet_train.prototxt</span></div><div class="line">@@ -36,7 +36,7 @@ layer &#123;</div><div class="line">     mean_value: 123</div><div class="line">   &#125;</div><div class="line">   data_param &#123;</div><div class="line"><span class="deletion">-    source: "../../data/yolo/lmdb/test2007_lmdb"</span></div><div class="line"><span class="addition">+    source: "../../data/yolo/lmdb/test_lmdb"</span></div><div class="line">     batch_size: 1</div><div class="line">     side: 7</div><div class="line">     backend: LMDB</div></pre></td></tr></table></figure>
</li>
<li><p>修改<code>train.sh</code>:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">diff --git a/examples/yolo/train.sh b/examples/yolo/train.sh</div><div class="line">index 416e2b0..ecd0872 100755</div><div class="line"><span class="comment">--- a/examples/yolo/train.sh</span></div><div class="line"><span class="comment">+++ b/examples/yolo/train.sh</span></div><div class="line"><span class="meta">@@ -3,8 +3,7 @@</span></div><div class="line"> CAFFE_HOME=../..</div><div class="line"></div><div class="line"> SOLVER=./gnet_solver.prototxt</div><div class="line"><span class="deletion">-WEIGHTS=/your/path/to/bvlc_googlenet.caffemodel</span></div><div class="line"><span class="addition">+WEIGHTS=/home/m/workspace/caffe-yolo/models/bvlc_googlenet/bvlc_googlenet.caffemodel</span></div><div class="line"></div><div class="line"> $CAFFE_HOME/build/tools/caffe train \</div><div class="line"><span class="deletion">-    --solver=$SOLVER --weights=$WEIGHTS --gpu=0,1</span></div><div class="line"><span class="addition">+    --solver=$SOLVER --weights=$WEIGHTS --gpu=0</span></div></pre></td></tr></table></figure>
</li>
<li><p>注意还要预先下载 GoogleNet 的<a href="http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel" target="_blank" rel="external">预训练权重文件</a>, 并且放在<code>caffe-yolo/models/bvlc_googlenet/</code>(当然放哪里是随便的, 注意改<code>train.sh</code>中的相应地址即可).</p>
</li>
</ul>
<h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># if everything goes well, the map of gnet_yolo_iter_32000.caffemodel may reach ~56.</div><div class="line">cd examples/yolo</div><div class="line">./test.sh model_path gpu_id</div></pre></td></tr></table></figure>
<p>(To be continued)</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;经过这几天不断地测试, YOLO 在 TX1 上跑得还是挺不错的, 符合我们实验室的要求. 但是 YOLO 依赖的 Darknet 框架还是太原始了, 不如 TensorFlow 或者 Caffe 用着顺手. 另外, 我负责的目标检测这一块还需要和梅老板写的新框架相结合, 所以更加需要把 YOLO 移植到一个成熟的框架上去.&lt;/p&gt;
&lt;p&gt;很幸运的是, YOLO 在各个框架上的移植都有前人做过了, 比如 &lt;a href=&quot;https://github.com/thtrieu/darktf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;darktf&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/yeahkun/caffe-yolo&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;caffe-yolo&lt;/a&gt;. 今天以 caffe-yolo 为例, 谈一下在其上使用自己的数据集来训练.&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="YOLO" scheme="http://www.yuthon.com/tags/YOLO/"/>
    
      <category term="Object detection" scheme="http://www.yuthon.com/tags/Object-detection/"/>
    
      <category term="Darknet" scheme="http://www.yuthon.com/tags/Darknet/"/>
    
  </entry>
  
  <entry>
    <title>Thesis Notes for ScribbleSup</title>
    <link href="http://www.yuthon.com/2016/11/20/Thesis-Notes-for-ScribbleSup/"/>
    <id>http://www.yuthon.com/2016/11/20/Thesis-Notes-for-ScribbleSup/</id>
    <published>2016-11-20T10:26:24.000Z</published>
    <updated>2016-11-22T14:30:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>毕设需要写一个图像标注的软件, 来给场景分割的数据集做标注. 经学长推荐, 看了今年的这篇文章, 作者中竟然还有 Kaiming He 大神, 给微软膜一秒.</p>
<p>这篇文章讲了一个弱监督的场景分割的算法 ScribbleSup, 主要是先通过 Graph Cut 将输入的 scribble 信息广播到没有标注的像素, 然后用 FCN 来做像素级别的预测. 令人遗憾的是 Github 上并没有人实现 (不能偷懒了TAT).</p>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>TBD</p>
<h2 id="Scribble-Supervised-Learning"><a href="#Scribble-Supervised-Learning" class="headerlink" title="Scribble-Supervised Learning"></a>Scribble-Supervised Learning</h2><h3 id="Objective-Functions"><a href="#Objective-Functions" class="headerlink" title="Objective Functions"></a>Objective Functions</h3><p>主要用到的记号如下:</p>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Name</th>
<th>Note</th>
</tr>
</thead>
<tbody>
<tr>
<td>$X$</td>
<td>training image</td>
<td></td>
</tr>
<tr>
<td>${x_i}$</td>
<td>set of non-overlapping superpixles</td>
<td>$\cup_i x_i = X; x_i \cap x_j = \varnothing, \forall i,j$</td>
</tr>
<tr>
<td>$S$</td>
<td>scribble annotations of image</td>
<td>$S={s_k, c_k}$</td>
</tr>
<tr>
<td>$s_k$</td>
<td>the pixels of a scribble $k$</td>
<td></td>
</tr>
<tr>
<td>$c_k$</td>
<td>the scribble’s category label</td>
<td>$0 \le c_k \le C$; $c_k=0$ for background</td>
</tr>
<tr>
<td>$Y$ or ${y_i}$</td>
<td>the category label of ${x_i}$</td>
<td>provides full annotations of the image</td>
</tr>
</tbody>
</table>
<p>定义目标函数为</p>
<p>$$\sum_i \psi_i (y_i | X,S) + \sum_{i,j} \psi_{ij} (y_i, y_j | X)$$</p>
<p>其中$\psi_i$是一个关于$x_i$的一元项 (unary term), 而$\psi\ _{ij}$是关于$x_i$与$x_j$的成对项 (pairwise term).</p>
<ul>
<li><p>$\psi _i$由两个部分组成, 一个是$\psi ^{scr}_i$, 另一个是$\psi^{net}_i$.两者权重相同, $\psi _i = \psi^{scr} _i + \psi^{net} _i$.</p>
<ul>
<li><p>$\psi ^{scr}_i$ 基于 scribble, 定义如下:<br>$$<br>\psi ^{scr}_i=<br>\begin{aligned}<br>&amp;0 &amp; \text{if $y_i=c_k$ and $x_i \cap s_k \ne \varnothing$}\\<br>&amp;-log(\frac{1}{|c_k|}) &amp; \text{if $y_i \in {c_k}$ and $x_i \cap S = \varnothing$} \\<br>&amp;\infty &amp; \text{otherwise} \\<br>\end{aligned}<br>$$</p>
<ul>
<li>当$x_i$与$s_k$有交集, 且标签是分到$c_k$时, 则$cost=0$</li>
<li>当$x_i$与所有 scribble 都没有交集, 则它可以被等概率地分给任何标签. 当然, $y_i$需要在${c_k}$之内. 此处$|{c_k}|$表示标签集内元素个数.</li>
<li>如果不是以上两种情况, 则$cost= \infty$</li>
</ul>
</li>
<li><p>$\psi ^{net}_i$基于 FCN 的输出, 定义为<br>$$<br>\psi^{net}_i (y_i) = -log P(y_i|X, \Theta)<br>$$</p>
<ul>
<li>$\Theta$表示网络的参数</li>
<li>$log P(y_i|X, \Theta)$表示了$x_i$属于标签$y_i$的对数概率, 实际上是$x_i$内所有像素的对数概率之和.</li>
</ul>
</li>
</ul>
</li>
<li><p>$\psi_{ij}$用以衡量相邻的两个超像素的相似程度, 主要是用色彩直方图与纹理直方图来量化 (均已归一化).<br>$$<br>\psi_{ij} (y_i, y_j | X) = [y_i \ne y_j] exp \left( -\frac{||h_c(x_i) - h_c(x_j)||^2_2}{\delta_c^2} - \frac{||h_t(x_i) - h_t(x_j)||^2_2}{\delta_t^2} \right)<br>$$</p>
<ul>
<li>$h_c(x_i)$ 表示RGB三个 channel 每个 channel 分成 25 bins 的色彩直方图</li>
<li>$h_t(x_i)$ 表示横向与纵向的梯度直方图, 每个方向 10 bins</li>
<li>$[\cdot]$表示一个符号函数, 条件为真则为$1$, 否则为$0$</li>
<li>$\delta_c=5, \delta_t = 10$</li>
<li>对于不是同一个标签的临近超像素来说, 它们间的外观越相似, 则 cost 越大</li>
</ul>
</li>
</ul>
<p>最后把上边这些合起来, 就成了一个对于以下式子进行最优化的问题:<br>$$<br>\sum_i \psi^{scr}_i (y_i |X, S) + \sum_i -log P(y_i  | X, \Theta) + \sum_{i,j} \psi_{ij} (y_i, y_j | X)<br>$$<br>其中有两组变量, 一个是所有超像素的标签$Y={y_i}$, 另一个是 FCN 的参数 $\Theta$.</p>
<p> <img src="/images/ScribbleSup_grapgical_model.png" alt="ScribbleSup_grapgical_model"></p>
<h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>论文里采用的是一种交替优化的方法:</p>
<ul>
<li>$\Theta$固定, 优化$Y$, slover 基于 scribbles, appearance 以及 FCN 网络的预测, 将标签传播到未标记的像素中</li>
<li>$Y$固定, 优化$\Theta$, slover 对 pixel-wise 的语义分割的 FCN 进行学习</li>
</ul>
<p>具体的来说就是</p>
<ul>
<li><p><strong>Propagating scribble information to unmarked pixels</strong></p>
<p>当$\Theta$固定时, 一元项$\psi _i = \psi^{scr} _i + \psi^{net} _i$能够用列举所有可能的标签$0 \le y_i \le C$得到, 成对项也能够预先计算生成一个 look-up table. 因此, 优化问题就能用 graph cut 的方法来解决. 论文里用的是<a href="http://www.csd.uwo.ca/faculty/yuri/Papers/pami04.pdf" target="_blank" rel="external">这一篇文章</a>的<a href="http://vision.csd.uwo.ca/code/gco- v3.0.zip" target="_blank" rel="external">现成代码</a>.</p>
</li>
<li><p><strong>Optimizing network parameters</strong></p>
<p>前一步做完后, 所有超像素的标签都已经定好了, 也就是说$Y$固定了. 之后优化$\Theta$就相当于用$Y$做为监督来训练 FCN. $Y$有了, 那么每个像素的标签就有了, 然后 FCN 面对的就是一个 pixel-wise 的回归问题. FCN 的最后一层输出的就是每个像素的分类的对数概率, 可以用来更新 graph 上的一元项.</p>
</li>
</ul>
<p>训练的时候有几点需要注意:</p>
<ul>
<li>初始化的时候没有 network prediction, 因此就是直接用 graph cut 初始化的. 之后则是在两步之间不断迭代.</li>
<li>每次 network optimizing step 的时候, 前50k次用0.0003的 learning rate, 后10k次用0.0001的 learning rate, batch size 为 8.</li>
<li>每次 network optimizing step 都是从一个 pre-trained 的 model (比如 VGG-16) 重新初始化的. 作者也试过复用上一次迭代后的权重, 但是效果不是很理想. 似乎是由于本来标签就不可靠, 导致训练的时候参数被调到了不太好的局部最优里面.</li>
<li>基本上3次迭代就能得到比较好的效果了, 再多得到的提升微乎其微.</li>
<li>做验证的时候只要用 FCN 就好了, 超像素和 graph model 之类的都只是用来训练用的.</li>
<li>Post-process 用了 CRF.</li>
</ul>
<p>迭代结果如下:</p>
<p> <img src="/images/ScribbleSup_training.png" alt="ScribbleSup_training"></p>
<h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="Graphical-models-for-segmentation"><a href="#Graphical-models-for-segmentation" class="headerlink" title="Graphical models for segmentation"></a>Graphical models for segmentation</h4><p>Graphical model 在交互式的图像分割和语义分割领域是很常见的, 通常是目标函数包含了一元项和成对项, 特别适用于对局部和全局的空间约束的建模.</p>
<p>有趣的是, FCN 作为目前最成功的语义分割的方法之一, 由于做的是 pixel-wise 的 regression, 因此其目标函数只有一元项. 不过像 CRF/MRF 这样给 FCN 做 post-processing 或是 joint-training 的方法在之后也发展起来了.</p>
<p>但是这一类 graph model 都是强监督的, 主要工作是在优化 mask 的边缘, 而 ScribbleSup 里面的 graph model 主要是用来把标签传播到其他未标注的像素上. 同时, 这类方法是 pixel-based, 而 ScribbleSup 是 super-pixel-based.</p>
<h4 id="Weakly-supervised-semantic-segmentation"><a href="#Weakly-supervised-semantic-segmentation" class="headerlink" title="Weakly-supervised semantic segmentation"></a>Weakly-supervised semantic segmentation</h4><p>用 CNN/FCN 来做弱监督的语义分割的方法很多, 用的标注方法也有很多种.</p>
<ul>
<li>Image-level 的标注很容易获取, 但是只用这个的话精度远低于强监督的结果</li>
<li>Box-level 的相比较而言结果与强监督的接近了不少. 由于 Box annotations 本身就提供了物体边缘以及可信的背景区域的信息, 因此就不需要 graph model 来传播标签.</li>
</ul>
<p>这些方法和本篇论文里面讲的 ScribbleSup 比起来到底哪个更胜一筹, 姿势水平更高, 就看下面的实验了.</p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Annotating-Scribbles"><a href="#Annotating-Scribbles" class="headerlink" title="Annotating Scribbles"></a>Annotating Scribbles</h3><p>主要使用了 PASCAL VOC 2012 (20个分类) 以及 PASCAL-CONTEXT (59个分类) 这两个数据集, 同时也标注了 PASCAL VOC 2007 (标注了59个分类). 不过 2007 没有 mask-level 的标注.</p>
<p>总共有10个人在标注, 每张图片一人标注一人检查. 平均下来20分类的话每张图片25秒, 59分类的话每张图片50秒, 算是相当快的了.</p>
<p>同时, 保证每个 object 上的 scribble 至少有其 bounding box 长边的 70% 以上的长度.</p>
<h3 id="Experiments-on-PASCAL-VOC-2012"><a href="#Experiments-on-PASCAL-VOC-2012" class="headerlink" title="Experiments on PASCAL VOC 2012"></a>Experiments on PASCAL VOC 2012</h3><h4 id="Strategies-of-utilizing-scribbles"><a href="#Strategies-of-utilizing-scribbles" class="headerlink" title="Strategies of utilizing scribbles"></a>Strategies of utilizing scribbles</h4><p>ScribbleSup 是将标签的扩散与网络的训练合起来考虑的, 但是一个更为简单的方案是把这两步分开来, 先用一些现成的工具 (比如说 GrabCut 或者是 LazySnapping) 把 scribble 转换成 mask, 然后再来训练 FCN 网络. 这个方案听起来也是很吼的, 那么中央到底兹不兹瓷呢, 我们来看看实验结果</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>mIoU(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GrabCut + FCN</td>
<td>49.1</td>
</tr>
<tr>
<td>LazySnapping + FCN</td>
<td>53.8</td>
</tr>
<tr>
<td>ours, w/o pairwise terms</td>
<td>60.5</td>
</tr>
<tr>
<td>ours, w/ pairwise terms</td>
<td>63.1</td>
</tr>
</tbody>
</table>
<p>所以说不要听风就是雨, 可以看出分两步走的方案是一个错误的道路, mIoU显著低于 ScribbleSup. 其中的原因主要是这些传统的方法仅仅针对 low-level 的空间或者是色彩信息建模, 并没有考虑到语义的层面. 也就是说, 这些方法得到的 mask 是不值得信赖的, 不能作为 ground truth 来用.</p>
<p>而 ScribbleSup 就不同了, 通过不断的迭代, FCN 能够逐渐学习到 high-level 的语义特征, 这些特征又能反哺给 graph-based scribble propagation. 这样就形成了一个良性循环, 自然 mIoU 就不知比传统方法高到哪里去了.</p>
<p>同时可以看出, 用了成对项的效果比不用的好. 这是因为如果没有了成对项, 那么目标函数就只剩下了一元项, graph cut 步骤变成了基于network prediction 的 winner-take-all 的模式. 这样的话, 信息的传播就只与全卷积有关, 会过于看重局部一致性, 最终导致准确度降低.</p>
<h4 id="Sensitivities-to-scribble-quality"><a href="#Sensitivities-to-scribble-quality" class="headerlink" title="Sensitivities to scribble quality"></a>Sensitivities to scribble quality</h4><p>Scribble quality 是个非常主观的东西, 所以为了研究这个对于准确度的影响, 论文里采用了将原 scribble 放缩为不同长度 (甚至是一个点), 然后实验来观察.</p>
<p> <img src="/images/ScribbleSup_scribble_of_different_length.png" alt="scribble_of_different_length"></p>
<table>
<thead>
<tr>
<th>Length ratio</th>
<th>mIoU (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>63.1</td>
</tr>
<tr>
<td>0.8</td>
<td>61.8</td>
</tr>
<tr>
<td>0.5</td>
<td>58.5</td>
</tr>
<tr>
<td>0.3</td>
<td>54.3</td>
</tr>
<tr>
<td>0 (spot)</td>
<td>51.6</td>
</tr>
</tbody>
</table>
<p>可以看出, ScribbleSup 对于 scribble length 还是比较鲁棒的, 甚至到了一个点都还能有不错的准确度.</p>
<h4 id="Comparisons-with-other-weakly-supervised-methods"><a href="#Comparisons-with-other-weakly-supervised-methods" class="headerlink" title="Comparisons with other weakly-supervised methods"></a>Comparisons with other weakly-supervised methods</h4><p>All methods are trained on the PASCAL VOC 2012 training images using VGG-16, except that the annotations are different.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Annotations</th>
<th>mIoU (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>MIL-FCN</td>
<td>image-level</td>
<td>25.1</td>
</tr>
<tr>
<td>WSSL</td>
<td>image-level</td>
<td>38.2</td>
</tr>
<tr>
<td>point supervision</td>
<td>spot</td>
<td>46.1</td>
</tr>
<tr>
<td>WSSL</td>
<td>box</td>
<td>60.6</td>
</tr>
<tr>
<td>BoxSup</td>
<td>box</td>
<td>62.0</td>
</tr>
<tr>
<td>ours</td>
<td>spot</td>
<td>51.6</td>
</tr>
<tr>
<td>ours</td>
<td>scribble</td>
<td>63.1</td>
</tr>
</tbody>
</table>
<p>可以看出</p>
<ul>
<li>虽然 image-level 的标注很容易标, 但是训练出来的结果惨不忍睹. </li>
<li>同时, 用 scribble 来标注得到的结果准确度很不错, 并且也是相对比较方便的.</li>
<li>ScribbleSup 即便是用 spot 标注, 结果的 mIoU 也比 point supervision 高了 5%.</li>
</ul>
<h4 id="Comparisons-with-using-masks"><a href="#Comparisons-with-using-masks" class="headerlink" title="Comparisons with using masks"></a>Comparisons with using masks</h4><p>虐了一遍同等级的 weakly-supervised 的方法之后, ScribbleSup 开始对比使用 scribble 和使用 mask 得到的结果. (在 PASCAL VOC 2012 上训练)</p>
<table>
<thead>
<tr>
<th>Supervision</th>
<th># w/ masks</th>
<th># w/scribbles</th>
<th>total</th>
<th>mIoU (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>weakly</td>
<td>-</td>
<td>11k</td>
<td>11k</td>
<td>63.1</td>
</tr>
<tr>
<td>strongly</td>
<td>11k</td>
<td>-</td>
<td>11k</td>
<td>68.5</td>
</tr>
<tr>
<td>semi</td>
<td>11k</td>
<td>10k (VOC07)</td>
<td>21k</td>
<td>71.3</td>
</tr>
</tbody>
</table>
<p>使用 scribble 比使用 mask 得到的结果差了5%左右, 考虑到这两者标注的困难程度, 这点差距还是可以忍的.</p>
<p>ScribbleSup 其实也是可以用 mask-level 的标注来训练的. 对于 mask-level 的标注, 不使用 graph model, 直接扔到 FCN 的训练里面去就行了. 注意的是这些只能用在 FCN 的训练步骤里, 优化 graph model 这一步骤中不使用. 可以看出, scribble 与 mask 联合起来能达到71.3%的 mIoU, 可以说是非常理想了.</p>
<p> <img src="/images/ScribbleSup_results_on_VOC_2012.png" alt="ScribbleSup_results_on_VOC_2012"></p>
<h3 id="Experiments-on-PASCAL-CONTEXT"><a href="#Experiments-on-PASCAL-CONTEXT" class="headerlink" title="Experiments on PASCAL-CONTEXT"></a>Experiments on PASCAL-CONTEXT</h3><p>To the best of our knowledge, our accuracy is the current state of the art on this dataset. (向dalao低头)</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Data/Annotations</th>
<th>mIoU (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>CFM</td>
<td>5k w/ masks</td>
<td>34.4</td>
</tr>
<tr>
<td>FCN</td>
<td>5k w/ masks</td>
<td>35.1</td>
</tr>
<tr>
<td>Boxsup</td>
<td>5k w/ masks + 133k w/ boxes (COCO+VOC7)</td>
<td>40.5</td>
</tr>
<tr>
<td>baseline</td>
<td>5k w/ masks</td>
<td>37.7</td>
</tr>
<tr>
<td>ours, weakly</td>
<td>5k w/ scribbles</td>
<td>36.1</td>
</tr>
<tr>
<td>ours, weakly</td>
<td>5k w/ scribbles + 10k w/ scribbles (VOC07)</td>
<td>39.3</td>
</tr>
<tr>
<td>ours, semi</td>
<td>5k w/ masks + 10k w/ scribbles (VOC07)</td>
<td>42.0</td>
</tr>
</tbody>
</table>
<p> <img src="/images/ScribbleSup_results_on_PASCAL_CONTEXT.png" alt="ScribbleSup_results_on_PASCAL_CONTEXT"></p>
<p>(To be continued…)</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;毕设需要写一个图像标注的软件, 来给场景分割的数据集做标注. 经学长推荐, 看了今年的这篇文章, 作者中竟然还有 Kaiming He 大神, 给微软膜一秒.&lt;/p&gt;
&lt;p&gt;这篇文章讲了一个弱监督的场景分割的算法 ScribbleSup, 主要是先通过 Graph Cut 将输入的 scribble 信息广播到没有标注的像素, 然后用 FCN 来做像素级别的预测. 令人遗憾的是 Github 上并没有人实现 (不能偷懒了TAT).&lt;/p&gt;
    
    </summary>
    
      <category term="Thesis Notes" scheme="http://www.yuthon.com/categories/Thesis-Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="ScribbleSup" scheme="http://www.yuthon.com/tags/ScribbleSup/"/>
    
      <category term="Scene Segmentation" scheme="http://www.yuthon.com/tags/Scene-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>Thesis Notes for YOLO</title>
    <link href="http://www.yuthon.com/2016/11/18/Thesis-Notes-for-YOLO/"/>
    <id>http://www.yuthon.com/2016/11/18/Thesis-Notes-for-YOLO/</id>
    <published>2016-11-18T14:43:26.000Z</published>
    <updated>2016-11-20T13:22:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>前几天发烧流鼻涕, 睡不了觉, 因此就熬夜读完了 YOLO 的论文. 可以说, YOLO 的实现方式相较于之前 R-CNN 一系的 Region Proposal 的方法来说, 很有新意. YOLO 将 Classification 和 Bounding Box Regression 合起来放进了 CNN 的输出层里面, 从而大大加快了速度.</p>
<a id="more"></a>
<h2 id="Unified-Detection"><a href="#Unified-Detection" class="headerlink" title="Unified Detection"></a>Unified Detection</h2><p>YOLO 将 Bounding Box 的位置回归和分类都放在了 CNN 的输出层中, 从整张图输入来预测 Bounding Box 的信息, 从而实现了 end-to-end 的训练, 实时的检测性能, 并且还保持了较高的精度.</p>
<p> <img src="/images/YOLO_the_model.png" alt="YOLO_the_model"></p>
<p>YOLO 将整张图分成了$S\times S$个网格 (论文中$S=2$), 如果一个物体的中心在某个网格内, 那么这个网格就负责预测这个物体的检测.</p>
<p>每个网格需要预测$B$个 Bounding Box (论文中$B=2$), 以及它们的置信度 (confidence).</p>
<ul>
<li>置信度定义为$Pr(Object) * IOU^{truth}_{pred}$<ul>
<li>$Pr(Object)$ 即为有物体的概率, 取0或1</li>
<li>$IOU^{truth}_{pred}$ 即为 ground truth 与predicted box 区域的交并比</li>
</ul>
</li>
<li>每个 Bounding Box 有5个属性$(x,y,w,h,c)$<ul>
<li>$(x,y)$ 代表 Bounding Box 的中心距离与网格边界的相对距离, 取值在0与1之间<ul>
<li>$x = \frac{x_{max} + x_{min}}{2 * width}$</li>
<li>$y = \frac{y_{max} + y_{min}}{2 * height}$</li>
</ul>
</li>
<li>$(w,h)$ 代表 Bounding Box 的长宽与整个图像长宽的相对比值, 取值在0与1之间<ul>
<li>$x = \frac{x_{max} - x_{min}}{width}$</li>
<li>$y = \frac{y_{max} - y_{min}}{height}$</li>
</ul>
</li>
<li>$c$ 即此 Bounding Box 的置信度</li>
</ul>
</li>
</ul>
<p>每个格子还要预测 $C$ 个类别的概率, 记为$Pr(Class_i|Object)$. 此概率与网格中是否有物体有关, 并且使相对于每个网格来说的, 与网格中的 Bounding Box 数量 $B$ 无关.</p>
<ul>
<li>测试时, 将 class 的条件概率和 box 的置信度乘起来, 得到每个 box 关于 class 的置信度</li>
<li>$Pr(Class_i|Object) * Pr(Object) * IOU^{truth}_{pred} = Pr(Class_i) * IOU^{truth}_{pred}$</li>
<li>这个概率既包含了 box 属于哪个 class 的概率, 又包含了这个 box 对于 object 的拟合度</li>
</ul>
<p>合起来看, 最终的预测张量的维数是 $S\times S \times (B*5 + C)$. 论文里用 PASCAL VOC 数据集, 取$S=7, B=2, C=20$, 因此总计$7\times 7 \times 30$.</p>
<h2 id="Network-Design"><a href="#Network-Design" class="headerlink" title="Network Design"></a>Network Design</h2><p> <img src="/images/YOLO_the_architecture.png" alt="YOLO_the_architecture"></p>
<p>整个网络参考了 GoogleNet, 总共有24个卷积层和两个全连接层.</p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>为了让整个网络有更好的性能, YOLO 使用了以下 tricks:</p>
<ul>
<li><p>前20层卷积层使用 ImageNet 进行 pretrain, 后4层卷积层和两层全连接层则是随机初始化</p>
</li>
<li><p>将输入图像的分辨率从$224\times 224$提升到$448*448$</p>
</li>
<li><p>将$(x,y,w,h)$全部都归一化 (详见上文)</p>
</li>
<li><p>最后一层(输出层)采用线性激活函数, 其它层都用 Leaky ReLU.</p>
</li>
<li><p>损失函数采用平方和误差(sum-squared error), 并且针对以下问题作出了改进:</p>
<ul>
<li>8维的 box 的位置信息$(x,y,w,h)$, 2维的置信度信息, 以及20维 box 的类别信息的平方和误差直接放在一起显然是不合理的. 因此增加 box 的位置信息的误差的权重系数$\lambda_{coord}$ (论文内取$5$).</li>
<li>同时, 一个图像会有很多网格没有物体, 那么就会把格子里的 box 的置信度变成 0, 导致那些真正有物体的柜子被压制, 最终导致整个网络发散.因此减少没有物体的 box 的权重系数$\lambda_noobj$ (论文内取$0.5$).</li>
<li>另外, 平方和误差会把 large box 和 small Box 的误差一视同仁. 然而相对于 large box 稍微偏一点, small box 的误差更加不能忍受. 因此使用$(\sqrt{w}, \sqrt{h})$而非$(w,h)$来计算误差.</li>
<li>每个格子里都有多个 Bounding Box, 但是在训练的时候我们希望对于每个物体只有一个 Bounding Box Predictor. 因此就选择与 ground truth 的 IoU 最大的那个, 称对该 box 对 该 object “负责” (responsible).</li>
</ul>
<p>最终整个的 loss function 如下:</p>
<p> <img src="/images/YOLO_loss_function.png" alt="YOLO_loss_function"></p>
<ul>
<li>$1^{obj}_{ij}$代表第$i$个网格中的第$j$个 box 是否对此 object “负责”, $1^{obj}_i$表示第$i$个网格中是否有 object.</li>
<li>该损失函数仅仅对有物体的网格的分类误差, 以及对 ground truth box 负责的 box 的位置误差进行惩罚</li>
</ul>
</li>
<li><p>另外还采用了 Dropout 和 Data Augmentation 的方法来增强泛化能力.</p>
<ul>
<li>$Dropout = 0.5$</li>
<li>对图像进行最大$20\%$的随机缩放和平移变换, 同时还有最大$1.5$的曝光与色调变换</li>
</ul>
</li>
</ul>
<h2 id="Limitations-of-YOLO"><a href="#Limitations-of-YOLO" class="headerlink" title="Limitations of YOLO"></a>Limitations of YOLO</h2><ul>
<li>由于 YOLO 每个网格只有 $B$ 个Bounding Box与1个 Class, 因此限制了临近物体检测到的个数</li>
<li>泛化能力不够, 由于降采样比较多导致只能用比较粗的特征</li>
<li>损失函数主要来源还是定位误差, 在对大小物体的位置误差的均衡上还需要改进.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前几天发烧流鼻涕, 睡不了觉, 因此就熬夜读完了 YOLO 的论文. 可以说, YOLO 的实现方式相较于之前 R-CNN 一系的 Region Proposal 的方法来说, 很有新意. YOLO 将 Classification 和 Bounding Box Regression 合起来放进了 CNN 的输出层里面, 从而大大加快了速度.&lt;/p&gt;
    
    </summary>
    
      <category term="Thesis Notes" scheme="http://www.yuthon.com/categories/Thesis-Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="YOLO" scheme="http://www.yuthon.com/tags/YOLO/"/>
    
      <category term="Object detection" scheme="http://www.yuthon.com/tags/Object-detection/"/>
    
      <category term="Darknet" scheme="http://www.yuthon.com/tags/Darknet/"/>
    
  </entry>
  
  <entry>
    <title>Train YOLO on our own dataset</title>
    <link href="http://www.yuthon.com/2016/11/12/Train-YOLO-on-our-own-dataset/"/>
    <id>http://www.yuthon.com/2016/11/12/Train-YOLO-on-our-own-dataset/</id>
    <published>2016-11-12T03:20:22.000Z</published>
    <updated>2016-11-18T07:39:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前到手 TX1 之后试了一下 YOLO 的 Demo, 感觉很是不错, 帧数勉强达到实时要求, 因此就萌生了使用自己的数据集来训练看看效果的想法. </p>
<a id="more"></a>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><h3 id="Get-The-ImageNet-Data"><a href="#Get-The-ImageNet-Data" class="headerlink" title="Get The ImageNet Data"></a>Get The ImageNet Data</h3><p>为了最大限度地利用资源 (其实是为了偷懒, 但是之后发现给自己挖了个大坑), 我用的是从 ImageNet 上的图片与 Bounding Box 标注. 本次使用了两个类别, 分别是 <a href="http://imagenet.stanford.edu/synset?wnid=n04254680#" target="_blank" rel="external">ball</a> 和 <a href="http://imagenet.stanford.edu/synset?wnid=n03820318" target="_blank" rel="external">goal</a>.</p>
<blockquote>
<ul>
<li>在<code>Downloads</code>内可以可以下到<code>images in the synset</code>以及<code>Bounding Boxes</code></li>
<li>ImageNet 里的图片看起来多, 实际上摊到每个子类上的就1000多张, 能下下来的就500多, 能直接和 Bounding Box 标注匹配的只剩此案100多了TAT. 果然还是需要自己标注, 自力更生.</li>
<li>另外 ImageNet 上的 Bounding Box 信息只有当前类别的. 比如说我下了 goal 的 Bounding Box, 其实某张图片里还有 ball 等 Object, 但是并不会被标出来. 这对于之后的训练有一定影响.</li>
<li>注意如果要从 ImageNet 上下原始图片的话是需要注册账号, 并且通过邮箱认证的 (还不能是 Gmail 这类的可以免费注册的邮箱, 需要机构或者学校邮箱才行).</li>
</ul>
</blockquote>
<p>下好的文件结构如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── Annotation</div><div class="line">│   ├── n03820318</div><div class="line">│   └── n04254680</div><div class="line">└── images</div><div class="line">    ├── n03820318</div><div class="line">    └── n04254680</div></pre></td></tr></table></figure>
<p>其中<code>Annotation</code>目录下放标注, <code>images</code>目录下放图片.</p>
<h3 id="Convert-labels-for-darknet"><a href="#Convert-labels-for-darknet" class="headerlink" title="Convert labels for darknet"></a>Convert labels for darknet</h3><p>ImageNet 上下下来的 Bounding Box 信息是 Pascal VOC 的 xml 格式:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">annotation</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">folder</span>&gt;</span>n03820318<span class="tag">&lt;/<span class="name">folder</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">filename</span>&gt;</span>n03820318_101<span class="tag">&lt;/<span class="name">filename</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">source</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">database</span>&gt;</span>ImageNet database<span class="tag">&lt;/<span class="name">database</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">source</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">size</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">width</span>&gt;</span>500<span class="tag">&lt;/<span class="name">width</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">height</span>&gt;</span>333<span class="tag">&lt;/<span class="name">height</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">depth</span>&gt;</span>3<span class="tag">&lt;/<span class="name">depth</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">size</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">segmented</span>&gt;</span>0<span class="tag">&lt;/<span class="name">segmented</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">object</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>n03820318<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">pose</span>&gt;</span>Unspecified<span class="tag">&lt;/<span class="name">pose</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">truncated</span>&gt;</span>0<span class="tag">&lt;/<span class="name">truncated</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">difficult</span>&gt;</span>0<span class="tag">&lt;/<span class="name">difficult</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">bndbox</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">xmin</span>&gt;</span>19<span class="tag">&lt;/<span class="name">xmin</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">ymin</span>&gt;</span>43<span class="tag">&lt;/<span class="name">ymin</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">xmax</span>&gt;</span>499<span class="tag">&lt;/<span class="name">xmax</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">ymax</span>&gt;</span>214<span class="tag">&lt;/<span class="name">ymax</span>&gt;</span></div><div class="line">		<span class="tag">&lt;/<span class="name">bndbox</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">object</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">annotation</span>&gt;</span></div></pre></td></tr></table></figure>
<p>而 darknet 需要的标注文件是 txt 格式:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;object-class&gt; &lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt;</div></pre></td></tr></table></figure>
<p>于是就需要对于 labels 进行转换. 我写了<a href="https://github.com/corenel/darknet/blob/yuthon/scripts/imagenet_bb_label.py" target="_blank" rel="external">一份 Python 脚本</a>, 将其放在数据集的根目录下执行即可:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ python imagenet_bb_label.py</div></pre></td></tr></table></figure>
<p>之后得到目录结构如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── Annotation</div><div class="line">│   ├── n03820318</div><div class="line">│   └── n04254680</div><div class="line">├── imagenet_bb_label.py</div><div class="line">├── images</div><div class="line">│   ├── n03820318</div><div class="line">│   └── n04254680</div><div class="line">├── labels</div><div class="line">│   ├── n03820318</div><div class="line">│   └── n04254680</div><div class="line">└── train.txt</div></pre></td></tr></table></figure>
<p>其中, <code>labels</code>目录保存着转换后的 Bounding Box 信息, <code>train.txt</code>则包含了所有图片文件的绝对路径.</p>
<blockquote>
<p>ImageNet 的 xml 文件里 object 的名字是类似<code>n03820318</code>这种格式的, 如果需要转成<code>goal</code>这样的话可以再目录下执行以下命令来批量替换:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; $ find . -name <span class="string">"*.xml"</span> -<span class="built_in">print</span> | xargs sed -i <span class="string">'s/&lt;name&gt;n03820318/&lt;name&gt;goal/g'</span></div><div class="line">&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h2 id="Modify-darknet"><a href="#Modify-darknet" class="headerlink" title="Modify darknet"></a>Modify darknet</h2><p>由于 class 的数量和名字都变了, 因此需要修改下 YOLO 的源代码.</p>
<p>首先是从 clone repository. 可以选择 clone <a href="https://github.com/pjreddie/darknet" target="_blank" rel="external">官方的</a>, 也可以直接下<a href="https://github.com/corenel/darknet" target="_blank" rel="external">我修改好的</a>. 此处里官方 repo 为例.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/pjreddie/darknet.git</div></pre></td></tr></table></figure>
<p>由于最新的 commit 修改了 label image 的显示方法, 并且改变了源文件里类别的定义, 因此需要先切回之前的 commit:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git checkout 73f7aacf35ec9b1d0f9de9ddf38af0889f213e99</div></pre></td></tr></table></figure>
<p>首先修改<code>Makefile</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">GPU=1</div><div class="line">CUDNN=1</div><div class="line">OPENCV=1</div></pre></td></tr></table></figure>
<p>之后是<code>src/yolo.c</code>, 主要是类别名称和数量, 以及<code>train.txt</code>与<code>backup</code>的地址.(<code>backup</code>目录用来存放训练得到的weights)</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// ...</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> NUM_CLASS 2</span></div><div class="line"><span class="keyword">char</span> *voc_names[] = &#123;<span class="string">"ball"</span>, <span class="string">"goal"</span>&#125;;</div><div class="line">image voc_labels[NUM_CLASS];</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">train_yolo</span><span class="params">(<span class="keyword">char</span> *cfgfile, <span class="keyword">char</span> *weightfile)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="keyword">char</span> *train_images = <span class="string">"/home/m/workspace/dataset/train.txt"</span>;</div><div class="line">    <span class="keyword">char</span> *backup_directory = <span class="string">"/home/m/workspace/backup/"</span>;</div><div class="line">&#125;</div><div class="line"><span class="comment">// ...</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">test_yolo</span><span class="params">(<span class="keyword">char</span> *cfgfile, <span class="keyword">char</span> *weightfile, <span class="keyword">char</span> *filename, <span class="keyword">float</span> thresh)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">  <span class="comment">// ...</span></div><div class="line">  draw_detections(im, l.side*l.side*l.n, thresh, boxes, probs, voc_names, voc_labels, NUM_CLASS);</div><div class="line">  <span class="comment">// ...</span></div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">run_yolo</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">  <span class="comment">// ...</span></div><div class="line">  <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; NUM_CLASS; ++i)&#123;</div><div class="line">    <span class="comment">// ...</span></div><div class="line">  &#125;</div><div class="line">  <span class="comment">// ...</span></div><div class="line">  <span class="keyword">else</span> <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[<span class="number">2</span>], <span class="string">"demo"</span>)) demo(cfg, weights, thresh, cam_index, filename, voc_names, voc_labels, NUM_CLASS, frame_skip);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>接着是<code>yolo_kernels.cu</code>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// ...</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> NUM_CLASS 2</span></div><div class="line"><span class="comment">// ...</span></div><div class="line"><span class="function"><span class="keyword">void</span> *<span class="title">detect_in_thread</span><span class="params">(<span class="keyword">void</span> *ptr)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">  <span class="comment">// ...</span></div><div class="line">  draw_detections(det, l.side*l.side*l.n, demo_thresh, boxes, probs, voc_names, voc_labels, NUM_CLASS);</div><div class="line">  <span class="comment">// ...</span></div><div class="line">&#125;</div><div class="line"><span class="comment">// ...</span></div></pre></td></tr></table></figure>
<p>然后是<code>cfg</code>(建议新建一个, 我的<a href="https://github.com/corenel/darknet/blob/yuthon/cfg/tiny-yolo.train.cfg" target="_blank" rel="external">配置</a>可作为参考):</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># ...</span></div><div class="line"><span class="section">[connected]</span></div><div class="line"><span class="comment"># output = Side x Side x (2x5 + class_num)</span></div><div class="line"><span class="attr">output</span>= <span class="number">588</span></div><div class="line"><span class="attr">activation</span>=linear</div><div class="line"></div><div class="line"><span class="section">[detection]</span></div><div class="line"><span class="comment"># modify the class num</span></div><div class="line"><span class="attr">classes</span>=<span class="number">2</span></div><div class="line"><span class="attr">coords</span>=<span class="number">4</span></div><div class="line"><span class="attr">rescore</span>=<span class="number">1</span></div><div class="line"><span class="attr">side</span>=<span class="number">7</span></div><div class="line"><span class="attr">num</span>=<span class="number">2</span></div><div class="line"><span class="attr">softmax</span>=<span class="number">0</span></div><div class="line"><span class="attr">sqrt</span>=<span class="number">1</span></div><div class="line"><span class="attr">jitter</span>=.<span class="number">2</span></div><div class="line"></div><div class="line"><span class="attr">object_scale</span>=<span class="number">1</span></div><div class="line"><span class="attr">noobject_scale</span>=.<span class="number">5</span></div><div class="line"><span class="attr">class_scale</span>=<span class="number">1</span></div><div class="line"><span class="attr">coord_scale</span>=<span class="number">5</span></div></pre></td></tr></table></figure>
<p>最后, 如果使用了新的 class 的话, 需要在<code>data/labels</code>里修改<code>make_labels.py</code>并执行来生成新的 label image:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line">l = [<span class="string">"ball"</span>, <span class="string">"goal"</span>]</div><div class="line"></div><div class="line"><span class="keyword">for</span> word <span class="keyword">in</span> l:</div><div class="line">    os.system(<span class="string">"convert -fill black -background white -bordercolor white -border 4 -font ubuntu-mono -pointsize 18 label:\"%s\" \"%s.png\""</span>%(word, word))</div></pre></td></tr></table></figure>
<p>至此前期准备完成, 可以开始训练了.</p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>首先还需要下载 pre-trained weights.</p>
<ul>
<li>全尺寸的 YOLO 使用<a href="http://pjreddie.com/media/files/extraction.conv.weights" target="_blank" rel="external">这个</a></li>
<li>tiny-YOLO 使用<a href="http://pjreddie.com/media/files/darknet.conv.weights" target="_blank" rel="external">这个</a></li>
</ul>
<p>之后就是慢慢训练之路了:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> darknet</div><div class="line">$ make -j8</div><div class="line">$ ./darknet yolo train cfg/tiny-yolo.train.cfg darknet.conv.weights</div></pre></td></tr></table></figure>
<p>我用上述的 dataset 训练 tiny-YOLO, 从 22:43 一直到 05:12, 总计 6 个小时左右, 最终得到<code>tiny-yolo_final.weights</code>文件.</p>
<p>之后就可以拿来 test 或者 demo 了:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./darknet yolo <span class="built_in">test</span> cfg/tiny-yolo.train.cfg tiny-yolo_final.weights</div></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./darknet yolo demo cfg/tiny-yolo.train.cfg tiny-yolo_final.weights</div></pre></td></tr></table></figure>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p><img src="/images/yolo-tiny_on_TX1_ball.png" alt="yolo-tiny_on_TX1_ball"></p>
<p><img src="/images/yolo-tiny_on_TX1_goal.png" alt="yolo-tiny_on_TX1_goal"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://guanghan.info/blog/en/my-works/train-yolo/" target="_blank" rel="external">Start Training YOLO with Our Own Data</a></li>
<li><a href="http://pjreddie.com/darknet/yolo/#train" target="_blank" rel="external">Training YOLO</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前到手 TX1 之后试了一下 YOLO 的 Demo, 感觉很是不错, 帧数勉强达到实时要求, 因此就萌生了使用自己的数据集来训练看看效果的想法. &lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="YOLO" scheme="http://www.yuthon.com/tags/YOLO/"/>
    
      <category term="Object detection" scheme="http://www.yuthon.com/tags/Object-detection/"/>
    
      <category term="Darknet" scheme="http://www.yuthon.com/tags/Darknet/"/>
    
  </entry>
  
  <entry>
    <title>YOLO on NVIDIA Jetson TX1</title>
    <link href="http://www.yuthon.com/2016/11/10/YOLO-on-NVIDIA-Jetson-TX1/"/>
    <id>http://www.yuthon.com/2016/11/10/YOLO-on-NVIDIA-Jetson-TX1/</id>
    <published>2016-11-10T12:36:34.000Z</published>
    <updated>2016-11-12T03:21:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>实验室昨天到了 NVIDIA 的 <a href="http://www.nvidia.com/object/jetson-tx1-module.html" target="_blank" rel="external">Jetson TX1</a>, 可以说是移动端比较好的带GPU的开发板子了, 于是可以试试在移动端上用YOLO (You Look Only Once) 来做目标识别.</p>
<a id="more"></a>
<h2 id="Specifications"><a href="#Specifications" class="headerlink" title="Specifications"></a>Specifications</h2><table>
<thead>
<tr>
<th>GPU</th>
<th>1 TFLOP/s 256-core with <a href="https://developer.nvidia.com/maxwell-compute-architecture" target="_blank" rel="external">NVIDIA Maxwell™ Architecture</a></th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>64-bit ARM® A57 CPUs</td>
</tr>
<tr>
<td>Memory</td>
<td>4 GB LPDDR4, 25.6 GB/s</td>
</tr>
<tr>
<td>Video decode</td>
<td>4K 60 Hz</td>
</tr>
<tr>
<td>Video encode</td>
<td>4K 30 Hz</td>
</tr>
<tr>
<td>CSI</td>
<td>Up to 6 cameras, 1400 Mpix/s</td>
</tr>
<tr>
<td>Display</td>
<td>2x DSI, 1x eDP 1.4, 1x DP 1.2/HDMI</td>
</tr>
<tr>
<td>Connectivity</td>
<td>Connects to 802.11ac Wi-Fi and Bluetooth-enabled devices</td>
</tr>
<tr>
<td>Networking</td>
<td>1 Gigabit Ethernet</td>
</tr>
<tr>
<td>PCIE</td>
<td>Gen 2 1x1 + 1x4</td>
</tr>
<tr>
<td>Storage</td>
<td>16 GB eMMC, SDIO, SATA</td>
</tr>
<tr>
<td>Other</td>
<td>3x UART, 3x SPI, 4x I2C, 4x I2S, GPIOs</td>
</tr>
</tbody>
</table>
<blockquote>
<p>标称1TFlops这个比较猛, 都快比得上XPS 15 9550的GTX960M了.</p>
</blockquote>
<h2 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h2><p>到手TX1之后发现是 Ubuntu 14.04 32-bit 的, 果断先用 <a href="https://developer.nvidia.com/embedded/jetpack" target="_blank" rel="external">JetPack 2.3</a> 升级到 Ubuntu 16.04 64bit. 用 JetPack 刷机的好处是能够顺便配置一大堆库, 比如说 CUDA, cuDNN, OpenCV4Terga 之类的.</p>
<ul>
<li><p>JetPack 在刷机之前需要下载一大堆 Package, 因此在国内的话最好在运行前配置好代理.</p>
</li>
<li><p>JetPack 刷完系统后会要求按 reset 键重启进 GUI, 之后就是不断地安装包安装依赖的过程, 因此在国内的话可以趁此机会修改<code>/etc/apt/source.list</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-updates main restricted universe multiverse</div><div class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-updates main restricted universe multiverse</div><div class="line">deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-security main restricted universe multiverse</div><div class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-security main restricted universe multiverse</div><div class="line">deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-backports main restricted universe multiverse</div><div class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-backports main restricted universe multiverse</div><div class="line">deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial main universe restricted</div><div class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial main universe restricted</div><div class="line">deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial universe</div></pre></td></tr></table></figure>
<blockquote>
<p>注意arm64的源与普通的x86-64的源是不一样的.</p>
</blockquote>
</li>
</ul>
<h2 id="Darknet"><a href="#Darknet" class="headerlink" title="Darknet"></a>Darknet</h2><p>为了用 Webcam demo, 所以需要 Compiling with CUDA and OpenCV:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/pjreddie/darknet.git</div><div class="line">$ <span class="built_in">cd</span> darknet</div><div class="line">$ sed <span class="string">'s/GPU=0/GPU=1/g'</span> Makefile</div><div class="line">$ sed <span class="string">'s/CUDNN=0/CUDNN=1/g'</span> Makefile</div><div class="line">$ sed <span class="string">'s/OPENCV=0/OPENCV=1/g'</span> Makefile</div><div class="line">$ make -j4</div></pre></td></tr></table></figure>
<p>上面编译完了之后输入以下指令, 与输出结果相对应, 那就说明成功了</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ ./darknet</div><div class="line">$ usage: ./darknet &lt;<span class="keyword">function</span>&gt;</div></pre></td></tr></table></figure>
<h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><p>先去下训练好的<a href="http://pjreddie.com/darknet/yolo/#models" target="_blank" rel="external">权重</a>, 建议选 yolo-tiny 的, 吃内存少. (毕竟 TX1 只有 4GB 内存, 还是 CPU 和 GPU 共用的)</p>
<p>之后运行一下命令即可测试 Real-Time Detection on a Webcam:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./darknet yolo demo cfg/tiny-yolo.cfg tiny-yolo.weights</div></pre></td></tr></table></figure>
<p>实际效果如下:</p>
<p> <img src="/images/yolo-tiny_on_TX1.png" alt="yolo-tiny_on_TX1"></p>
<p>左下为摄像头实拍屏幕的画面, 可以看出检测结果还是很不错的.</p>
<p>帧数有12fps左右, 基本上达到实时要求.</p>
<h2 id="Re-train"><a href="#Re-train" class="headerlink" title="Re-train"></a>Re-train</h2><p>重新训练 YOLO, 使其识别球与球门.</p>
<p>(To be continued…)</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="external">YOLO: Real-Time Object Detection</a></li>
<li><a href="http://guanghan.info/blog/en/my-works/train-yolo/" target="_blank" rel="external">Start Training YOLO with Our Own Data</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;实验室昨天到了 NVIDIA 的 &lt;a href=&quot;http://www.nvidia.com/object/jetson-tx1-module.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Jetson TX1&lt;/a&gt;, 可以说是移动端比较好的带GPU的开发板子了, 于是可以试试在移动端上用YOLO (You Look Only Once) 来做目标识别.&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="YOLO" scheme="http://www.yuthon.com/tags/YOLO/"/>
    
      <category term="Object detection" scheme="http://www.yuthon.com/tags/Object-detection/"/>
    
      <category term="Darknet" scheme="http://www.yuthon.com/tags/Darknet/"/>
    
      <category term="NVIDIA Jetson TX1" scheme="http://www.yuthon.com/tags/NVIDIA-Jetson-TX1/"/>
    
  </entry>
  
  <entry>
    <title>Solution for &#39;import tensorflow&#39; error in REPL on macOS</title>
    <link href="http://www.yuthon.com/2016/11/02/Solution-for-import-tensorflow-error-in-REPL-on-macOS/"/>
    <id>http://www.yuthon.com/2016/11/02/Solution-for-import-tensorflow-error-in-REPL-on-macOS/</id>
    <published>2016-11-02T03:32:01.000Z</published>
    <updated>2016-11-02T03:44:04.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>使用 pip 安装 Tensorflow 之后, 在 REPL 中执行<code>import tensorflow as tf</code>之后, 报出以下错误:</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">Python <span class="number">2.7</span><span class="number">.10</span> (default, Oct <span class="number">23</span> <span class="number">2015</span>, <span class="number">19</span>:<span class="number">19</span>:<span class="number">21</span>)</div><div class="line">[GCC <span class="number">4.2</span><span class="number">.1</span> Compatible Apple LLVM <span class="number">7.0</span><span class="number">.0</span> (clang<span class="number">-700.0</span><span class="number">.59</span><span class="number">.5</span>)] on darwin</div><div class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> <span class="keyword">or</span> <span class="string">"license"</span> <span class="keyword">for</span> more information.</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/__init__.py"</span>, line <span class="number">23</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    <span class="keyword">from</span> tensorflow.python <span class="keyword">import</span> *</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py"</span>, line <span class="number">53</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    <span class="keyword">from</span> tensorflow.core.framework.graph_pb2 <span class="keyword">import</span> *</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/core/framework/graph_pb2.py"</span>, line <span class="number">16</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    <span class="keyword">from</span> tensorflow.core.framework <span class="keyword">import</span> node_def_pb2 <span class="keyword">as</span> tensorflow_dot_core_dot_framework_dot_node__def__pb2</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/core/framework/node_def_pb2.py"</span>, line <span class="number">16</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    <span class="keyword">from</span> tensorflow.core.framework <span class="keyword">import</span> attr_value_pb2 <span class="keyword">as</span> tensorflow_dot_core_dot_framework_dot_attr__value__pb2</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py"</span>, line <span class="number">16</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    <span class="keyword">from</span> tensorflow.core.framework <span class="keyword">import</span> tensor_pb2 <span class="keyword">as</span> tensorflow_dot_core_dot_framework_dot_tensor__pb2</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/core/framework/tensor_pb2.py"</span>, line <span class="number">16</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    <span class="keyword">from</span> tensorflow.core.framework <span class="keyword">import</span> tensor_shape_pb2 <span class="keyword">as</span> tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py"</span>, line <span class="number">22</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    serialized_pb=_b(<span class="string">'\n,tensorflow/core/framework/tensor_shape.proto\x12\ntensorflow\"z\n\x10TensorShapeProto\x12-\n\x03\x64im\x18\x02 \x03(\x0b\x32 .tensorflow.TensorShapeProto.Dim\x12\x14\n\x0cunknown_rank\x18\x03 \x01(\x08\x1a!\n\x03\x44im\x12\x0c\n\x04size\x18\x01 \x01(\x03\x12\x0c\n\x04name\x18\x02 \x01(\tB2\n\x18org.tensorflow.frameworkB\x11TensorShapeProtosP\x01\xf8\x01\x01\x62\x06proto3'</span>)</div><div class="line">TypeError: __init__() got an unexpected keyword argument <span class="string">'syntax'</span></div></pre></td></tr></table></figure>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>经检查是<code>protobuf</code>的锅. TF 需要<code>protobuf&gt;=3.0.0a3</code>, 而macOS里似乎有两份<code>protobuf</code>, 一份是之前装的2.6.1, 另外一份是随着 TF 装的. 默认似乎是调用到了 2.6.1 的那个版本.</p>
<p>找到原因就好办了, 卸掉重装呗:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash"> sudo pip uninstall protobuf</span></div><div class="line"><span class="meta">$</span><span class="bash"> sudo pip uninstall tensorflow</span></div><div class="line"><span class="meta">$</span><span class="bash"> brew uninstall protobuf</span></div><div class="line"><span class="meta">$</span><span class="bash"> sudo pip install --upgrade <span class="variable">$TF_BINARY_URL</span></span></div></pre></td></tr></table></figure>
<p>之后就好了, 确认一下是不是调用了3.0.0的版本:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> google.protobuf</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>google.protobuf.__file__</div><div class="line"><span class="string">'/Library/Python/2.7/site-packages/google/protobuf/__init__.pyc'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>google.protobuf.__version__</div><div class="line"><span class="string">'3.0.0'</span></div></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://stackoverflow.com/questions/33622842/error-in-python-after-import-tensorflow-typeerror-init-got-an-unexpect" target="_blank" rel="external">Error in python after ‘import tensorflow’: TypeError: <strong>init</strong>() got an unexpected keyword argument ‘syntax’</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Description&quot;&gt;&lt;a href=&quot;#Description&quot; class=&quot;headerlink&quot; title=&quot;Description&quot;&gt;&lt;/a&gt;Description&lt;/h2&gt;&lt;p&gt;使用 pip 安装 Tensorflow 之后, 在 REPL 中执行&lt;code&gt;import tensorflow as tf&lt;/code&gt;之后, 报出以下错误:&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="macOS" scheme="http://www.yuthon.com/tags/macOS/"/>
    
      <category term="Tensorflow" scheme="http://www.yuthon.com/tags/Tensorflow/"/>
    
      <category term="Python" scheme="http://www.yuthon.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Solution for matplotlib importing error on macOS</title>
    <link href="http://www.yuthon.com/2016/11/01/Solution-for-matplotlib-importing-error-on-Mac-OS-X/"/>
    <id>http://www.yuthon.com/2016/11/01/Solution-for-matplotlib-importing-error-on-Mac-OS-X/</id>
    <published>2016-11-01T13:57:58.000Z</published>
    <updated>2016-11-02T03:34:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天在做 CS231n 的 Assignment #2 的时候遇到了导入 matplotlib.pyplot 的问题, 特此记录.</p>
<h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>打开 IPython Notebook 之后, 执行以下命令:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div></pre></td></tr></table></figure>
<p>出现错误:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ValueError: unknown locale: UTF<span class="number">-8</span></div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>把下面这些加到<code>~/.zshrc</code>或者是<code>~/.bash_profile</code>里面:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export LC_ALL=en_US.UTF-8</div><div class="line">export LANG=en_US.UTF-8</div></pre></td></tr></table></figure>
<p>同时, 如果用 iTerm 的话, 还需要在 Preference -&gt; Profiles -&gt; Terminal -&gt; Environment 中, 取消选择 <code>Set locale variables automatically</code>.</p>
<p><strong>然而</strong>又出现了下列报错:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ImportError: cannot <span class="keyword">import</span> name _thread</div></pre></td></tr></table></figure>
<p>这个问题已经在最新的<code>six</code>和<code>dateutil</code>库中解决了, 然而 macOS 本身却还在使用旧版本的库. 解决方法如下:</p>
<ul>
<li><p>执行以下命令安装最新版本的<code>six</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash"> sudo pip install six -U</span></div></pre></td></tr></table></figure>
</li>
<li><p>开 python 看看是否还在使用旧版本的库:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> six</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>six.__file__</div><div class="line">/System/Library/Frameworks/Python.framework/Versions/<span class="number">2.7</span>/Extras/lib/python/six.pyc</div></pre></td></tr></table></figure>
<p>显然确实是这样</p>
</li>
<li><p>删除旧版本的库</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">$</span><span class="bash"> rm -rf /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six.*</span></div></pre></td></tr></table></figure>
<p>这样就可以了, 之后 python 会使用我们之前新装的版本的<code>six</code>库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> six</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>six.__file__</div><div class="line"><span class="string">'/Library/Python/2.7/site-packages/six.pyc'</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p>之后再执行<code>import matplotlib.pyplot as plt</code>之后就没问题了.</p>
<p>说到底还是 macOS 的锅…</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><p><a href="https://coderwall.com/p/-k_93g/mac-os-x-valueerror-unknown-locale-utf-8-in-python" target="_blank" rel="external">Mac OS X: ValueError: unknown locale: UTF-8 in Python</a></p>
</li>
<li><p><a href="http://stackoverflow.com/questions/27630114/matplotlib-issue-on-os-x-importerror-cannot-import-name-thread" target="_blank" rel="external">Matplotlib issue on OS X (“ImportError: cannot import name _thread”)</a></p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天在做 CS231n 的 Assignment #2 的时候遇到了导入 matplotlib.pyplot 的问题, 特此记录.&lt;/p&gt;
&lt;h2 id=&quot;Description&quot;&gt;&lt;a href=&quot;#Description&quot; class=&quot;headerlink&quot; title=&quot;Description&quot;&gt;&lt;/a&gt;Description&lt;/h2&gt;&lt;p&gt;打开 IPython Notebook 之后, 执行以下命令:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;出现错误:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;ValueError: unknown locale: UTF&lt;span class=&quot;number&quot;&gt;-8&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="macOS" scheme="http://www.yuthon.com/tags/macOS/"/>
    
      <category term="Python" scheme="http://www.yuthon.com/tags/Python/"/>
    
      <category term="Matplotlib" scheme="http://www.yuthon.com/tags/Matplotlib/"/>
    
  </entry>
  
</feed>
