<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yuthon&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.yuthon.com/"/>
  <updated>2017-04-25T13:25:27.000Z</updated>
  <id>http://www.yuthon.com/</id>
  
  <author>
    <name>Yusu Pan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Modern C++ Libraries: Getting Started</title>
    <link href="http://www.yuthon.com/2017/04/25/Modern-C-Libraries-Getting-Started/"/>
    <id>http://www.yuthon.com/2017/04/25/Modern-C-Libraries-Getting-Started/</id>
    <published>2017-04-25T13:06:39.000Z</published>
    <updated>2017-04-25T13:25:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post is a note for Modern C++ Libraries in Pluralsight.</p>
<a id="more"></a>
<h2 id="Assertions"><a href="#Assertions" class="headerlink" title="Assertions"></a>Assertions</h2><h3 id="Course-content"><a href="#Course-content" class="headerlink" title="Course content"></a>Course content</h3><p>An <strong>Assertion</strong> may be a function, but usually a macro, that brings your application to an immediate standstill if an assumption is broken.</p>
<p>Assertions <strong>document the assumptions</strong> such that those assumptions can be <strong>validated</strong> usually at run time, but also increasingly at compile time.</p>
<ul>
<li><p>Check pointers before using them (at run time):</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">assert(pointer);</div></pre></td></tr></table></figure>
</li>
<li><p>Confirm the  validity of some input at the boundary of application or component (at run time):</p>
</li>
<li><p>Confirm some expressions at compile time (no need to execute)</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static_assert</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) == <span class="number">4</span>, <span class="string">"I can’t float like that!"</span>);</div></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>Static or compile time Assertions</strong> are checked staticially during build. And <strong>Runtime Assertions</strong> are typically conditionally complied and included owning debug builds and stripped out of release or free builds.</p>
<blockquote>
<p>Runtime Assertions must not include expression that the application relies upon. It should not change the application state in any way.</p>
</blockquote>
<h3 id="C-documentation"><a href="#C-documentation" class="headerlink" title="C++ documentation"></a>C++ documentation</h3><h4 id="assert"><a href="#assert" class="headerlink" title="assert"></a>assert</h4><p>The definition of the macro <code>assert</code> depends on another macro, <strong><code>NDEBUG</code></strong>, which is not defined by the standard library.</p>
<p>If <code>NDEBUG</code> is defined as a macro name at the point in the source code where <code>&lt;cassert&gt;</code> is included, then <code>assert</code> does nothing.</p>
<p>If <code>NDEBUG</code> is not defined, then <code>assert</code> checks if its argument (which must have scalar type) compares equal to zero. If it does, <code>assert</code> outputs implementation-specific diagnostic information on the standard error output and calls <em>std::abort</em>.</p>
<h5 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h5><p>Because <code>assert</code> is a <em>function-like macro</em>, commas anywhere in condition that are not protected by parentheses are interpreted as macro argument separators. Such commas are often found in template argument lists:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">assert(<span class="built_in">std</span>::is_same_v&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;); <span class="comment">// error: assert does not take two arguments</span></div><div class="line">assert((<span class="built_in">std</span>::is_same_v&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;)); <span class="comment">// OK: one argument</span></div><div class="line"><span class="keyword">static_assert</span>(<span class="built_in">std</span>::is_same_v&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;); <span class="comment">// OK: not a macro</span></div></pre></td></tr></table></figure>
<h5 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h5><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="comment">// uncomment to disable assert()</span></div><div class="line"><span class="comment">// #define NDEBUG</span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cassert&gt;</span></span></div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">    assert(<span class="number">2</span>+<span class="number">2</span>==<span class="number">4</span>);</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Execution continues past the first assert\n"</span>;</div><div class="line">    assert(<span class="number">2</span>+<span class="number">2</span>==<span class="number">5</span>);</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Execution continues past the second assert\n"</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Possible output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Execution continues past the first assert</div><div class="line">test: test.cc:10: int main(): Assertion `2+2==5&apos; failed.</div><div class="line">Aborted</div></pre></td></tr></table></figure>
<h4 id="Static-Assertion"><a href="#Static-Assertion" class="headerlink" title="Static Assertion"></a>Static Assertion</h4><p>Performs compile-time assertion checking</p>
<h5 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h5><p>Since message has to be a string literal, it cannot contain dynamic information or even a constant expression that is not a string literal itself. In particular, it cannot contain the name of the template type argument.</p>
<h5 id="Example-1"><a href="#Example-1" class="headerlink" title="Example"></a>Example</h5><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;type_traits&gt;</span></span></div><div class="line"> </div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> T&gt;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(T&amp; a, T&amp; b)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">static_assert</span>(<span class="built_in">std</span>::is_copy_constructible&lt;T&gt;::value,</div><div class="line">                  <span class="string">"Swap requires copying"</span>);</div><div class="line">    <span class="keyword">static_assert</span>(<span class="built_in">std</span>::is_nothrow_move_constructible&lt;T&gt;::value</div><div class="line">               &amp;&amp; <span class="built_in">std</span>::is_nothrow_move_assignable&lt;T&gt;::value,</div><div class="line">                  <span class="string">"Swap may throw"</span>);</div><div class="line">    <span class="keyword">auto</span> c = b;</div><div class="line">    b = a;</div><div class="line">    a = c;</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> T&gt;</div><div class="line"><span class="keyword">struct</span> data_structure</div><div class="line">&#123;</div><div class="line">    <span class="keyword">static_assert</span>(<span class="built_in">std</span>::is_default_constructible&lt;T&gt;::value,</div><div class="line">                  <span class="string">"Data Structure requires default-constructible elements"</span>);</div><div class="line">&#125;;</div><div class="line"> </div><div class="line"><span class="keyword">struct</span> no_copy</div><div class="line">&#123;</div><div class="line">    no_copy ( <span class="keyword">const</span> no_copy&amp; ) = <span class="keyword">delete</span>;</div><div class="line">    no_copy () = <span class="keyword">default</span>;</div><div class="line">&#125;;</div><div class="line"> </div><div class="line"><span class="keyword">struct</span> no_default</div><div class="line">&#123;</div><div class="line">    no_default () = <span class="keyword">delete</span>;</div><div class="line">&#125;;</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> a, b;</div><div class="line">    swap(a, b);</div><div class="line"> </div><div class="line">    no_copy nc_a, nc_b;</div><div class="line">    swap(nc_a, nc_b); <span class="comment">// 1</span></div><div class="line"> </div><div class="line">    data_structure&lt;<span class="keyword">int</span>&gt; ds_ok;</div><div class="line">    data_structure&lt;no_default&gt; ds_error; <span class="comment">// 2</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Possible output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1: error: static assertion failed: Swap requires copying</div><div class="line">2: error: static assertion failed: Data Structure requires default-constructible elements</div></pre></td></tr></table></figure>
<h2 id="VERIFY"><a href="#VERIFY" class="headerlink" title="VERIFY"></a>VERIFY</h2><p><code>Verify</code> behaves exactly the same as Assert in debug builds. Indeed it is an Assertion. But in release builds it drops the verification, but keeps the expression. </p>
<p>Unlike runtime Assertions the Verify macro is used for those cases where the expression is essential to the applications operation and cannot simply be stripped out of release builds.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> NDEBUG</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> VERIFY assert</span></div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> VERIFY(experssion) (expression)</span></div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div></pre></td></tr></table></figure>
<h2 id="TRACE"><a href="#TRACE" class="headerlink" title="TRACE"></a>TRACE</h2><p><code>Trace</code> macro is to provide formatted output for the debugger to display, but that can be stripped out of release builds. </p>
<p>Following code works only in VC++:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line">#include &lt;stdio.h&gt;</div><div class="line">#include &lt;windows.h&gt;</div><div class="line"></div><div class="line">#ifdef _DEBUG</div><div class="line">inline auto Trace(wchar_t const * format, ...) -&gt; void</div><div class="line">&#123;</div><div class="line">    va_list args;</div><div class="line">    va_start(args, format);</div><div class="line"></div><div class="line">    wchar_t buffer [256];</div><div class="line"></div><div class="line">    ASSERT(-1 != _vsnwprintf_s(buffer,</div><div class="line">                               _countof(buffer) -1,</div><div class="line">                               format,</div><div class="line">                               args));</div><div class="line"></div><div class="line">    va_end(args);</div><div class="line"></div><div class="line">    OutputDebugString(buffer);</div><div class="line">&#125;</div><div class="line"></div><div class="line">struct Tracer</div><div class="line">&#123;</div><div class="line">    char const * m_filename;</div><div class="line">    unsigned m_line;</div><div class="line"></div><div class="line">    Tracer(char const * filename, unsigned const line) :</div><div class="line">        m_filename &#123; filename &#125;,</div><div class="line">        m_line &#123; line &#125;</div><div class="line">    &#123;</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    template &lt;typename... Args&gt;</div><div class="line">    auto operator()(wchar_t const * format, Args... args) const -&gt; void</div><div class="line">    &#123;</div><div class="line">        wchar_t buffer [256];</div><div class="line"></div><div class="line">        auto count = swprintf_s(buffer,</div><div class="line">                                L"%S(%d): ",</div><div class="line">                                m_filename,</div><div class="line">                                m_line);</div><div class="line"></div><div class="line">        ASSERT(-1 != count);</div><div class="line"></div><div class="line">        ASSERT(-1 != _snwprintf_s(buffer + count,</div><div class="line">                                  _countof(buffer) - count,</div><div class="line">                                  _countof(buffer) - count - 1,</div><div class="line">                                  format,</div><div class="line">                                  args...));</div><div class="line"></div><div class="line">        OutputDebugString(buffer);</div><div class="line">    &#125;</div><div class="line">&#125;;</div><div class="line"></div><div class="line">#endif</div><div class="line"></div><div class="line">#ifdef _DEBUG</div><div class="line">#define TRACE Tracer(__FILE__, __LINE__)</div><div class="line">#else</div><div class="line">#define TRACE __noop</div><div class="line">#endif</div><div class="line"></div><div class="line">auto main() -&gt; int</div><div class="line">&#123;</div><div class="line">    TRACE(L"1 + 2 = %d\n", 1 + 2);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="My-point"><a href="#My-point" class="headerlink" title="My point"></a>My point</h2><p>Assertions are useful, however, <code>VERIFY</code> and <code>TRACE</code> can be repalced by other logging libaries.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is a note for Modern C++ Libraries in Pluralsight.&lt;/p&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="C++" scheme="http://www.yuthon.com/tags/C/"/>
    
      <category term="Pluralsight" scheme="http://www.yuthon.com/tags/Pluralsight/"/>
    
      <category term="C++11" scheme="http://www.yuthon.com/tags/C-11/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow r1.0 on TX1 (now successful)</title>
    <link href="http://www.yuthon.com/2017/03/10/TensorFlow-r1-0-on-TX1/"/>
    <id>http://www.yuthon.com/2017/03/10/TensorFlow-r1-0-on-TX1/</id>
    <published>2017-03-10T10:12:18.000Z</published>
    <updated>2017-03-17T08:27:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>TensorFlow r1.0已经发布了不少时间，事实证明1.0版本在内存使用上改善了不少，以前一些在r0.11上内存满报错的程序在r1.0上能够正常运行了。同时，r1.0相较于r0.11在API上做了很大的改动，也有很多新的东西（比如Keras）将要集成进TF。</p>
<p>总而言之，r1.0是未来的方向，所以说我希望将原先在TX1上装的r0.11换成r1.0。不过网上最新的教程还是只有r0.11的。<a href="https://github.com/rwightman" target="_blank" rel="external">rwightman</a>这位仁兄编译成功了r1.0alpha版本，并且放出了<a href="https://github.com/rwightman/tensorflow/releases/tag/v1.0.0-alpha-tegra-ugly_hack" target="_blank" rel="external">whl文件</a>，不过没有编译正式版。本文将阐述如何在TX1上安装TensorFlow r1.0的正式版本<del>，不过目前由于<code>nvcc</code>的一个bug，还没有编译成功</del>。</p>
<p>Update: 做了一些非常ugly的改动之后编译成功了。</p>
<a id="more"></a>
<h2 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h2><p>Thsi article aims to install TensorFlow r1.0 on NVIDIA Jetson TX1 with JetPack 2.3.1:</p>
<ul>
<li>Ubuntu 16.04 (aarch64)</li>
<li>CUDA 8.0</li>
<li>cuDNN 5.1.5 or 5.1.10</li>
</ul>
<blockquote>
<p><del>Note that I still <strong>CAN’T</strong> build TensorFlow r1.0 yet. The reason is explained at the end of this post.</del></p>
</blockquote>
<h2 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h2><p>Before installation, you need to add swap space for TX1 since this device only has 4G memory and 16GB eMMC Storage. I use an external HDD via USB. Maybe you could use SSD for higher speed. </p>
<p>Thanks to <a href="https://github.com/jetsonhacks" target="_blank" rel="external">jetsonhacks</a>, we can simply deal this with a script:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/jetsonhacks/postFlashTX1.git</div><div class="line">$ sudo ./createSwapfile.sh <span class="_">-d</span> /path/to/swap/ <span class="_">-s</span> 8</div></pre></td></tr></table></figure>
<p>8G swap is enough for compilation, and ensure you have <strong>&gt;5.5GB</strong> free space on TX1.</p>
<h2 id="Install-Deps"><a href="#Install-Deps" class="headerlink" title="Install Deps"></a>Install Deps</h2><p>Thanks to <a href="https://github.com/jetsonhacks" target="_blank" rel="external">jetsonhacks</a>, we can deal with deps more convinently. I forked this repo and modify something to fit TF r1.0. You can just clone mine.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/corenel/installTensorFlowTX1.git</div><div class="line">$ <span class="built_in">cd</span> installTensorFlowTX1</div><div class="line"><span class="comment"># tell the dynamic linker to use /usr/local/lib</span></div><div class="line">$ ./<span class="built_in">set</span>LocalLib.sh</div><div class="line"><span class="comment"># install prerequisties</span></div><div class="line">$ ./installPrerequisites.sh</div></pre></td></tr></table></figure>
<blockquote>
<p>If you meet an error that bazel can’t find <code>cudnn.h</code> in <code>/usr/lib/aarch64-linux-gnu/</code>, just download cuDNN from NVIDIA Developers website and place it into that path.</p>
<p>Or you can just edit <code>setTensorFlowEV.sh</code> and replace <code>default_cudnn_path=/usr/lib/aarch64-linux-gnu/</code> with <code>default_cudnn_path=/usr/</code> since the default <code>cudnn.h</code> is in <code>/usr/include/</code>.</p>
</blockquote>
<h2 id="Build-TensorFlow"><a href="#Build-TensorFlow" class="headerlink" title="Build TensorFlow"></a>Build TensorFlow</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># clone tensorflow r1.0</span></div><div class="line">$ ./<span class="built_in">clone</span>TensorFlow.sh</div><div class="line"><span class="comment"># set environment variables for tensorflow</span></div><div class="line">$ ./<span class="built_in">set</span>TensorFlowEV.sh</div><div class="line"><span class="comment"># build tensorflow</span></div><div class="line">$ ./buildTensorFlow.sh</div><div class="line"><span class="comment"># package builds into a wheel file</span></div><div class="line">$ ./packageTensorFlow.sh</div></pre></td></tr></table></figure>
<p>Then you’ll find your wheel file in your home folder.</p>
<h2 id="Install-and-Test"><a href="#Install-and-Test" class="headerlink" title="Install and Test"></a>Install and Test</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># install tensorflow</span></div><div class="line">$ pip install ~/tensorflow-1.0.0-py2-none-any.whl</div><div class="line"><span class="comment"># test tensorflow</span></div><div class="line">$ python -c <span class="string">'import tensorflow as tf; print(tf.__version__)'</span></div></pre></td></tr></table></figure>
<h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><p>Until now, I still can’t build tensorflow successfully. An error occured:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:2498:1: output <span class="string">'tensorflow/core/kernels/_objs/softmax_op_gpu/tensorflow/core/kernels/softmax_op_gpu.cu.pic.o'</span> was not created.</div><div class="line">ERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:2498:1: not all outputs were created or valid.</div><div class="line">Target //tensorflow/tools/pip_package:build_pip_package failed to build</div></pre></td></tr></table></figure>
<p>According to <a href="https://devtalk.nvidia.com/default/topic/987306/?comment=5059105" target="_blank" rel="external">this post</a>, this may due to a bug of <code>nvcc</code>. An expert in NVIDIA says they solved it with their internal nvcc compiler, which is not yet available in JetPack. Maybe next release of JetPack (3.0 on March 14) will solve it. So I’ll update this post then.</p>
<h2 id="An-ugly-hack"><a href="#An-ugly-hack" class="headerlink" title="An ugly hack"></a>An ugly hack</h2><p>Thanks to <a href="https://github.com/rwightman/tensorflow/commit/a1cde1d55f76a1d4eb806ba81d7c63fe72466e6d" target="_blank" rel="external">rwightman’s hack</a>,  I finally compiled TF1.0 successfully. Just following hacks:</p>
<ul>
<li>Revert Eigen to revision used in Tensorflow r0.11 to avoid cuda compile error</li>
<li>Remove expm1 op that was added with new additions to Eigen</li>
</ul>
<p>My fork for <code>installTensorFlowTX1</code> has contained this hack. And my build for TensorFlow r1.0 with Python 2.7 can be find <a href="https://www.dropbox.com/s/m6bgd3sq8kggul7/tensorflow-1.0.1-cp27-cp27mu-linux_aarch64.whl?dl=0" target="_blank" rel="external">here</a>.</p>
<blockquote>
<p><strong>Update</strong>: <a href="https://github.com/barty777" target="_blank" rel="external">@barty777</a> build TF 1.0.1 with Python 3.5, and his wheel file can be found <a href="https://drive.google.com/open?id=0B2jw9AHXtUJ_OFJDV19TWTEyaWc" target="_blank" rel="external">here</a>.</p>
</blockquote>
<h2 id="Acknowledgment"><a href="#Acknowledgment" class="headerlink" title="Acknowledgment"></a>Acknowledgment</h2><p>Thanks for following posts and issues:</p>
<ul>
<li>Github issue: <a href="https://github.com/tensorflow/tensorflow/issues/851" target="_blank" rel="external">tensorflow for Nvidia TX1</a></li>
<li>NVIDIA forum post: <a href="https://devtalk.nvidia.com/default/topic/901148/jetson-tx1/tensorflow-on-jetson-tx1/" target="_blank" rel="external">TensorFlow on Jetson TX1</a></li>
<li>My earilier blog post: <a href="http://www.yuthon.com/2016/12/04/Installation-of-TensorFlow-r0-11-on-TX1/">Installation of TensorFlow r0.11 on TX1</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TensorFlow r1.0已经发布了不少时间，事实证明1.0版本在内存使用上改善了不少，以前一些在r0.11上内存满报错的程序在r1.0上能够正常运行了。同时，r1.0相较于r0.11在API上做了很大的改动，也有很多新的东西（比如Keras）将要集成进TF。&lt;/p&gt;
&lt;p&gt;总而言之，r1.0是未来的方向，所以说我希望将原先在TX1上装的r0.11换成r1.0。不过网上最新的教程还是只有r0.11的。&lt;a href=&quot;https://github.com/rwightman&quot;&gt;rwightman&lt;/a&gt;这位仁兄编译成功了r1.0alpha版本，并且放出了&lt;a href=&quot;https://github.com/rwightman/tensorflow/releases/tag/v1.0.0-alpha-tegra-ugly_hack&quot;&gt;whl文件&lt;/a&gt;，不过没有编译正式版。本文将阐述如何在TX1上安装TensorFlow r1.0的正式版本&lt;del&gt;，不过目前由于&lt;code&gt;nvcc&lt;/code&gt;的一个bug，还没有编译成功&lt;/del&gt;。&lt;/p&gt;
&lt;p&gt;Update: 做了一些非常ugly的改动之后编译成功了。&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="NVIDIA" scheme="http://www.yuthon.com/tags/NVIDIA/"/>
    
      <category term="Jetson TX1" scheme="http://www.yuthon.com/tags/Jetson-TX1/"/>
    
      <category term="Ubuntu" scheme="http://www.yuthon.com/tags/Ubuntu/"/>
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://www.yuthon.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Backup system partition on TX1</title>
    <link href="http://www.yuthon.com/2016/12/18/Backup-system-partition-on-TX1/"/>
    <id>http://www.yuthon.com/2016/12/18/Backup-system-partition-on-TX1/</id>
    <published>2016-12-18T10:18:59.000Z</published>
    <updated>2016-12-18T10:57:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>由于实验室只有要用到多块 TX1 开发板, 然而一个个都用 JetPack 刷机, 再用自动化脚本装软件和依赖实在是太麻烦了, 因此我和梅老板就开始研究怎么直接备份 TX1 上的 Ubuntu 系统.</p>
<a id="more"></a>
<h2 id="Failed-attempts"><a href="#Failed-attempts" class="headerlink" title="Failed attempts"></a>Failed attempts</h2><p>最开始想的是直接用<code>dd</code>来备份整块 eMMC到外置的存储上, 于是尝试了</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo dd <span class="keyword">if</span>=/dev/mmcblk0p1 of=/media/ubuntu/backup/backup.img</div></pre></td></tr></table></figure>
<p>后来还发现可以用<code>ssh</code>来远程<code>dd</code></p>
<ul>
<li><strong>run from remote computer</strong>:</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ dd <span class="keyword">if</span>=/dev/mmcblk0p1 | gzip -1 - | ssh yuthon@mac dd of=image.gz</div></pre></td></tr></table></figure>
<ul>
<li><strong>run from local computer</strong>:</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ssh ubuntu@tx1 <span class="string">"dd if=/dev/mmcblk0p1 | gzip -1 -"</span> | dd of=image.gz</div></pre></td></tr></table></figure>
<p>之后, 我们发现<code>64_TX1/Linux_for_Tegra_64_tx1/rootfs</code>目录中应该就是之后需要拷到 TX1 的<code>/</code>目录下的内容. 因此我们直接将之前备份好的<code>bakcup.img</code>解压到了此目录下, 并使用 JetPack 重新 Flash OS.</p>
<p>最后的结果是 TX1 在重启后卡在了登录界面, 经典的 login-loop.</p>
<p>此方案, 扑街.</p>
<h2 id="Using-tegraflash-py"><a href="#Using-tegraflash-py" class="headerlink" title="Using tegraflash.py"></a>Using <code>tegraflash.py</code></h2><p>Then we found a <a href="https://devtalk.nvidia.com/default/topic/898999/tx1-r23-1-new-flash-structure-how-to-clone-/" target="_blank" rel="external">post</a> on NVIDIA Developer Forums, and method in this post works for us.</p>
<blockquote>
<p>Assumed we’re in <code>64_TX1/Linux_for_Tegra_64_tx1/bootloader</code> directory.</p>
</blockquote>
<h3 id="Backup-an-image"><a href="#Backup-an-image" class="headerlink" title="Backup an image"></a>Backup an image</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo ./tegraflash.py --bl cboot.bin --applet nvtboot_recovery.bin --chip 0x21 --cmd <span class="string">"read APP my_backup_image_APP.img"</span></div></pre></td></tr></table></figure>
<h3 id="Restore-an-image"><a href="#Restore-an-image" class="headerlink" title="Restore an image"></a>Restore an image</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo ./tegraflash.py --bl cboot.bin --applet nvtboot_recovery.bin --chip 0x21 --cmd <span class="string">"write APP my_backup_image_APP.img"</span></div></pre></td></tr></table></figure>
<h3 id="One-more-thing"><a href="#One-more-thing" class="headerlink" title="One more thing"></a>One more thing</h3><p>It’s recommended in the post to use <code>flash.py</code> front-end instead of <code>tegraflash.py</code> to make sure you use the same L4T release version.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Backup</span></div><div class="line">$ sudo flash.sh -S 14580MiB jetson-tx1 mmcblk0p1</div><div class="line"><span class="comment"># Restore</span></div><div class="line">$ sudo flash.sh -r -S 14580MiB jetson-tx1 mmcblk0p1</div></pre></td></tr></table></figure>
<p>Note that the <code>-r</code>  param re-uses <code>system.img</code> in <code>bootloader</code> directory, and if a clone file is there in place, that installs the clone.<br><strong>I haven’t tried this method, maybe you could have a try and report.</strong></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于实验室只有要用到多块 TX1 开发板, 然而一个个都用 JetPack 刷机, 再用自动化脚本装软件和依赖实在是太麻烦了, 因此我和梅老板就开始研究怎么直接备份 TX1 上的 Ubuntu 系统.&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Tensorflow" scheme="http://www.yuthon.com/tags/Tensorflow/"/>
    
      <category term="NVIDIA" scheme="http://www.yuthon.com/tags/NVIDIA/"/>
    
      <category term="Jetson TX1" scheme="http://www.yuthon.com/tags/Jetson-TX1/"/>
    
      <category term="Ubuntu" scheme="http://www.yuthon.com/tags/Ubuntu/"/>
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Installation of TensorFlow r0.11 on TX1</title>
    <link href="http://www.yuthon.com/2016/12/04/Installation-of-TensorFlow-r0-11-on-TX1/"/>
    <id>http://www.yuthon.com/2016/12/04/Installation-of-TensorFlow-r0-11-on-TX1/</id>
    <published>2016-12-04T12:53:55.000Z</published>
    <updated>2016-12-14T14:09:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天折腾了一个下午, 特此记录一下其中遇到的坑, 主要还是因为 TX1 的 aarch64 架构, 以及小得可怜的内存与存储容量.</p>
<a id="more"></a>
<h2 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h2><ul>
<li><strong>Hardware</strong>: NVIDIA Jetson TX1 Developer Kit</li>
<li><strong>Software</strong>: JetPack 2.3.1<ul>
<li>Ubuntu 16.04 64-bit (aarch64)</li>
<li>CUDA 8.0</li>
<li>cuDNN 5.1</li>
</ul>
</li>
</ul>
<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>建议全程开HTTP/HTTPS代理, 否则国内下载速度堪忧.</p>
<h3 id="Install-Java"><a href="#Install-Java" class="headerlink" title="Install Java"></a>Install Java</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo add-apt-repository ppa:webupd8team/java</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install oracle-java8-installer</div></pre></td></tr></table></figure>
<h3 id="Install-dependencens"><a href="#Install-dependencens" class="headerlink" title="Install dependencens"></a>Install dependencens</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install git zip unzip autoconf automake libtool curl zlib1g-dev maven</div><div class="line">$ sudo apt-get install python-numpy swig python-dev python-wheel</div></pre></td></tr></table></figure>
<h3 id="Build-protobuf"><a href="#Build-protobuf" class="headerlink" title="Build protobuf"></a>Build protobuf</h3><p>这里测 Protobuf 要编译两份, 分别给 grpc 和 Bazel 用.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># For grpc-java build</span></div><div class="line">$ git <span class="built_in">clone</span> https://github.com/google/protobuf.git</div><div class="line">$ <span class="built_in">cd</span> protobuf</div><div class="line">$ git checkout master</div><div class="line">$ ./autogen.sh</div><div class="line">$ git checkout v3.0.0-beta-3</div><div class="line">$ ./autogen.sh</div><div class="line">$ LDFLAGS=-static ./configure --prefix=$(<span class="built_in">pwd</span>)/../</div><div class="line">$ sed -i <span class="_">-e</span> <span class="string">'s/LDFLAGS = -static/LDFLAGS = -all-static/'</span> ./src/Makefile</div><div class="line">$ make -j 4</div><div class="line">$ make install</div><div class="line"></div><div class="line"><span class="comment"># For bazel build</span></div><div class="line">$ git checkout v3.0.0-beta-2</div><div class="line">$./autogen.sh</div><div class="line">$ LDFLAGS=-static ./configure --prefix=$(<span class="built_in">pwd</span>)/../</div><div class="line">$ sed -i <span class="_">-e</span> <span class="string">'s/LDFLAGS = -static/LDFLAGS = -all-static/'</span> ./src/Makefile</div><div class="line">$ make -j 4</div><div class="line">$ <span class="built_in">cd</span> ..</div></pre></td></tr></table></figure>
<blockquote>
<p>注意: 给 Bazel 用的不用<code>make install</code>, 之后直接<code>cp</code>过去.</p>
</blockquote>
<h3 id="Build-grpc-java-compiler"><a href="#Build-grpc-java-compiler" class="headerlink" title="Build grpc-java compiler"></a>Build grpc-java compiler</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/neo-titans/odroid.git</div><div class="line">$ git <span class="built_in">clone</span> https://github.com/grpc/grpc-java.git</div><div class="line">$ <span class="built_in">cd</span> grpc-java/</div><div class="line">$ git checkout v0.15.0</div><div class="line">$ patch -p0 &lt; ../odroid/build_tensorflow/grpc-java.v0.15.0.patch</div><div class="line">$ CXXFLAGS=<span class="string">"-I<span class="variable">$(pwd)</span>/../include"</span> LDFLAGS=<span class="string">"-L<span class="variable">$(pwd)</span>/../lib"</span> ./gradlew java_pluginExecutable -Pprotoc=$(<span class="built_in">pwd</span>)/../bin/protoc</div><div class="line">$ <span class="built_in">cd</span> ..</div></pre></td></tr></table></figure>
<h3 id="Build-bazel"><a href="#Build-bazel" class="headerlink" title="Build bazel"></a>Build bazel</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/bazelbuild/bazel.git</div><div class="line">$ <span class="built_in">cd</span> bazel</div><div class="line">$ git checkout 0.3.2</div><div class="line">$ cp ../protobuf/src/protoc third_party/protobuf/protoc-linux-arm32.exe</div><div class="line">$ cp ../grpc-java/compiler/build/exe/java_plugin/protoc-gen-grpc-java third_party/grpc/protoc-gen-grpc-java-0.15.0-linux-arm32.exe</div></pre></td></tr></table></figure>
<p>在编译 Bazel 之前, 还需要改一些配置, 使得 Bazel 将 aarch64 认作 arm64, 以便编译成功.</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line">diff --git a/compile.sh b/compile.sh</div><div class="line">index 53fc412..11035d9 100755</div><div class="line"><span class="comment">--- a/compile.sh</span></div><div class="line"><span class="comment">+++ b/compile.sh</span></div><div class="line">@@ -27,7 +27,7 @@ cd "$(dirname "$0")"</div><div class="line"> # Set the default verbose mode in buildenv.sh so that we do not display command</div><div class="line"> # output unless there is a failure.  We do this conditionally to offer the user</div><div class="line"> # a chance of overriding this in case they want to do so.</div><div class="line"><span class="deletion">-: $&#123;VERBOSE:=no&#125;</span></div><div class="line"><span class="addition">+: $&#123;VERBOSE:=yes&#125;</span></div><div class="line"></div><div class="line"> source scripts/bootstrap/buildenv.sh</div><div class="line"></div><div class="line">diff --git a/scripts/bootstrap/compile.sh b/scripts/bootstrap/compile.sh</div><div class="line">index 77372f0..657b254 100755</div><div class="line"><span class="comment">--- a/scripts/bootstrap/compile.sh</span></div><div class="line"><span class="comment">+++ b/scripts/bootstrap/compile.sh</span></div><div class="line">@@ -48,6 +48,7 @@ linux)</div><div class="line">   else</div><div class="line">     if [ "$&#123;MACHINE_IS_ARM&#125;" = 'yes' ]; then</div><div class="line">       PROTOC=$&#123;PROTOC:-third_party/protobuf/protoc-linux-arm32.exe&#125;</div><div class="line"><span class="addition">+      GRPC_JAVA_PLUGIN=$&#123;GRPC_JAVA_PLUGIN:-third_party/grpc/protoc-gen-grpc-java-0.15.0-linux-arm32.exe&#125;</span></div><div class="line">     else</div><div class="line">       PROTOC=$&#123;PROTOC:-third_party/protobuf/protoc-linux-x86_32.exe&#125;</div><div class="line">       GRPC_JAVA_PLUGIN=$&#123;GRPC_JAVA_PLUGIN:-third_party/grpc/protoc-gen-grpc-java-0.15.0-linux-x86_32.exe&#125;</div><div class="line">@@ -150,7 +151,7 @@ function java_compilation() &#123;</div><div class="line"></div><div class="line">   run "$&#123;JAVAC&#125;" -classpath "$&#123;classpath&#125;" -sourcepath "$&#123;sourcepath&#125;" \</div><div class="line">       -d "$&#123;output&#125;/classes" -source "$JAVA_VERSION" -target "$JAVA_VERSION" \</div><div class="line"><span class="deletion">-      -encoding UTF-8 "@$&#123;paramfile&#125;"</span></div><div class="line"><span class="addition">+      -encoding UTF-8 "@$&#123;paramfile&#125;" -J-Xmx500M</span></div><div class="line"></div><div class="line">   log "Extracting helper classes for $name..."</div><div class="line">   for f in $&#123;library_jars&#125; ; do</div><div class="line">diff --git a/src/main/java/com/google/devtools/build/lib/util/CPU.java b/src/main/java/com/google/devtools/build/lib/util/CPU.java</div><div class="line">index 41af4b1..4d80610 100644</div><div class="line"><span class="comment">--- a/src/main/java/com/google/devtools/build/lib/util/CPU.java</span></div><div class="line"><span class="comment">+++ b/src/main/java/com/google/devtools/build/lib/util/CPU.java</span></div><div class="line">@@ -26,7 +26,7 @@ public enum CPU &#123;</div><div class="line">   X86_32("x86_32", ImmutableSet.of("i386", "i486", "i586", "i686", "i786", "x86")),</div><div class="line">   X86_64("x86_64", ImmutableSet.of("amd64", "x86_64", "x64")),</div><div class="line">   PPC("ppc", ImmutableSet.of("ppc", "ppc64", "ppc64le")),</div><div class="line"><span class="deletion">-  ARM("arm", ImmutableSet.of("arm", "armv7l")),</span></div><div class="line"><span class="addition">+  ARM("arm", ImmutableSet.of("arm", "armv7l", "aarch64")),</span></div><div class="line">   UNKNOWN("unknown", ImmutableSet.&lt;String&gt;of());</div><div class="line"></div><div class="line">   private final String canonicalName;</div><div class="line">diff --git a/third_party/grpc/BUILD b/third_party/grpc/BUILD</div><div class="line">index 2ba07e3..c7925ff 100644</div><div class="line"><span class="comment">--- a/third_party/grpc/BUILD</span></div><div class="line"><span class="comment">+++ b/third_party/grpc/BUILD</span></div><div class="line">@@ -29,7 +29,7 @@ filegroup(</div><div class="line">         "//third_party:darwin": ["protoc-gen-grpc-java-0.15.0-osx-x86_64.exe"],</div><div class="line">         "//third_party:k8": ["protoc-gen-grpc-java-0.15.0-linux-x86_64.exe"],</div><div class="line">         "//third_party:piii": ["protoc-gen-grpc-java-0.15.0-linux-x86_32.exe"],</div><div class="line"><span class="deletion">-        "//third_party:arm": ["protoc-gen-grpc-java-0.15.0-linux-x86_32.exe"],</span></div><div class="line"><span class="addition">+        "//third_party:arm": ["protoc-gen-grpc-java-0.15.0-linux-arm32.exe"],</span></div><div class="line">         "//third_party:freebsd": ["protoc-gen-grpc-java-0.15.0-linux-x86_32.exe"],</div><div class="line">     &#125;),</div><div class="line"> )</div><div class="line">diff --git a/third_party/protobuf/BUILD b/third_party/protobuf/BUILD</div><div class="line">index 203fe51..4c2a316 100644</div><div class="line"><span class="comment">--- a/third_party/protobuf/BUILD</span></div><div class="line"><span class="comment">+++ b/third_party/protobuf/BUILD</span></div><div class="line">@@ -28,6 +28,7 @@ filegroup(</div><div class="line">         "//third_party:darwin": ["protoc-osx-x86_32.exe"],</div><div class="line">         "//third_party:k8": ["protoc-linux-x86_64.exe"],</div><div class="line">         "//third_party:piii": ["protoc-linux-x86_32.exe"],</div><div class="line"><span class="addition">+        "//third_party:arm": ["protoc-linux-arm32.exe"],</span></div><div class="line">         "//third_party:freebsd": ["protoc-linux-x86_32.exe"],</div><div class="line">     &#125;),</div><div class="line"> )</div><div class="line">diff --git a/tools/cpp/cc_configure.bzl b/tools/cpp/cc_configure.bzl</div><div class="line">index aeb0715..688835d 100644</div><div class="line"><span class="comment">--- a/tools/cpp/cc_configure.bzl</span></div><div class="line"><span class="comment">+++ b/tools/cpp/cc_configure.bzl</span></div><div class="line">@@ -150,7 +150,12 @@ def _get_cpu_value(repository_ctx):</div><div class="line">     return "x64_windows"</div><div class="line">   # Use uname to figure out whether we are on x86_32 or x86_64</div><div class="line">   result = repository_ctx.execute(["uname", "-m"])</div><div class="line"><span class="deletion">-  return "k8" if result.stdout.strip() in ["amd64", "x86_64", "x64"] else "piii"</span></div><div class="line"><span class="addition">+  machine = result.stdout.strip()</span></div><div class="line"><span class="addition">+  if machine in ["arm", "armv7l", "aarch64"]:</span></div><div class="line"><span class="addition">+   return "arm"</span></div><div class="line"><span class="addition">+  elif machine in ["amd64", "x86_64", "x64"]:</span></div><div class="line"><span class="addition">+   return "k8"</span></div><div class="line"><span class="addition">+  return "piii"</span></div><div class="line"></div><div class="line"></div><div class="line"> _INC_DIR_MARKER_BEGIN = "#include &lt;...&gt;"</div></pre></td></tr></table></figure>
<p>之后编译安装:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ ./compile.sh </div><div class="line">$ sudo cp output/bazel /usr/<span class="built_in">local</span>/bin</div><div class="line">$ <span class="built_in">cd</span> ..</div></pre></td></tr></table></figure>
<h3 id="Build-Tensorflow"><a href="#Build-Tensorflow" class="headerlink" title="Build Tensorflow"></a>Build Tensorflow</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ git clone https://github.com/tensorflow/tensorflow.git</div><div class="line">$ git checkout r0.11</div></pre></td></tr></table></figure>
<p>同样地, 修改配置文件:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div></pre></td><td class="code"><pre><div class="line">diff --git a/tensorflow/core/kernels/BUILD b/tensorflow/core/kernels/BUILD</div><div class="line">index 2e04827..867aaca 100644</div><div class="line"><span class="comment">--- a/tensorflow/core/kernels/BUILD</span></div><div class="line"><span class="comment">+++ b/tensorflow/core/kernels/BUILD</span></div><div class="line">@@ -1184,7 +1184,7 @@ tf_kernel_libraries(</div><div class="line">         "segment_reduction_ops",</div><div class="line">         "scan_ops",</div><div class="line">         "sequence_ops",</div><div class="line"><span class="deletion">-        "sparse_matmul_op",</span></div><div class="line"><span class="addition">+       #DC "sparse_matmul_op",</span></div><div class="line">     ],</div><div class="line">     deps = [</div><div class="line">         ":bounds_check",</div><div class="line">diff --git a/tensorflow/core/kernels/cwise_op_gpu_select.cu.cc b/tensorflow/core/kernels/cwise_op_gpu_select.cu.cc</div><div class="line">index 02058a8..880252c 100644</div><div class="line"><span class="comment">--- a/tensorflow/core/kernels/cwise_op_gpu_select.cu.cc</span></div><div class="line"><span class="comment">+++ b/tensorflow/core/kernels/cwise_op_gpu_select.cu.cc</span></div><div class="line">@@ -43,8 +43,14 @@ struct BatchSelectFunctor&lt;GPUDevice, T&gt; &#123;</div><div class="line">     const int all_but_batch = then_flat_outer_dims.dimension(1);</div><div class="line"></div><div class="line"> #if !defined(EIGEN_HAS_INDEX_LIST)</div><div class="line"><span class="deletion">-    Eigen::array&lt;int, 2&gt; broadcast_dims&#123;&#123; 1, all_but_batch &#125;&#125;;</span></div><div class="line"><span class="deletion">-    Eigen::Tensor&lt;int, 2&gt;::Dimensions reshape_dims&#123;&#123; batch, 1 &#125;&#125;;</span></div><div class="line"><span class="addition">+    // Eigen::array&lt;int, 2&gt; broadcast_dims&#123;&#123; 1, all_but_batch &#125;&#125;;</span></div><div class="line"><span class="addition">+    Eigen::array&lt;int, 2&gt; broadcast_dims;</span></div><div class="line"><span class="addition">+    broadcast_dims[0] = 1;</span></div><div class="line"><span class="addition">+    broadcast_dims[1] = all_but_batch;</span></div><div class="line"><span class="addition">+    // Eigen::Tensor&lt;int, 2&gt;::Dimensions reshape_dims&#123;&#123; batch, 1 &#125;&#125;;</span></div><div class="line"><span class="addition">+    Eigen::Tensor&lt;int, 2&gt;::Dimensions reshape_dims;</span></div><div class="line"><span class="addition">+    reshape_dims[0] = batch;</span></div><div class="line"><span class="addition">+    reshape_dims[1] = 1;</span></div><div class="line"> #else</div><div class="line">     Eigen::IndexList&lt;Eigen::type2index&lt;1&gt;, int&gt; broadcast_dims;</div><div class="line">     broadcast_dims.set(1, all_but_batch);</div><div class="line">diff --git a/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc b/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc</div><div class="line">index a177696..75b67ba 100644</div><div class="line"><span class="comment">--- a/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc</span></div><div class="line"><span class="comment">+++ b/tensorflow/core/kernels/sparse_tensor_dense_matmul_op_gpu.cu.cc</span></div><div class="line">@@ -104,9 +104,17 @@ struct SparseTensorDenseMatMulFunctor&lt;GPUDevice, T, ADJ_A, ADJ_B&gt; &#123;</div><div class="line">     int n = (ADJ_B) ? b.dimension(0) : b.dimension(1);</div><div class="line"></div><div class="line"> #if !defined(EIGEN_HAS_INDEX_LIST)</div><div class="line"><span class="deletion">-    Eigen::Tensor&lt;int, 2&gt;::Dimensions matrix_1_by_nnz&#123;&#123; 1, nnz &#125;&#125;;</span></div><div class="line"><span class="deletion">-    Eigen::array&lt;int, 2&gt; n_by_1&#123;&#123; n, 1 &#125;&#125;;</span></div><div class="line"><span class="deletion">-    Eigen::array&lt;int, 1&gt; reduce_on_rows&#123;&#123; 0 &#125;&#125;;</span></div><div class="line"><span class="addition">+    // Eigen::Tensor&lt;int, 2&gt;::Dimensions matrix_1_by_nnz&#123;&#123; 1, nnz &#125;&#125;;</span></div><div class="line"><span class="addition">+    Eigen::Tensor&lt;int, 2&gt;::Dimensions matrix_1_by_nnz;</span></div><div class="line"><span class="addition">+    matrix_1_by_nnz[0] = 1;</span></div><div class="line"><span class="addition">+    matrix_1_by_nnz[1] = nnz;</span></div><div class="line"><span class="addition">+    // Eigen::array&lt;int, 2&gt; n_by_1&#123;&#123; n, 1 &#125;&#125;;</span></div><div class="line"><span class="addition">+    Eigen::Tensor&lt;int, 2&gt;::Dimensions matrix_1_by_nnz;</span></div><div class="line"><span class="addition">+    matrix_1_by_nnz[0] = 1;</span></div><div class="line"><span class="addition">+    matrix_1_by_nnz[1] = nnz;</span></div><div class="line"><span class="addition">+    // Eigen::array&lt;int, 2&gt; n_by_1&#123;&#123; n, 1 &#125;&#125;;</span></div><div class="line"><span class="addition">+    Eigen::array&lt;int, 2&gt; n_by_1;</span></div><div class="line"><span class="addition">+    n_by_1[0] = n;</span></div><div class="line"><span class="addition">+    n_by_1[1] = 1;</span></div><div class="line"><span class="addition">+    // Eigen::array&lt;int, 1&gt; reduce_on_rows&#123;&#123; 0 &#125;&#125;;</span></div><div class="line"><span class="addition">+    Eigen::array&lt;int, 1&gt; reduce_on_rows;</span></div><div class="line"><span class="addition">+    reduce_on_rows[0]= 0;</span></div><div class="line"> #else</div><div class="line">     Eigen::IndexList&lt;Eigen::type2index&lt;1&gt;, int&gt; matrix_1_by_nnz;</div><div class="line">     matrix_1_by_nnz.set(1, nnz);</div><div class="line">diff --git a/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc b/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc</div><div class="line">index 52256a7..1d027b9 100644</div><div class="line"><span class="comment">--- a/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc</span></div><div class="line"><span class="comment">+++ b/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc</span></div><div class="line">@@ -888,6 +888,9 @@ CudaContext* CUDAExecutor::cuda_context() &#123; return context_; &#125;</div><div class="line"> // For anything more complicated/prod-focused than this, you'll likely want to</div><div class="line"> // turn to gsys' topology modeling.</div><div class="line"> static int TryToReadNumaNode(const string &amp;pci_bus_id, int device_ordinal) &#123;</div><div class="line"><span class="addition">+// DC - make this clever later. ARM has no NUMA node, just return 0</span></div><div class="line"><span class="addition">+LOG(INFO) &lt;&lt; "ARM has no NUMA node, hardcoding to return zero";</span></div><div class="line"><span class="addition">+return 0;</span></div><div class="line"> #if defined(__APPLE__)</div><div class="line">   LOG(INFO) &lt;&lt; "OS X does not support NUMA - returning NUMA node zero";</div><div class="line">   return 0;</div></pre></td></tr></table></figure>
<p>之后即可编译:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ ./configure</div><div class="line">$ bazel build -c opt --jobs 2 --local_resources 1024,4.0,1.0 --config=cuda //tensorflow/tools/pip_package:build_pip_package</div><div class="line">$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg</div><div class="line"><span class="comment"># The name of the .whl file will depend on your platform.</span></div><div class="line">$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.11.0-py2-none-any.whl</div></pre></td></tr></table></figure>
<p>这里有我自己编译好的 <a href="https://drive.google.com/open?id=0B0AsKkiz_kZRZG9BbFRxZ1FYWTg" target="_blank" rel="external">tensorflow_gpu-0.11.0-py2-none-aarch64.whl</a>, 可供使用.</p>
<h3 id="Start-using-Tensorflow"><a href="#Start-using-Tensorflow" class="headerlink" title="Start using Tensorflow"></a>Start using Tensorflow</h3><p>首先还得装一下 OpenCV 的 Python port:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install -y libopencv4tegra–python</div></pre></td></tr></table></figure>
<p>之后即可测试 TensorFlow 是否成功安装:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ python</div><div class="line">Python 2.7.12 (default, Nov 19 2016, 06:48:10)</div><div class="line">[GCC 5.4.0 20160609] on linux2</div><div class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> or <span class="string">"license"</span> <span class="keyword">for</span> more information.</div><div class="line">&gt;&gt;&gt; import tensorflow as tf</div><div class="line">I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally</div><div class="line">I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally</div><div class="line">I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally</div><div class="line">I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally</div><div class="line">I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure>
<p>不报错即是安装成功.</p>
<h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><h3 id="Swap-Memory"><a href="#Swap-Memory" class="headerlink" title="Swap Memory"></a>Swap Memory</h3><p>如果编译失败, 很有可能是内存不足的原因, 因此可以外接U盘或是SSD等, 并且将一部分缓存放在上面.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> /path/to/your/storage</div><div class="line">$ fallocate <span class="_">-l</span> 8G swapfile</div><div class="line">$ chmod 600 swapfile</div><div class="line">$ mkswap swapfile</div><div class="line">$ sudo swapon swapfile</div><div class="line">$ swapon <span class="_">-s</span></div></pre></td></tr></table></figure>
<p>8G的 swap 空间应该是够用了, 如果还嫌不够可以再设个大点的.</p>
<p>之后再运行 Bazel 编译:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bazel build -c opt --local_resources 3072,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package</div></pre></td></tr></table></figure>
<h3 id="Build-on-external-storage"><a href="#Build-on-external-storage" class="headerlink" title="Build on external storage"></a>Build on external storage</h3><p>整个安装过程所需的空间大概是3G以上, 而 TX1 装完系统之后只剩下了 4G 的剩余空间. 所以最好将安装时的根目录选在外置的存储上, 以免因为内置存储空间不足而导致失败.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="http://stackoverflow.com/questions/39783919/tensorflow-on-nvidia-tx1/" target="_blank" rel="external">http://stackoverflow.com/questions/39783919/tensorflow-on-nvidia-tx1/</a></li>
<li><a href="https://github.com/tensorflow/tensorflow/issues/851" target="_blank" rel="external">https://github.com/tensorflow/tensorflow/issues/851</a></li>
<li><a href="https://www.neotitans.net/install-tensorflow-on-odroid-c2.html" target="_blank" rel="external">https://www.neotitans.net/install-tensorflow-on-odroid-c2.html</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天折腾了一个下午, 特此记录一下其中遇到的坑, 主要还是因为 TX1 的 aarch64 架构, 以及小得可怜的内存与存储容量.&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Tensorflow" scheme="http://www.yuthon.com/tags/Tensorflow/"/>
    
      <category term="NVIDIA" scheme="http://www.yuthon.com/tags/NVIDIA/"/>
    
      <category term="Jetson TX1" scheme="http://www.yuthon.com/tags/Jetson-TX1/"/>
    
      <category term="Ubuntu" scheme="http://www.yuthon.com/tags/Ubuntu/"/>
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Train YOLOv2 on my own dataset</title>
    <link href="http://www.yuthon.com/2016/12/03/Train-YOLOv2-on-my-own-dataset/"/>
    <id>http://www.yuthon.com/2016/12/03/Train-YOLOv2-on-my-own-dataset/</id>
    <published>2016-12-03T03:29:04.000Z</published>
    <updated>2016-12-03T10:15:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看 Darkflow 的时候, 发现连 YOLOv2 都出了, 据称 mAP 和速度都提升了不少, 立马 clone 下来试了一番.</p>
<a id="more"></a>
<h2 id="Instruction"><a href="#Instruction" class="headerlink" title="Instruction"></a>Instruction</h2><h3 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h3><p>下面是<a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="external">官网</a>挂出来的一个对比表, 可以看出, YOLOv2 有 76.8 的mAP, 和 SSD500 相同, 但是 FPS 不知比 SSD 高到哪里去了. YOLOv2 544x544 提升到了 78.6 mAP, 比 Faster-RCNN 的 Baseline (ResNet-101, VOC07+12) 的 76.4 mAP 高, 但是比其 Baseline+++ (ResNet-101, COCO+VOC2007+VOC2012) 的 85.6 mAP 还是逊色了不少. 不过官网没有挂出来 training on Pascal + COCO data and testing on Pascal data 的数据, 想见应该也会在 80 mAP 以上.</p>
<p>更加可喜的是, Tiny-YOLOv2比之前的Tiny-YOLO高出了52FPS, 达到了惊人的207FPS, 想必在 TX1 上跑的话应该也能上20FPS.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Train</th>
<th>Test</th>
<th>mAP</th>
<th>FLOPS</th>
<th>FPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Old YOLO</td>
<td>VOC 2007+2012</td>
<td>2007</td>
<td>63.4</td>
<td>40.19 Bn</td>
<td>45</td>
</tr>
<tr>
<td>SSD300</td>
<td>VOC 2007+2012</td>
<td>2007</td>
<td>74.3</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>SSD500</td>
<td>VOC 2007+2012</td>
<td>2007</td>
<td>76.8</td>
<td>-</td>
<td>19</td>
</tr>
<tr>
<td>YOLOv2</td>
<td>VOC 2007+2012</td>
<td>2007</td>
<td>76.8</td>
<td>34.90 Bn</td>
<td>67</td>
</tr>
<tr>
<td>YOLOv2 544x544</td>
<td>VOC 2007+2012</td>
<td>2007</td>
<td>78.6</td>
<td>59.68 Bn</td>
<td>40</td>
</tr>
<tr>
<td>Tiny YOLO</td>
<td>VOC 2007+2012</td>
<td>2007</td>
<td>57.1</td>
<td>6.97 Bn</td>
<td>207</td>
</tr>
<tr>
<td>SSD300</td>
<td>COCO trainval</td>
<td>test-dev</td>
<td>41.2</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>SSD500</td>
<td>COCO trainval</td>
<td>test-dev</td>
<td>46.5</td>
<td>-</td>
<td>19</td>
</tr>
<tr>
<td>YOLOv2 544x544</td>
<td>COCO trainval</td>
<td>test-dev</td>
<td>44.0</td>
<td>59.68 Bn</td>
<td>40</td>
</tr>
</tbody>
</table>
<h3 id="What’s-New-in-Version-2"><a href="#What’s-New-in-Version-2" class="headerlink" title="What’s New in Version 2"></a>What’s New in Version 2</h3><p>具体的文章还没在 Arxiv 上挂出来, 按照目前透露的信息, 主要是像 SSD 和 Overfeat 那样全部都用了卷积层, 而不是后面还跟着全连接层. 但是不同的是, 仍然是对整个图像进行训练. 同时还借鉴了 Faster-RCNN, 调整了 Bounding Box 的优先级, 不直接预测<code>w</code>, <code>h</code>, 但是仍然是直接预测<code>x</code>, <code>y</code>坐标.</p>
<h3 id="Video"><a href="#Video" class="headerlink" title="Video"></a>Video</h3><p>官网还挂了一个用 YOLOv2 识别过的 007 的 Trailer, 配乐+Bounding Box 使得这个视频莫名其妙地非常搞笑, 建议去<a href="https://youtu.be/VOC3huqHrss" target="_blank" rel="external">看看</a>.</p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>下面讲讲使用 YOLOv2 在我自己做的数据集 ROBdevkit 上的训练过程.</p>
<h3 id="Make"><a href="#Make" class="headerlink" title="Make"></a>Make</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/pjreddie/darknet</div><div class="line">$ <span class="built_in">cd</span> darknet</div><div class="line">$ make -j8</div></pre></td></tr></table></figure>
<p>在<code>make</code>之前, 为了最大发挥机器的性能, 还需要修改<code>Makefile</code>:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">diff --git a/Makefile b/Makefile</div><div class="line">index 3d3d5e4..dd7a33d 100644</div><div class="line"><span class="comment">--- a/Makefile</span></div><div class="line"><span class="comment">+++ b/Makefile</span></div><div class="line"><span class="meta">@@ -1,6 +1,6 @@</span></div><div class="line"><span class="deletion">-GPU=0</span></div><div class="line"><span class="deletion">-CUDNN=0</span></div><div class="line"><span class="deletion">-OPENCV=0</span></div><div class="line"><span class="addition">+GPU=1</span></div><div class="line"><span class="addition">+CUDNN=1</span></div><div class="line"><span class="addition">+OPENCV=1</span></div><div class="line"> DEBUG=0</div><div class="line"></div><div class="line"> ARCH= -gencode arch=compute_20,code=[sm_20,sm_21] \</div><div class="line">@@ -10,47 +10,47 @@ ARCH= -gencode arch=compute_20,code=[sm_20,sm_21] \</div><div class="line">       -gencode arch=compute_52,code=[sm_52,compute_52]</div><div class="line"></div><div class="line"> # This is what I use, uncomment if you know your arch and want to specify</div><div class="line"><span class="deletion">-# ARCH=  -gencode arch=compute_52,code=compute_52</span></div><div class="line"><span class="addition">+ARCH=  -gencode arch=compute_61,code=compute_61</span></div></pre></td></tr></table></figure>
<h3 id="Prepare"><a href="#Prepare" class="headerlink" title="Prepare"></a>Prepare</h3><p>YOLOv2这次不用改<code>yolo.c</code>源文件了, 只需要修改一些配置文件即可, 大大方便了我们用自己的数据集训练.</p>
<p>首先修改<code>data/voc.names</code>, 另存为<code>data/rob.names</code>:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ball</div><div class="line">goal</div><div class="line">robot</div></pre></td></tr></table></figure>
<p>修改<code>cfg/voc.data</code>, 另存为<code>cfg/rob.names</code>:</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="attr">classes</span>= <span class="number">3</span></div><div class="line"><span class="attr">train</span>  = /home/m/data/ROBdevkit/train.txt</div><div class="line"><span class="attr">valid</span>  = /home/m/data/ROBdevkit/<span class="number">2017</span>_test.txt</div><div class="line"><span class="attr">names</span> = data/rob.names</div><div class="line"><span class="attr">backup</span> = /home/m/workspace/backup/</div></pre></td></tr></table></figure>
<p>其次修改<code>script/voc_label.py</code>, 另存为<code>script/rob_label.py</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</div><div class="line"><span class="keyword">import</span> pickle</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">from</span> os <span class="keyword">import</span> listdir, getcwd</div><div class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join</div><div class="line"></div><div class="line">sets = [(<span class="string">'2017'</span>, <span class="string">'train'</span>), (<span class="string">'2017'</span>, <span class="string">'val'</span>), (<span class="string">'2017'</span>, <span class="string">'test'</span>)]</div><div class="line"></div><div class="line">classes = [<span class="string">'ball'</span>, <span class="string">'goal'</span>, <span class="string">'robot'</span>]</div><div class="line"></div><div class="line">...</div></pre></td></tr></table></figure>
<p>然后在<code>ROBdevkit</code>的根目录下执行<code>python rob_label.py</code>来生成 label 文件, 并用<code>cat 2017_* &gt; train.txt</code>生成<code>train.txt</code>. 最终目录结构为:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── ROBdevkit</div><div class="line">│   ├── 2017_test.txt</div><div class="line">│   ├── 2017_train.txt</div><div class="line">│   ├── 2017_val.txt</div><div class="line">│   ├── results</div><div class="line">│   ├── ROB2017</div><div class="line">│   ├── scripts</div><div class="line">│   ├── train.txt</div><div class="line">│   └── VOC0712</div><div class="line">├── rob_label.py</div></pre></td></tr></table></figure>
<p>最后, 修改<code>cfg/voc.data</code>, 另存为<code>cfg/rob.names</code>:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">diff --git a/cfg/tiny-yolo-voc.cfg b/cfg/tiny-yolo-rob.cfg</div><div class="line"><span class="deletion">-- a/cfg/tiny-yolo-voc.cfg</span></div><div class="line"><span class="addition">++ b/cfg/tiny-yolo-rob.cfg</span></div><div class="line">[convolutional]</div><div class="line">size=1</div><div class="line">stride=1</div><div class="line">pad=1</div><div class="line"><span class="deletion">-filters=250</span></div><div class="line"><span class="addition">+filters=40</span></div><div class="line">activation=linear</div><div class="line"></div><div class="line">[region]</div><div class="line">anchors = 1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52</div><div class="line">bias_match=1</div><div class="line"><span class="deletion">-classes=20</span></div><div class="line"><span class="addition">+classes=3</span></div><div class="line">coords=4</div><div class="line">num=5</div><div class="line">softmax=1</div><div class="line">jitter=.2</div><div class="line">rescore=1</div><div class="line"></div><div class="line">object_scale=5</div><div class="line">noobject_scale=1</div><div class="line">class_scale=1</div><div class="line">coord_scale=1</div><div class="line"></div><div class="line">absolute=1</div><div class="line">thresh = .6</div><div class="line">random=1</div></pre></td></tr></table></figure>
<blockquote>
<p>注意<code>region</code>的前一层的<code>filter</code>值的计算方法为$num \times (classes+coords+1)$.</p>
</blockquote>
<h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./darknet detector train cfg/rob.data cfg/tiny-yolo-rob.cfg darknet.conv.weights</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在看 Darkflow 的时候, 发现连 YOLOv2 都出了, 据称 mAP 和速度都提升了不少, 立马 clone 下来试了一番.&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="YOLO" scheme="http://www.yuthon.com/tags/YOLO/"/>
    
      <category term="Object detection" scheme="http://www.yuthon.com/tags/Object-detection/"/>
    
      <category term="Darknet" scheme="http://www.yuthon.com/tags/Darknet/"/>
    
  </entry>
  
  <entry>
    <title>1Password Problem browser could not be verified</title>
    <link href="http://www.yuthon.com/2016/12/02/1Password-Problem-browser-could-not-be-verified/"/>
    <id>http://www.yuthon.com/2016/12/02/1Password-Problem-browser-could-not-be-verified/</id>
    <published>2016-12-02T08:41:04.000Z</published>
    <updated>2016-12-02T08:52:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在 Chrome 上使用 1Password 总是会提示<code>Browser could not be verified</code>, 经过查阅资料解决, 特此记录.</p>
<a id="more"></a>
<p>现象如图:</p>
<p><img src="/images/1Password-Problem-browser-could-not-be-verified-1.png" alt="1Password-Problem-browser-could-not-be-verified-1"></p>
<p>根据官网<a href="https://support.1password.com/firewall-proxy/" target="_blank" rel="external">这篇文章</a>, 应该是 Surge 代理的原因. 只要将<code>127.0.0.1</code>加入到白名单里面就好了.</p>
<h2 id="Configure-your-proxy-settings"><a href="#Configure-your-proxy-settings" class="headerlink" title="Configure your proxy settings"></a>Configure your proxy settings</h2><ol>
<li><p>Choose Apple menu () &gt; System Preferences, then click the Network icon.</p>
</li>
<li><p>Select your primary network interface (typically Wi-Fi, or Ethernet if you have a wired connection).</p>
</li>
<li><p>Click Advanced, then select the Proxies tab.</p>
<p><img src="/images/1Password-Problem-browser-could-not-be-verified-2.png" alt="1Password-Problem-browser-could-not-be-verified-2"></p>
</li>
<li><p>Select Web Proxy (HTTP).</p>
</li>
<li><p>Under “Bypass proxy settings for these Hosts &amp; Domains”, click to the right of the existing text, and type a comma followed by <code>127.0.0.1</code>. Then click OK.</p>
</li>
</ol>
<blockquote>
<p>If “Secure Web Proxy (HTTPS)” is also enabled, select it, and add <code>127.0.0.1</code> as above.</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在 Chrome 上使用 1Password 总是会提示&lt;code&gt;Browser could not be verified&lt;/code&gt;, 经过查阅资料解决, 特此记录.&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="1Password" scheme="http://www.yuthon.com/tags/1Password/"/>
    
      <category term="macOS" scheme="http://www.yuthon.com/tags/macOS/"/>
    
  </entry>
  
  <entry>
    <title>Train Caffe-YOLO on our own dataset</title>
    <link href="http://www.yuthon.com/2016/11/26/Train-Caffe-YOLO-on-our-own-dataset/"/>
    <id>http://www.yuthon.com/2016/11/26/Train-Caffe-YOLO-on-our-own-dataset/</id>
    <published>2016-11-26T10:11:14.000Z</published>
    <updated>2016-11-26T11:26:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>经过这几天不断地测试, YOLO 在 TX1 上跑得还是挺不错的, 符合我们实验室的要求. 但是 YOLO 依赖的 Darknet 框架还是太原始了, 不如 TensorFlow 或者 Caffe 用着顺手. 另外, 我负责的目标检测这一块还需要和梅老板写的新框架相结合, 所以更加需要把 YOLO 移植到一个成熟的框架上去.</p>
<p>很幸运的是, YOLO 在各个框架上的移植都有前人做过了, 比如 <a href="https://github.com/thtrieu/darktf" target="_blank" rel="external">darktf</a> 和 <a href="https://github.com/yeahkun/caffe-yolo" target="_blank" rel="external">caffe-yolo</a>. 今天以 caffe-yolo 为例, 谈一下在其上使用自己的数据集来训练.</p>
<a id="more"></a>
<h2 id="Reformat-our-dataset-as-PASCAL-VOC-style"><a href="#Reformat-our-dataset-as-PASCAL-VOC-style" class="headerlink" title="Reformat our dataset as PASCAL VOC style"></a>Reformat our dataset as PASCAL VOC style</h2><p>为了之后的方便起见, 首先将我们的数据集转成 PASCAL VOC 的标准的目录格式.</p>
<h3 id="Structure-of-PASCAL-VOC-dataset"><a href="#Structure-of-PASCAL-VOC-dataset" class="headerlink" title="Structure of PASCAL VOC dataset"></a>Structure of PASCAL VOC dataset</h3><p>其目录结构如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── VOC2007</div><div class="line">│   ├── Annotations</div><div class="line">│   ├── ImageSets</div><div class="line">│   ├── JPEGImages</div><div class="line">│   ├── SegmentationClass</div><div class="line">│   └── SegmentationObject</div><div class="line">└── VOC2012</div><div class="line">    ├── Annotations</div><div class="line">    ├── ImageSets</div><div class="line">    ├── JPEGImages</div><div class="line">    ├── SegmentationClass</div><div class="line">    └── SegmentationObject</div></pre></td></tr></table></figure>
<p>其中<code>Annotations</code>目录放的是<code>.xml</code>文件, <code>JPEGImages</code>目录中存放的是对应的<code>.jpg</code>图像. 由于我们不做语义分割, 所以<code>SegmentationClass</code>与<code>SegmentationObject</code>对我们没什么用.</p>
<p> <code>ImageSets</code>目录中结构如下, 主要关注的是<code>Main</code>文件夹中的<code>trainval.txt</code>, <code>train.txt</code> , <code>val.txt</code>以及<code>test.txt</code>四个文件.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── Layout</div><div class="line">│   ├── test.txt</div><div class="line">│   ├── train.txt</div><div class="line">│   ├── trainval.txt</div><div class="line">│   └── val.txt</div><div class="line">├── Main</div><div class="line">│   ├── aeroplane_test.txt</div><div class="line">│   ├── aeroplane_train.txt</div><div class="line">│   ├── aeroplane_trainval.txt</div><div class="line">│   ├── aeroplane_val.txt</div><div class="line">│   ├── ...</div><div class="line">│   ├── test.txt</div><div class="line">│   ├── train.txt</div><div class="line">│   ├── trainval.txt</div><div class="line">│   └── val.txt</div><div class="line">└── Segmentation</div><div class="line">    ├── test.txt</div><div class="line">    ├── train.txt</div><div class="line">    ├── trainval.txt</div><div class="line">    └── val.txt</div></pre></td></tr></table></figure>
<h3 id="Reformat-our-dataset"><a href="#Reformat-our-dataset" class="headerlink" title="Reformat our dataset"></a>Reformat our dataset</h3><p>首先是把之前杂乱的图片文件名重新整理, 如下所示:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── image00001.jpg</div><div class="line">├── image00002.jpg</div><div class="line">├── image00012.jpg</div><div class="line">├── ...</div><div class="line">├── image04524.jpg</div><div class="line">├── image04525.jpg</div><div class="line">└── image04526.jpg</div></pre></td></tr></table></figure>
<p>随后用<code>labelImg</code>重新标注这些图. 标注完成后, 建立我们自己的数据集的结构, 并且将图片和标注放到对应的文件夹里:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── ROB2017</div><div class="line">│   ├── Annotations</div><div class="line">│   ├── ImageSets</div><div class="line">│   ├── JPEGImages</div><div class="line">│   └── JPEGImages_original</div><div class="line">└── scripts</div><div class="line">    ├── clean.py</div><div class="line">    ├── conf.json</div><div class="line">    ├── convert_png2jpg.py</div><div class="line">    └── split_dataset.py</div></pre></td></tr></table></figure>
<p>之后写了几个脚本, 其中<code>clean.py</code>用来清理未标注的图片; <code>split_dataset.py</code>用来分割训练集验证集测试集, 并且保存到<code>ImageSets/Main</code>中.</p>
<p>至此, 把我们的数据集转成 PASCAL VOC 标准目录的工作就完成了, 可以进行下一步的训练工作.</p>
<h2 id="Train-YOLO-on-Caffe"><a href="#Train-YOLO-on-Caffe" class="headerlink" title="Train YOLO on Caffe"></a>Train YOLO on Caffe</h2><h3 id="Clone-amp-Make"><a href="#Clone-amp-Make" class="headerlink" title="Clone &amp; Make"></a>Clone &amp; Make</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/yeahkun/caffe-yolo.git</div><div class="line">$ <span class="built_in">cd</span> caffe-yolo</div><div class="line">$ cp Makefile.config.example Makefile.config</div><div class="line">$ make -j8</div></pre></td></tr></table></figure>
<p>若是出现<code>src/caffe/net.cpp:8:18: fatal error: hdf5.h: No such file or directory</code>这一错误, 可以照下文修改<code>Makefile.config</code>文件:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">diff --git a/Makefile.config b/Makefile.config</div><div class="line">index a873502..88828cc 100644</div><div class="line"><span class="comment">--- a/Makefile.config.example</span></div><div class="line"><span class="comment">+++ b/Makefile.config.example</span></div><div class="line">@@ -69,8 +69,8 @@ PYTHON_LIB := /usr/lib</div><div class="line"> # WITH_PYTHON_LAYER := 1</div><div class="line"></div><div class="line"> # Whatever else you find you need goes here.</div><div class="line"><span class="deletion">-INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include</span></div><div class="line"><span class="deletion">-LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib</span></div><div class="line"><span class="addition">+INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/</span></div><div class="line"><span class="addition">+LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu/hdf5/serial/</span></div><div class="line"></div><div class="line"> # If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies</div><div class="line"> # INCLUDE_DIRS += $(shell brew --prefix)/include</div></pre></td></tr></table></figure>
<p>同时还可以开启 cuDNN 以及修改 compute, 充分发挥 GTX1080 的性能:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">## Refer to http://caffe.berkeleyvision.org/installation.html</div><div class="line"># Contributions simplifying and improving our build system are welcome!</div><div class="line"></div><div class="line"># cuDNN acceleration switch (uncomment to build with cuDNN).</div><div class="line"><span class="deletion">-# USE_CUDNN := 1</span></div><div class="line"><span class="addition">+USE_CUDNN := 1</span></div><div class="line"></div><div class="line"># CPU-only switch (uncomment to build without GPU support).</div><div class="line"># CPU_ONLY := 1</div><div class="line">...</div><div class="line"># CUDA architecture setting: going with all of them.</div><div class="line"># For CUDA &lt; 6.0, comment the *_50 lines for compatibility.</div><div class="line">CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \</div><div class="line">                -gencode arch=compute_20,code=sm_21 \</div><div class="line">                -gencode arch=compute_30,code=sm_30 \</div><div class="line">                -gencode arch=compute_35,code=sm_35 \</div><div class="line">                -gencode arch=compute_50,code=sm_50 \</div><div class="line"><span class="deletion">-                -gencode arch=compute_50,code=compute_50</span></div><div class="line"><span class="addition">+                -gencode arch=compute_50,code=compute_50 \</span></div><div class="line"><span class="addition">+                -gencode arch=compute_61,code=compute_61</span></div></pre></td></tr></table></figure>
<h3 id="Data-preparation"><a href="#Data-preparation" class="headerlink" title="Data preparation"></a>Data preparation</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> data/yolo</div><div class="line">$ ln <span class="_">-s</span> /your/path/to/VOCdevkit/ .</div><div class="line">$ python ./get_list.py</div><div class="line"><span class="comment"># change related path in script convert.sh</span></div><div class="line">$ sudo rm -r lmdb</div><div class="line">$ mkdir lmdb</div><div class="line">$ ./convert.sh</div></pre></td></tr></table></figure>
<p>有一些注意点:</p>
<ul>
<li><p>记得将<code>ln -s /your/path/to/VOCdevkit/ .</code>中的<code>/your/path/to/VOCdevkit/</code>换成自己数据集的路径, 例如<code>ln -s ~/data/ROBdevkit/ .</code></p>
</li>
<li><p>修改<code>./get_list.py</code>:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line">diff --git a/data/yolo/get_list.py b/data/yolo/get_list.py</div><div class="line">index f519f1a..73b9858 100755</div><div class="line"><span class="comment">--- a/data/yolo/get_list.py</span></div><div class="line"><span class="comment">+++ b/data/yolo/get_list.py</span></div><div class="line">@@ -3,12 +3,15 @@ import os</div><div class="line"></div><div class="line"> trainval_jpeg_list = []</div><div class="line"> trainval_xml_list = []</div><div class="line"><span class="deletion">-test07_jpeg_list = []</span></div><div class="line"><span class="deletion">-test07_xml_list = []</span></div><div class="line"><span class="deletion">-test12_jpeg_list = []</span></div><div class="line"><span class="deletion">-</span></div><div class="line"><span class="deletion">-for name in ["VOC2007", "VOC2012"]:</span></div><div class="line"><span class="deletion">-  voc_dir = os.path.join("VOCdevkit", name)</span></div><div class="line"><span class="addition">+test_jpeg_list = []</span></div><div class="line"><span class="addition">+test_xml_list = []</span></div><div class="line"><span class="addition">+</span></div><div class="line"><span class="addition">+for name in ['ROB2017']:</span></div><div class="line"><span class="addition">+  # voc_dir = os.path.join("VOCdevkit", name)</span></div><div class="line"><span class="addition">+  voc_dir = os.path.join('ROBdevkit', name)</span></div><div class="line">   txt_fold = os.path.join(voc_dir, "ImageSets/Main")</div><div class="line">   jpeg_fold = os.path.join(voc_dir, "JPEGImages")</div><div class="line">   xml_fold = os.path.join(voc_dir, "Annotations")</div><div class="line">@@ -23,35 +26,49 @@ for name in ["VOC2007", "VOC2012"]:</div><div class="line">           print trainval_jpeg_list[-1], "not exist"</div><div class="line">         if not os.path.exists(trainval_xml_list[-1]):</div><div class="line">           print trainval_xml_list[-1], "not exist"</div><div class="line"><span class="deletion">-  if name == "VOC2007":</span></div><div class="line"><span class="deletion">-    file_path = os.path.join(txt_fold, "test.txt")</span></div><div class="line"><span class="deletion">-    with open(file_path, 'r') as fp:</span></div><div class="line"><span class="deletion">-      for line in fp:</span></div><div class="line"><span class="deletion">-        line = line.strip()</span></div><div class="line"><span class="deletion">-        test07_jpeg_list.append(os.path.join(jpeg_fold, "&#123;&#125;.jpg".format(line)))</span></div><div class="line"><span class="deletion">-        test07_xml_list.append(os.path.join(xml_fold, "&#123;&#125;.xml".format(line)))</span></div><div class="line"><span class="deletion">-        if not os.path.exists(test07_jpeg_list[-1]):</span></div><div class="line"><span class="deletion">-          print test07_jpeg_list[-1], "not exist"</span></div><div class="line"><span class="deletion">-        if not os.path.exists(test07_xml_list[-1]):</span></div><div class="line"><span class="deletion">-          print test07_xml_list[-1], "not exist"</span></div><div class="line"><span class="deletion">-  elif name == "VOC2012":</span></div><div class="line"><span class="addition">+  if name == "ROB2017":</span></div><div class="line">     file_path = os.path.join(txt_fold, "test.txt")</div><div class="line">     with open(file_path, 'r') as fp:</div><div class="line">       for line in fp:</div><div class="line">         line = line.strip()</div><div class="line"><span class="deletion">-        test12_jpeg_list.append(os.path.join(jpeg_fold, "&#123;&#125;.jpg".format(line)))</span></div><div class="line"><span class="deletion">-        if not os.path.exists(test12_jpeg_list[-1]):</span></div><div class="line"><span class="deletion">-          print test12_jpeg_list[-1], "not exist"</span></div><div class="line"><span class="addition">+        test_jpeg_list.append(os.path.join(jpeg_fold, "&#123;&#125;.jpg".format(line)))</span></div><div class="line"><span class="addition">+        test_xml_list.append(os.path.join(xml_fold, "&#123;&#125;.xml".format(line)))</span></div><div class="line"><span class="addition">+        if not os.path.exists(test_jpeg_list[-1]):</span></div><div class="line"><span class="addition">+          print test_jpeg_list[-1], "not exist"</span></div><div class="line"><span class="addition">+        if not os.path.exists(test_xml_list[-1]):</span></div><div class="line"><span class="addition">+          print test_xml_list[-1], "not exist"</span></div><div class="line"></div><div class="line"> with open("trainval.txt", "w") as wr:</div><div class="line">   for i in range(len(trainval_jpeg_list)):</div><div class="line">     wr.write("&#123;&#125; &#123;&#125;\n".format(trainval_jpeg_list[i], trainval_xml_list[i]))</div><div class="line"></div><div class="line"><span class="deletion">-with open("test_2007.txt", "w") as wr:</span></div><div class="line"><span class="deletion">-  for i in range(len(test07_jpeg_list)):</span></div><div class="line"><span class="deletion">-    wr.write("&#123;&#125; &#123;&#125;\n".format(test07_jpeg_list[i], test07_xml_list[i]))</span></div><div class="line"><span class="deletion">-</span></div><div class="line"><span class="deletion">-with open("test_2012.txt", "w") as wr:</span></div><div class="line"><span class="deletion">-  for i in range(len(test12_jpeg_list)):</span></div><div class="line"><span class="deletion">-    wr.write("&#123;&#125;\n".format(test12_jpeg_list[i]))</span></div><div class="line"><span class="addition">+with open("test.txt", "w") as wr:</span></div><div class="line"><span class="addition">+  for i in range(len(test_jpeg_list)):</span></div><div class="line"><span class="addition">+    wr.write("&#123;&#125; &#123;&#125;\n".format(test_jpeg_list[i], test_xml_list[i]))</span></div></pre></td></tr></table></figure>
</li>
<li><p>修改<code>convert.sh</code></p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">diff --git a/data/yolo/convert.sh b/data/yolo/convert.sh</div><div class="line">index 8a52525..a06eb69 100755</div><div class="line"><span class="comment">--- a/data/yolo/convert.sh</span></div><div class="line"><span class="comment">+++ b/data/yolo/convert.sh</span></div><div class="line"><span class="meta">@@ -1,7 +1,7 @@</span></div><div class="line"> #!/usr/bin/env sh</div><div class="line"></div><div class="line"> CAFFE_ROOT=../..</div><div class="line"><span class="deletion">-ROOT_DIR=/your/path/to/vocroot/</span></div><div class="line"><span class="addition">+ROOT_DIR=/home/m/data/</span></div><div class="line"> LABEL_FILE=$CAFFE_ROOT/data/yolo/label_map.txt</div><div class="line"></div><div class="line"> # 2007 + 2012 trainval</div><div class="line">@@ -10,13 +10,15 @@ LMDB_DIR=./lmdb/trainval_lmdb</div><div class="line"> SHUFFLE=true</div><div class="line"></div><div class="line"> # 2007 test</div><div class="line"><span class="deletion">-# LIST_FILE=$CAFFE_ROOT/data/yolo/test_2007.txt</span></div><div class="line"><span class="deletion">-# LMDB_DIR=./lmdb/test2007_lmdb</span></div><div class="line"><span class="addition">+# LIST_FILE=$CAFFE_ROOT/data/yolo/test.txt</span></div><div class="line"><span class="addition">+# LMDB_DIR=./lmdb/test_lmdb</span></div><div class="line"> # SHUFFLE=false</div><div class="line"></div><div class="line"> RESIZE_W=448</div><div class="line"> RESIZE_H=448</div><div class="line"></div><div class="line"> $CAFFE_ROOT/build/tools/convert_box_data --resize_width=$RESIZE_W --resize_height=$RESIZE_H \</div><div class="line">   --label_file=$LABEL_FILE $ROOT_DIR $LIST_FILE $LMDB_DIR --encoded=true --encode_type=jpg --shuffle=$SHUFFLE</div></pre></td></tr></table></figure>
</li>
<li><p>修改<code>label_map.txt</code>:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">diff --git a/data/yolo/label_map.txt b/data/yolo/label_map.txt</div><div class="line">index 1fe873a..bee8f82 100644</div><div class="line"><span class="comment">--- a/data/yolo/label_map.txt</span></div><div class="line"><span class="comment">+++ b/data/yolo/label_map.txt</span></div><div class="line"><span class="meta">@@ -1,20 +1,3 @@</span></div><div class="line"><span class="deletion">-aeroplane 0</span></div><div class="line"><span class="deletion">-bicycle 1</span></div><div class="line"><span class="deletion">-bird 2</span></div><div class="line"><span class="deletion">-boat 3</span></div><div class="line"><span class="deletion">-bottle 4</span></div><div class="line"><span class="deletion">-bus 5</span></div><div class="line"><span class="deletion">-car 6</span></div><div class="line"><span class="deletion">-cat 7</span></div><div class="line"><span class="deletion">-chair 8</span></div><div class="line"><span class="deletion">-cow 9</span></div><div class="line"><span class="deletion">-diningtable 10</span></div><div class="line"><span class="deletion">-dog 11</span></div><div class="line"><span class="deletion">-horse 12</span></div><div class="line"><span class="deletion">-motorbike 13</span></div><div class="line"><span class="deletion">-person 14</span></div><div class="line"><span class="deletion">-pottedplant 15</span></div><div class="line"><span class="deletion">-sheep 16</span></div><div class="line"><span class="deletion">-sofa 17</span></div><div class="line"><span class="deletion">-train 18</span></div><div class="line"><span class="deletion">-tvmonitor 19</span></div><div class="line"><span class="addition">+ball 0</span></div><div class="line"><span class="addition">+goal 1</span></div><div class="line"><span class="addition">+robot 2</span></div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cd examples/yolo</div><div class="line"># change related path in script train.sh</div><div class="line">mkdir models</div><div class="line">nohup ./train.sh &amp;</div></pre></td></tr></table></figure>
<p>也有一些注意点:</p>
<ul>
<li><p>修改<code>gnet_train.prototxt</code>:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">diff --git a/examples/yolo/gnet_train.prototxt b/examples/yolo/gnet_train.prototxt</div><div class="line">index 8483a32..da01daf 100644</div><div class="line"><span class="comment">--- a/examples/yolo/gnet_train.prototxt</span></div><div class="line"><span class="comment">+++ b/examples/yolo/gnet_train.prototxt</span></div><div class="line">@@ -36,7 +36,7 @@ layer &#123;</div><div class="line">     mean_value: 123</div><div class="line">   &#125;</div><div class="line">   data_param &#123;</div><div class="line"><span class="deletion">-    source: "../../data/yolo/lmdb/test2007_lmdb"</span></div><div class="line"><span class="addition">+    source: "../../data/yolo/lmdb/test_lmdb"</span></div><div class="line">     batch_size: 1</div><div class="line">     side: 7</div><div class="line">     backend: LMDB</div></pre></td></tr></table></figure>
</li>
<li><p>修改<code>train.sh</code>:</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">diff --git a/examples/yolo/train.sh b/examples/yolo/train.sh</div><div class="line">index 416e2b0..ecd0872 100755</div><div class="line"><span class="comment">--- a/examples/yolo/train.sh</span></div><div class="line"><span class="comment">+++ b/examples/yolo/train.sh</span></div><div class="line"><span class="meta">@@ -3,8 +3,7 @@</span></div><div class="line"> CAFFE_HOME=../..</div><div class="line"></div><div class="line"> SOLVER=./gnet_solver.prototxt</div><div class="line"><span class="deletion">-WEIGHTS=/your/path/to/bvlc_googlenet.caffemodel</span></div><div class="line"><span class="addition">+WEIGHTS=/home/m/workspace/caffe-yolo/models/bvlc_googlenet/bvlc_googlenet.caffemodel</span></div><div class="line"></div><div class="line"> $CAFFE_HOME/build/tools/caffe train \</div><div class="line"><span class="deletion">-    --solver=$SOLVER --weights=$WEIGHTS --gpu=0,1</span></div><div class="line"><span class="addition">+    --solver=$SOLVER --weights=$WEIGHTS --gpu=0</span></div></pre></td></tr></table></figure>
</li>
<li><p>注意还要预先下载 GoogleNet 的<a href="http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel" target="_blank" rel="external">预训练权重文件</a>, 并且放在<code>caffe-yolo/models/bvlc_googlenet/</code>(当然放哪里是随便的, 注意改<code>train.sh</code>中的相应地址即可).</p>
</li>
</ul>
<h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># if everything goes well, the map of gnet_yolo_iter_32000.caffemodel may reach ~56.</div><div class="line">cd examples/yolo</div><div class="line">./test.sh model_path gpu_id</div></pre></td></tr></table></figure>
<p>(To be continued)</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;经过这几天不断地测试, YOLO 在 TX1 上跑得还是挺不错的, 符合我们实验室的要求. 但是 YOLO 依赖的 Darknet 框架还是太原始了, 不如 TensorFlow 或者 Caffe 用着顺手. 另外, 我负责的目标检测这一块还需要和梅老板写的新框架相结合, 所以更加需要把 YOLO 移植到一个成熟的框架上去.&lt;/p&gt;
&lt;p&gt;很幸运的是, YOLO 在各个框架上的移植都有前人做过了, 比如 &lt;a href=&quot;https://github.com/thtrieu/darktf&quot;&gt;darktf&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/yeahkun/caffe-yolo&quot;&gt;caffe-yolo&lt;/a&gt;. 今天以 caffe-yolo 为例, 谈一下在其上使用自己的数据集来训练.&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="YOLO" scheme="http://www.yuthon.com/tags/YOLO/"/>
    
      <category term="Object detection" scheme="http://www.yuthon.com/tags/Object-detection/"/>
    
      <category term="Darknet" scheme="http://www.yuthon.com/tags/Darknet/"/>
    
  </entry>
  
  <entry>
    <title>Thesis Notes for ScribbleSup</title>
    <link href="http://www.yuthon.com/2016/11/20/Thesis-Notes-for-ScribbleSup/"/>
    <id>http://www.yuthon.com/2016/11/20/Thesis-Notes-for-ScribbleSup/</id>
    <published>2016-11-20T10:26:24.000Z</published>
    <updated>2016-11-22T14:30:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>毕设需要写一个图像标注的软件, 来给场景分割的数据集做标注. 经学长推荐, 看了今年的这篇文章, 作者中竟然还有 Kaiming He 大神, 给微软膜一秒.</p>
<p>这篇文章讲了一个弱监督的场景分割的算法 ScribbleSup, 主要是先通过 Graph Cut 将输入的 scribble 信息广播到没有标注的像素, 然后用 FCN 来做像素级别的预测. 令人遗憾的是 Github 上并没有人实现 (不能偷懒了TAT).</p>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>TBD</p>
<h2 id="Scribble-Supervised-Learning"><a href="#Scribble-Supervised-Learning" class="headerlink" title="Scribble-Supervised Learning"></a>Scribble-Supervised Learning</h2><h3 id="Objective-Functions"><a href="#Objective-Functions" class="headerlink" title="Objective Functions"></a>Objective Functions</h3><p>主要用到的记号如下:</p>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Name</th>
<th>Note</th>
</tr>
</thead>
<tbody>
<tr>
<td>$X$</td>
<td>training image</td>
<td></td>
</tr>
<tr>
<td>${x_i}$</td>
<td>set of non-overlapping superpixles</td>
<td>$\cup_i x_i = X; x_i \cap x_j = \varnothing, \forall i,j$</td>
</tr>
<tr>
<td>$S$</td>
<td>scribble annotations of image</td>
<td>$S={s_k, c_k}$</td>
</tr>
<tr>
<td>$s_k$</td>
<td>the pixels of a scribble $k$</td>
<td></td>
</tr>
<tr>
<td>$c_k$</td>
<td>the scribble’s category label</td>
<td>$0 \le c_k \le C$; $c_k=0$ for background</td>
</tr>
<tr>
<td>$Y$ or ${y_i}$</td>
<td>the category label of ${x_i}$</td>
<td>provides full annotations of the image</td>
</tr>
</tbody>
</table>
<p>定义目标函数为</p>
<p>$$\sum_i \psi_i (y_i | X,S) + \sum_{i,j} \psi_{ij} (y_i, y_j | X)$$</p>
<p>其中$\psi_i$是一个关于$x_i$的一元项 (unary term), 而$\psi\ _{ij}$是关于$x_i$与$x_j$的成对项 (pairwise term).</p>
<ul>
<li><p>$\psi _i$由两个部分组成, 一个是$\psi ^{scr}_i$, 另一个是$\psi^{net}_i$.两者权重相同, $\psi _i = \psi^{scr} _i + \psi^{net} _i$.</p>
<ul>
<li><p>$\psi ^{scr}_i$ 基于 scribble, 定义如下:<br>$$<br>\psi ^{scr}_i=<br>\begin{aligned}<br>&amp;0 &amp; \text{if $y_i=c_k$ and $x_i \cap s_k \ne \varnothing$}\\<br>&amp;-log(\frac{1}{|c_k|}) &amp; \text{if $y_i \in {c_k}$ and $x_i \cap S = \varnothing$} \\<br>&amp;\infty &amp; \text{otherwise} \\<br>\end{aligned}<br>$$</p>
<ul>
<li>当$x_i$与$s_k$有交集, 且标签是分到$c_k$时, 则$cost=0$</li>
<li>当$x_i$与所有 scribble 都没有交集, 则它可以被等概率地分给任何标签. 当然, $y_i$需要在${c_k}$之内. 此处$|{c_k}|$表示标签集内元素个数.</li>
<li>如果不是以上两种情况, 则$cost= \infty$</li>
</ul>
</li>
<li><p>$\psi ^{net}_i$基于 FCN 的输出, 定义为<br>$$<br>\psi^{net}_i (y_i) = -log P(y_i|X, \Theta)<br>$$</p>
<ul>
<li>$\Theta$表示网络的参数</li>
<li>$log P(y_i|X, \Theta)$表示了$x_i$属于标签$y_i$的对数概率, 实际上是$x_i$内所有像素的对数概率之和.</li>
</ul>
</li>
</ul>
</li>
<li><p>$\psi_{ij}$用以衡量相邻的两个超像素的相似程度, 主要是用色彩直方图与纹理直方图来量化 (均已归一化).<br>$$<br>\psi_{ij} (y_i, y_j | X) = [y_i \ne y_j] exp \left( -\frac{||h_c(x_i) - h_c(x_j)||^2_2}{\delta_c^2} - \frac{||h_t(x_i) - h_t(x_j)||^2_2}{\delta_t^2} \right)<br>$$</p>
<ul>
<li>$h_c(x_i)$ 表示RGB三个 channel 每个 channel 分成 25 bins 的色彩直方图</li>
<li>$h_t(x_i)$ 表示横向与纵向的梯度直方图, 每个方向 10 bins</li>
<li>$[\cdot]$表示一个符号函数, 条件为真则为$1$, 否则为$0$</li>
<li>$\delta_c=5, \delta_t = 10$</li>
<li>对于不是同一个标签的临近超像素来说, 它们间的外观越相似, 则 cost 越大</li>
</ul>
</li>
</ul>
<p>最后把上边这些合起来, 就成了一个对于以下式子进行最优化的问题:<br>$$<br>\sum_i \psi^{scr}_i (y_i |X, S) + \sum_i -log P(y_i  | X, \Theta) + \sum_{i,j} \psi_{ij} (y_i, y_j | X)<br>$$<br>其中有两组变量, 一个是所有超像素的标签$Y={y_i}$, 另一个是 FCN 的参数 $\Theta$.</p>
<p> <img src="/images/ScribbleSup_grapgical_model.png" alt="ScribbleSup_grapgical_model"></p>
<h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>论文里采用的是一种交替优化的方法:</p>
<ul>
<li>$\Theta$固定, 优化$Y$, slover 基于 scribbles, appearance 以及 FCN 网络的预测, 将标签传播到未标记的像素中</li>
<li>$Y$固定, 优化$\Theta$, slover 对 pixel-wise 的语义分割的 FCN 进行学习</li>
</ul>
<p>具体的来说就是</p>
<ul>
<li><p><strong>Propagating scribble information to unmarked pixels</strong></p>
<p>当$\Theta$固定时, 一元项$\psi _i = \psi^{scr} _i + \psi^{net} _i$能够用列举所有可能的标签$0 \le y_i \le C$得到, 成对项也能够预先计算生成一个 look-up table. 因此, 优化问题就能用 graph cut 的方法来解决. 论文里用的是<a href="http://www.csd.uwo.ca/faculty/yuri/Papers/pami04.pdf" target="_blank" rel="external">这一篇文章</a>的<a href="http://vision.csd.uwo.ca/code/gco- v3.0.zip" target="_blank" rel="external">现成代码</a>.</p>
</li>
<li><p><strong>Optimizing network parameters</strong></p>
<p>前一步做完后, 所有超像素的标签都已经定好了, 也就是说$Y$固定了. 之后优化$\Theta$就相当于用$Y$做为监督来训练 FCN. $Y$有了, 那么每个像素的标签就有了, 然后 FCN 面对的就是一个 pixel-wise 的回归问题. FCN 的最后一层输出的就是每个像素的分类的对数概率, 可以用来更新 graph 上的一元项.</p>
</li>
</ul>
<p>训练的时候有几点需要注意:</p>
<ul>
<li>初始化的时候没有 network prediction, 因此就是直接用 graph cut 初始化的. 之后则是在两步之间不断迭代.</li>
<li>每次 network optimizing step 的时候, 前50k次用0.0003的 learning rate, 后10k次用0.0001的 learning rate, batch size 为 8.</li>
<li>每次 network optimizing step 都是从一个 pre-trained 的 model (比如 VGG-16) 重新初始化的. 作者也试过复用上一次迭代后的权重, 但是效果不是很理想. 似乎是由于本来标签就不可靠, 导致训练的时候参数被调到了不太好的局部最优里面.</li>
<li>基本上3次迭代就能得到比较好的效果了, 再多得到的提升微乎其微.</li>
<li>做验证的时候只要用 FCN 就好了, 超像素和 graph model 之类的都只是用来训练用的.</li>
<li>Post-process 用了 CRF.</li>
</ul>
<p>迭代结果如下:</p>
<p> <img src="/images/ScribbleSup_training.png" alt="ScribbleSup_training"></p>
<h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="Graphical-models-for-segmentation"><a href="#Graphical-models-for-segmentation" class="headerlink" title="Graphical models for segmentation"></a>Graphical models for segmentation</h4><p>Graphical model 在交互式的图像分割和语义分割领域是很常见的, 通常是目标函数包含了一元项和成对项, 特别适用于对局部和全局的空间约束的建模.</p>
<p>有趣的是, FCN 作为目前最成功的语义分割的方法之一, 由于做的是 pixel-wise 的 regression, 因此其目标函数只有一元项. 不过像 CRF/MRF 这样给 FCN 做 post-processing 或是 joint-training 的方法在之后也发展起来了.</p>
<p>但是这一类 graph model 都是强监督的, 主要工作是在优化 mask 的边缘, 而 ScribbleSup 里面的 graph model 主要是用来把标签传播到其他未标注的像素上. 同时, 这类方法是 pixel-based, 而 ScribbleSup 是 super-pixel-based.</p>
<h4 id="Weakly-supervised-semantic-segmentation"><a href="#Weakly-supervised-semantic-segmentation" class="headerlink" title="Weakly-supervised semantic segmentation"></a>Weakly-supervised semantic segmentation</h4><p>用 CNN/FCN 来做弱监督的语义分割的方法很多, 用的标注方法也有很多种.</p>
<ul>
<li>Image-level 的标注很容易获取, 但是只用这个的话精度远低于强监督的结果</li>
<li>Box-level 的相比较而言结果与强监督的接近了不少. 由于 Box annotations 本身就提供了物体边缘以及可信的背景区域的信息, 因此就不需要 graph model 来传播标签.</li>
</ul>
<p>这些方法和本篇论文里面讲的 ScribbleSup 比起来到底哪个更胜一筹, 姿势水平更高, 就看下面的实验了.</p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Annotating-Scribbles"><a href="#Annotating-Scribbles" class="headerlink" title="Annotating Scribbles"></a>Annotating Scribbles</h3><p>主要使用了 PASCAL VOC 2012 (20个分类) 以及 PASCAL-CONTEXT (59个分类) 这两个数据集, 同时也标注了 PASCAL VOC 2007 (标注了59个分类). 不过 2007 没有 mask-level 的标注.</p>
<p>总共有10个人在标注, 每张图片一人标注一人检查. 平均下来20分类的话每张图片25秒, 59分类的话每张图片50秒, 算是相当快的了.</p>
<p>同时, 保证每个 object 上的 scribble 至少有其 bounding box 长边的 70% 以上的长度.</p>
<h3 id="Experiments-on-PASCAL-VOC-2012"><a href="#Experiments-on-PASCAL-VOC-2012" class="headerlink" title="Experiments on PASCAL VOC 2012"></a>Experiments on PASCAL VOC 2012</h3><h4 id="Strategies-of-utilizing-scribbles"><a href="#Strategies-of-utilizing-scribbles" class="headerlink" title="Strategies of utilizing scribbles"></a>Strategies of utilizing scribbles</h4><p>ScribbleSup 是将标签的扩散与网络的训练合起来考虑的, 但是一个更为简单的方案是把这两步分开来, 先用一些现成的工具 (比如说 GrabCut 或者是 LazySnapping) 把 scribble 转换成 mask, 然后再来训练 FCN 网络. 这个方案听起来也是很吼的, 那么中央到底兹不兹瓷呢, 我们来看看实验结果</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>mIoU(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GrabCut + FCN</td>
<td>49.1</td>
</tr>
<tr>
<td>LazySnapping + FCN</td>
<td>53.8</td>
</tr>
<tr>
<td>ours, w/o pairwise terms</td>
<td>60.5</td>
</tr>
<tr>
<td>ours, w/ pairwise terms</td>
<td>63.1</td>
</tr>
</tbody>
</table>
<p>所以说不要听风就是雨, 可以看出分两步走的方案是一个错误的道路, mIoU显著低于 ScribbleSup. 其中的原因主要是这些传统的方法仅仅针对 low-level 的空间或者是色彩信息建模, 并没有考虑到语义的层面. 也就是说, 这些方法得到的 mask 是不值得信赖的, 不能作为 ground truth 来用.</p>
<p>而 ScribbleSup 就不同了, 通过不断的迭代, FCN 能够逐渐学习到 high-level 的语义特征, 这些特征又能反哺给 graph-based scribble propagation. 这样就形成了一个良性循环, 自然 mIoU 就不知比传统方法高到哪里去了.</p>
<p>同时可以看出, 用了成对项的效果比不用的好. 这是因为如果没有了成对项, 那么目标函数就只剩下了一元项, graph cut 步骤变成了基于network prediction 的 winner-take-all 的模式. 这样的话, 信息的传播就只与全卷积有关, 会过于看重局部一致性, 最终导致准确度降低.</p>
<h4 id="Sensitivities-to-scribble-quality"><a href="#Sensitivities-to-scribble-quality" class="headerlink" title="Sensitivities to scribble quality"></a>Sensitivities to scribble quality</h4><p>Scribble quality 是个非常主观的东西, 所以为了研究这个对于准确度的影响, 论文里采用了将原 scribble 放缩为不同长度 (甚至是一个点), 然后实验来观察.</p>
<p> <img src="/images/ScribbleSup_scribble_of_different_length.png" alt="scribble_of_different_length"></p>
<table>
<thead>
<tr>
<th>Length ratio</th>
<th>mIoU (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>63.1</td>
</tr>
<tr>
<td>0.8</td>
<td>61.8</td>
</tr>
<tr>
<td>0.5</td>
<td>58.5</td>
</tr>
<tr>
<td>0.3</td>
<td>54.3</td>
</tr>
<tr>
<td>0 (spot)</td>
<td>51.6</td>
</tr>
</tbody>
</table>
<p>可以看出, ScribbleSup 对于 scribble length 还是比较鲁棒的, 甚至到了一个点都还能有不错的准确度.</p>
<h4 id="Comparisons-with-other-weakly-supervised-methods"><a href="#Comparisons-with-other-weakly-supervised-methods" class="headerlink" title="Comparisons with other weakly-supervised methods"></a>Comparisons with other weakly-supervised methods</h4><p>All methods are trained on the PASCAL VOC 2012 training images using VGG-16, except that the annotations are different.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Annotations</th>
<th>mIoU (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>MIL-FCN</td>
<td>image-level</td>
<td>25.1</td>
</tr>
<tr>
<td>WSSL</td>
<td>image-level</td>
<td>38.2</td>
</tr>
<tr>
<td>point supervision</td>
<td>spot</td>
<td>46.1</td>
</tr>
<tr>
<td>WSSL</td>
<td>box</td>
<td>60.6</td>
</tr>
<tr>
<td>BoxSup</td>
<td>box</td>
<td>62.0</td>
</tr>
<tr>
<td>ours</td>
<td>spot</td>
<td>51.6</td>
</tr>
<tr>
<td>ours</td>
<td>scribble</td>
<td>63.1</td>
</tr>
</tbody>
</table>
<p>可以看出</p>
<ul>
<li>虽然 image-level 的标注很容易标, 但是训练出来的结果惨不忍睹. </li>
<li>同时, 用 scribble 来标注得到的结果准确度很不错, 并且也是相对比较方便的.</li>
<li>ScribbleSup 即便是用 spot 标注, 结果的 mIoU 也比 point supervision 高了 5%.</li>
</ul>
<h4 id="Comparisons-with-using-masks"><a href="#Comparisons-with-using-masks" class="headerlink" title="Comparisons with using masks"></a>Comparisons with using masks</h4><p>虐了一遍同等级的 weakly-supervised 的方法之后, ScribbleSup 开始对比使用 scribble 和使用 mask 得到的结果. (在 PASCAL VOC 2012 上训练)</p>
<table>
<thead>
<tr>
<th>Supervision</th>
<th># w/ masks</th>
<th># w/scribbles</th>
<th>total</th>
<th>mIoU (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>weakly</td>
<td>-</td>
<td>11k</td>
<td>11k</td>
<td>63.1</td>
</tr>
<tr>
<td>strongly</td>
<td>11k</td>
<td>-</td>
<td>11k</td>
<td>68.5</td>
</tr>
<tr>
<td>semi</td>
<td>11k</td>
<td>10k (VOC07)</td>
<td>21k</td>
<td>71.3</td>
</tr>
</tbody>
</table>
<p>使用 scribble 比使用 mask 得到的结果差了5%左右, 考虑到这两者标注的困难程度, 这点差距还是可以忍的.</p>
<p>ScribbleSup 其实也是可以用 mask-level 的标注来训练的. 对于 mask-level 的标注, 不使用 graph model, 直接扔到 FCN 的训练里面去就行了. 注意的是这些只能用在 FCN 的训练步骤里, 优化 graph model 这一步骤中不使用. 可以看出, scribble 与 mask 联合起来能达到71.3%的 mIoU, 可以说是非常理想了.</p>
<p> <img src="/images/ScribbleSup_results_on_VOC_2012.png" alt="ScribbleSup_results_on_VOC_2012"></p>
<h3 id="Experiments-on-PASCAL-CONTEXT"><a href="#Experiments-on-PASCAL-CONTEXT" class="headerlink" title="Experiments on PASCAL-CONTEXT"></a>Experiments on PASCAL-CONTEXT</h3><p>To the best of our knowledge, our accuracy is the current state of the art on this dataset. (向dalao低头)</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Data/Annotations</th>
<th>mIoU (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>CFM</td>
<td>5k w/ masks</td>
<td>34.4</td>
</tr>
<tr>
<td>FCN</td>
<td>5k w/ masks</td>
<td>35.1</td>
</tr>
<tr>
<td>Boxsup</td>
<td>5k w/ masks + 133k w/ boxes (COCO+VOC7)</td>
<td>40.5</td>
</tr>
<tr>
<td>baseline</td>
<td>5k w/ masks</td>
<td>37.7</td>
</tr>
<tr>
<td>ours, weakly</td>
<td>5k w/ scribbles</td>
<td>36.1</td>
</tr>
<tr>
<td>ours, weakly</td>
<td>5k w/ scribbles + 10k w/ scribbles (VOC07)</td>
<td>39.3</td>
</tr>
<tr>
<td>ours, semi</td>
<td>5k w/ masks + 10k w/ scribbles (VOC07)</td>
<td>42.0</td>
</tr>
</tbody>
</table>
<p> <img src="/images/ScribbleSup_results_on_PASCAL_CONTEXT.png" alt="ScribbleSup_results_on_PASCAL_CONTEXT"></p>
<p>(To be continued…)</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;毕设需要写一个图像标注的软件, 来给场景分割的数据集做标注. 经学长推荐, 看了今年的这篇文章, 作者中竟然还有 Kaiming He 大神, 给微软膜一秒.&lt;/p&gt;
&lt;p&gt;这篇文章讲了一个弱监督的场景分割的算法 ScribbleSup, 主要是先通过 Graph Cut 将输入的 scribble 信息广播到没有标注的像素, 然后用 FCN 来做像素级别的预测. 令人遗憾的是 Github 上并没有人实现 (不能偷懒了TAT).&lt;/p&gt;
    
    </summary>
    
      <category term="Thesis Notes" scheme="http://www.yuthon.com/categories/Thesis-Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="ScribbleSup" scheme="http://www.yuthon.com/tags/ScribbleSup/"/>
    
      <category term="Scene Segmentation" scheme="http://www.yuthon.com/tags/Scene-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>Thesis Notes for YOLO</title>
    <link href="http://www.yuthon.com/2016/11/18/Thesis-Notes-for-YOLO/"/>
    <id>http://www.yuthon.com/2016/11/18/Thesis-Notes-for-YOLO/</id>
    <published>2016-11-18T14:43:26.000Z</published>
    <updated>2016-11-20T13:22:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>前几天发烧流鼻涕, 睡不了觉, 因此就熬夜读完了 YOLO 的论文. 可以说, YOLO 的实现方式相较于之前 R-CNN 一系的 Region Proposal 的方法来说, 很有新意. YOLO 将 Classification 和 Bounding Box Regression 合起来放进了 CNN 的输出层里面, 从而大大加快了速度.</p>
<a id="more"></a>
<h2 id="Unified-Detection"><a href="#Unified-Detection" class="headerlink" title="Unified Detection"></a>Unified Detection</h2><p>YOLO 将 Bounding Box 的位置回归和分类都放在了 CNN 的输出层中, 从整张图输入来预测 Bounding Box 的信息, 从而实现了 end-to-end 的训练, 实时的检测性能, 并且还保持了较高的精度.</p>
<p> <img src="/images/YOLO_the_model.png" alt="YOLO_the_model"></p>
<p>YOLO 将整张图分成了$S\times S$个网格 (论文中$S=2$), 如果一个物体的中心在某个网格内, 那么这个网格就负责预测这个物体的检测.</p>
<p>每个网格需要预测$B$个 Bounding Box (论文中$B=2$), 以及它们的置信度 (confidence).</p>
<ul>
<li>置信度定义为$Pr(Object) * IOU^{truth}_{pred}$<ul>
<li>$Pr(Object)$ 即为有物体的概率, 取0或1</li>
<li>$IOU^{truth}_{pred}$ 即为 ground truth 与predicted box 区域的交并比</li>
</ul>
</li>
<li>每个 Bounding Box 有5个属性$(x,y,w,h,c)$<ul>
<li>$(x,y)$ 代表 Bounding Box 的中心距离与网格边界的相对距离, 取值在0与1之间<ul>
<li>$x = \frac{x_{max} + x_{min}}{2 * width}$</li>
<li>$y = \frac{y_{max} + y_{min}}{2 * height}$</li>
</ul>
</li>
<li>$(w,h)$ 代表 Bounding Box 的长宽与整个图像长宽的相对比值, 取值在0与1之间<ul>
<li>$x = \frac{x_{max} - x_{min}}{width}$</li>
<li>$y = \frac{y_{max} - y_{min}}{height}$</li>
</ul>
</li>
<li>$c$ 即此 Bounding Box 的置信度</li>
</ul>
</li>
</ul>
<p>每个格子还要预测 $C$ 个类别的概率, 记为$Pr(Class_i|Object)$. 此概率与网格中是否有物体有关, 并且使相对于每个网格来说的, 与网格中的 Bounding Box 数量 $B$ 无关.</p>
<ul>
<li>测试时, 将 class 的条件概率和 box 的置信度乘起来, 得到每个 box 关于 class 的置信度</li>
<li>$Pr(Class_i|Object) * Pr(Object) * IOU^{truth}_{pred} = Pr(Class_i) * IOU^{truth}_{pred}$</li>
<li>这个概率既包含了 box 属于哪个 class 的概率, 又包含了这个 box 对于 object 的拟合度</li>
</ul>
<p>合起来看, 最终的预测张量的维数是 $S\times S \times (B*5 + C)$. 论文里用 PASCAL VOC 数据集, 取$S=7, B=2, C=20$, 因此总计$7\times 7 \times 30$.</p>
<h2 id="Network-Design"><a href="#Network-Design" class="headerlink" title="Network Design"></a>Network Design</h2><p> <img src="/images/YOLO_the_architecture.png" alt="YOLO_the_architecture"></p>
<p>整个网络参考了 GoogleNet, 总共有24个卷积层和两个全连接层.</p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>为了让整个网络有更好的性能, YOLO 使用了以下 tricks:</p>
<ul>
<li><p>前20层卷积层使用 ImageNet 进行 pretrain, 后4层卷积层和两层全连接层则是随机初始化</p>
</li>
<li><p>将输入图像的分辨率从$224\times 224$提升到$448*448$</p>
</li>
<li><p>将$(x,y,w,h)$全部都归一化 (详见上文)</p>
</li>
<li><p>最后一层(输出层)采用线性激活函数, 其它层都用 Leaky ReLU.</p>
</li>
<li><p>损失函数采用平方和误差(sum-squared error), 并且针对以下问题作出了改进:</p>
<ul>
<li>8维的 box 的位置信息$(x,y,w,h)$, 2维的置信度信息, 以及20维 box 的类别信息的平方和误差直接放在一起显然是不合理的. 因此增加 box 的位置信息的误差的权重系数$\lambda_{coord}$ (论文内取$5$).</li>
<li>同时, 一个图像会有很多网格没有物体, 那么就会把格子里的 box 的置信度变成 0, 导致那些真正有物体的柜子被压制, 最终导致整个网络发散.因此减少没有物体的 box 的权重系数$\lambda_noobj$ (论文内取$0.5$).</li>
<li>另外, 平方和误差会把 large box 和 small Box 的误差一视同仁. 然而相对于 large box 稍微偏一点, small box 的误差更加不能忍受. 因此使用$(\sqrt{w}, \sqrt{h})$而非$(w,h)$来计算误差.</li>
<li>每个格子里都有多个 Bounding Box, 但是在训练的时候我们希望对于每个物体只有一个 Bounding Box Predictor. 因此就选择与 ground truth 的 IoU 最大的那个, 称对该 box 对 该 object “负责” (responsible).</li>
</ul>
<p>最终整个的 loss function 如下:</p>
<p> <img src="/images/YOLO_loss_function.png" alt="YOLO_loss_function"></p>
<ul>
<li>$1^{obj}_{ij}$代表第$i$个网格中的第$j$个 box 是否对此 object “负责”, $1^{obj}_i$表示第$i$个网格中是否有 object.</li>
<li>该损失函数仅仅对有物体的网格的分类误差, 以及对 ground truth box 负责的 box 的位置误差进行惩罚</li>
</ul>
</li>
<li><p>另外还采用了 Dropout 和 Data Augmentation 的方法来增强泛化能力.</p>
<ul>
<li>$Dropout = 0.5$</li>
<li>对图像进行最大$20\%$的随机缩放和平移变换, 同时还有最大$1.5$的曝光与色调变换</li>
</ul>
</li>
</ul>
<h2 id="Limitations-of-YOLO"><a href="#Limitations-of-YOLO" class="headerlink" title="Limitations of YOLO"></a>Limitations of YOLO</h2><ul>
<li>由于 YOLO 每个网格只有 $B$ 个Bounding Box与1个 Class, 因此限制了临近物体检测到的个数</li>
<li>泛化能力不够, 由于降采样比较多导致只能用比较粗的特征</li>
<li>损失函数主要来源还是定位误差, 在对大小物体的位置误差的均衡上还需要改进.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前几天发烧流鼻涕, 睡不了觉, 因此就熬夜读完了 YOLO 的论文. 可以说, YOLO 的实现方式相较于之前 R-CNN 一系的 Region Proposal 的方法来说, 很有新意. YOLO 将 Classification 和 Bounding Box Regression 合起来放进了 CNN 的输出层里面, 从而大大加快了速度.&lt;/p&gt;
    
    </summary>
    
      <category term="Thesis Notes" scheme="http://www.yuthon.com/categories/Thesis-Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="YOLO" scheme="http://www.yuthon.com/tags/YOLO/"/>
    
      <category term="Object detection" scheme="http://www.yuthon.com/tags/Object-detection/"/>
    
      <category term="Darknet" scheme="http://www.yuthon.com/tags/Darknet/"/>
    
  </entry>
  
  <entry>
    <title>Train YOLO on our own dataset</title>
    <link href="http://www.yuthon.com/2016/11/12/Train-YOLO-on-our-own-dataset/"/>
    <id>http://www.yuthon.com/2016/11/12/Train-YOLO-on-our-own-dataset/</id>
    <published>2016-11-12T03:20:22.000Z</published>
    <updated>2016-11-18T07:39:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前到手 TX1 之后试了一下 YOLO 的 Demo, 感觉很是不错, 帧数勉强达到实时要求, 因此就萌生了使用自己的数据集来训练看看效果的想法. </p>
<a id="more"></a>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><h3 id="Get-The-ImageNet-Data"><a href="#Get-The-ImageNet-Data" class="headerlink" title="Get The ImageNet Data"></a>Get The ImageNet Data</h3><p>为了最大限度地利用资源 (其实是为了偷懒, 但是之后发现给自己挖了个大坑), 我用的是从 ImageNet 上的图片与 Bounding Box 标注. 本次使用了两个类别, 分别是 <a href="http://imagenet.stanford.edu/synset?wnid=n04254680#" target="_blank" rel="external">ball</a> 和 <a href="http://imagenet.stanford.edu/synset?wnid=n03820318" target="_blank" rel="external">goal</a>.</p>
<blockquote>
<ul>
<li>在<code>Downloads</code>内可以可以下到<code>images in the synset</code>以及<code>Bounding Boxes</code></li>
<li>ImageNet 里的图片看起来多, 实际上摊到每个子类上的就1000多张, 能下下来的就500多, 能直接和 Bounding Box 标注匹配的只剩此案100多了TAT. 果然还是需要自己标注, 自力更生.</li>
<li>另外 ImageNet 上的 Bounding Box 信息只有当前类别的. 比如说我下了 goal 的 Bounding Box, 其实某张图片里还有 ball 等 Object, 但是并不会被标出来. 这对于之后的训练有一定影响.</li>
<li>注意如果要从 ImageNet 上下原始图片的话是需要注册账号, 并且通过邮箱认证的 (还不能是 Gmail 这类的可以免费注册的邮箱, 需要机构或者学校邮箱才行).</li>
</ul>
</blockquote>
<p>下好的文件结构如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── Annotation</div><div class="line">│   ├── n03820318</div><div class="line">│   └── n04254680</div><div class="line">└── images</div><div class="line">    ├── n03820318</div><div class="line">    └── n04254680</div></pre></td></tr></table></figure>
<p>其中<code>Annotation</code>目录下放标注, <code>images</code>目录下放图片.</p>
<h3 id="Convert-labels-for-darknet"><a href="#Convert-labels-for-darknet" class="headerlink" title="Convert labels for darknet"></a>Convert labels for darknet</h3><p>ImageNet 上下下来的 Bounding Box 信息是 Pascal VOC 的 xml 格式:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">annotation</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">folder</span>&gt;</span>n03820318<span class="tag">&lt;/<span class="name">folder</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">filename</span>&gt;</span>n03820318_101<span class="tag">&lt;/<span class="name">filename</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">source</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">database</span>&gt;</span>ImageNet database<span class="tag">&lt;/<span class="name">database</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">source</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">size</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">width</span>&gt;</span>500<span class="tag">&lt;/<span class="name">width</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">height</span>&gt;</span>333<span class="tag">&lt;/<span class="name">height</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">depth</span>&gt;</span>3<span class="tag">&lt;/<span class="name">depth</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">size</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">segmented</span>&gt;</span>0<span class="tag">&lt;/<span class="name">segmented</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">object</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>n03820318<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">pose</span>&gt;</span>Unspecified<span class="tag">&lt;/<span class="name">pose</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">truncated</span>&gt;</span>0<span class="tag">&lt;/<span class="name">truncated</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">difficult</span>&gt;</span>0<span class="tag">&lt;/<span class="name">difficult</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">bndbox</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">xmin</span>&gt;</span>19<span class="tag">&lt;/<span class="name">xmin</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">ymin</span>&gt;</span>43<span class="tag">&lt;/<span class="name">ymin</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">xmax</span>&gt;</span>499<span class="tag">&lt;/<span class="name">xmax</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">ymax</span>&gt;</span>214<span class="tag">&lt;/<span class="name">ymax</span>&gt;</span></div><div class="line">		<span class="tag">&lt;/<span class="name">bndbox</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">object</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">annotation</span>&gt;</span></div></pre></td></tr></table></figure>
<p>而 darknet 需要的标注文件是 txt 格式:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;object-class&gt; &lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt;</div></pre></td></tr></table></figure>
<p>于是就需要对于 labels 进行转换. 我写了<a href="https://github.com/corenel/darknet/blob/yuthon/scripts/imagenet_bb_label.py" target="_blank" rel="external">一份 Python 脚本</a>, 将其放在数据集的根目录下执行即可:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ python imagenet_bb_label.py</div></pre></td></tr></table></figure>
<p>之后得到目录结构如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── Annotation</div><div class="line">│   ├── n03820318</div><div class="line">│   └── n04254680</div><div class="line">├── imagenet_bb_label.py</div><div class="line">├── images</div><div class="line">│   ├── n03820318</div><div class="line">│   └── n04254680</div><div class="line">├── labels</div><div class="line">│   ├── n03820318</div><div class="line">│   └── n04254680</div><div class="line">└── train.txt</div></pre></td></tr></table></figure>
<p>其中, <code>labels</code>目录保存着转换后的 Bounding Box 信息, <code>train.txt</code>则包含了所有图片文件的绝对路径.</p>
<blockquote>
<p>ImageNet 的 xml 文件里 object 的名字是类似<code>n03820318</code>这种格式的, 如果需要转成<code>goal</code>这样的话可以再目录下执行以下命令来批量替换:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; $ find . -name <span class="string">"*.xml"</span> -print | xargs sed -i <span class="string">'s/&lt;name&gt;n03820318/&lt;name&gt;goal/g'</span></div><div class="line">&gt;</div></pre></td></tr></table></figure>
</blockquote>
<h2 id="Modify-darknet"><a href="#Modify-darknet" class="headerlink" title="Modify darknet"></a>Modify darknet</h2><p>由于 class 的数量和名字都变了, 因此需要修改下 YOLO 的源代码.</p>
<p>首先是从 clone repository. 可以选择 clone <a href="https://github.com/pjreddie/darknet" target="_blank" rel="external">官方的</a>, 也可以直接下<a href="https://github.com/corenel/darknet" target="_blank" rel="external">我修改好的</a>. 此处里官方 repo 为例.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/pjreddie/darknet.git</div></pre></td></tr></table></figure>
<p>由于最新的 commit 修改了 label image 的显示方法, 并且改变了源文件里类别的定义, 因此需要先切回之前的 commit:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git checkout 73f7aacf35ec9b1d0f9de9ddf38af0889f213e99</div></pre></td></tr></table></figure>
<p>首先修改<code>Makefile</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">GPU=1</div><div class="line">CUDNN=1</div><div class="line">OPENCV=1</div></pre></td></tr></table></figure>
<p>之后是<code>src/yolo.c</code>, 主要是类别名称和数量, 以及<code>train.txt</code>与<code>backup</code>的地址.(<code>backup</code>目录用来存放训练得到的weights)</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// ...</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> NUM_CLASS 2</span></div><div class="line"><span class="keyword">char</span> *voc_names[] = &#123;<span class="string">"ball"</span>, <span class="string">"goal"</span>&#125;;</div><div class="line">image voc_labels[NUM_CLASS];</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">train_yolo</span><span class="params">(<span class="keyword">char</span> *cfgfile, <span class="keyword">char</span> *weightfile)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">char</span> *train_images = <span class="string">"/home/m/workspace/dataset/train.txt"</span>;</div><div class="line">    <span class="keyword">char</span> *backup_directory = <span class="string">"/home/m/workspace/backup/"</span>;</div><div class="line">&#125;</div><div class="line"><span class="comment">// ...</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">test_yolo</span><span class="params">(<span class="keyword">char</span> *cfgfile, <span class="keyword">char</span> *weightfile, <span class="keyword">char</span> *filename, <span class="keyword">float</span> thresh)</span></span></div><div class="line">&#123;</div><div class="line">  <span class="comment">// ...</span></div><div class="line">  draw_detections(im, l.side*l.side*l.n, thresh, boxes, probs, voc_names, voc_labels, NUM_CLASS);</div><div class="line">  <span class="comment">// ...</span></div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">run_yolo</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></div><div class="line">&#123;</div><div class="line">  <span class="comment">// ...</span></div><div class="line">  <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; NUM_CLASS; ++i)&#123;</div><div class="line">    <span class="comment">// ...</span></div><div class="line">  &#125;</div><div class="line">  <span class="comment">// ...</span></div><div class="line">  <span class="keyword">else</span> <span class="keyword">if</span>(<span class="number">0</span>==<span class="built_in">strcmp</span>(argv[<span class="number">2</span>], <span class="string">"demo"</span>)) demo(cfg, weights, thresh, cam_index, filename, voc_names, voc_labels, NUM_CLASS, frame_skip);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>接着是<code>yolo_kernels.cu</code>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// ...</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> NUM_CLASS 2</span></div><div class="line"><span class="comment">// ...</span></div><div class="line"><span class="function"><span class="keyword">void</span> *<span class="title">detect_in_thread</span><span class="params">(<span class="keyword">void</span> *ptr)</span></span></div><div class="line">&#123;</div><div class="line">  <span class="comment">// ...</span></div><div class="line">  draw_detections(det, l.side*l.side*l.n, demo_thresh, boxes, probs, voc_names, voc_labels, NUM_CLASS);</div><div class="line">  <span class="comment">// ...</span></div><div class="line">&#125;</div><div class="line"><span class="comment">// ...</span></div></pre></td></tr></table></figure>
<p>然后是<code>cfg</code>(建议新建一个, 我的<a href="https://github.com/corenel/darknet/blob/yuthon/cfg/tiny-yolo.train.cfg" target="_blank" rel="external">配置</a>可作为参考):</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># ...</span></div><div class="line"><span class="section">[connected]</span></div><div class="line"><span class="comment"># output = Side x Side x (2x5 + class_num)</span></div><div class="line"><span class="attr">output</span>= <span class="number">588</span></div><div class="line"><span class="attr">activation</span>=linear</div><div class="line"><span class="section"></span></div><div class="line">[detection]</div><div class="line"><span class="comment"># modify the class num</span></div><div class="line"><span class="attr">classes</span>=<span class="number">2</span></div><div class="line"><span class="attr">coords</span>=<span class="number">4</span></div><div class="line"><span class="attr">rescore</span>=<span class="number">1</span></div><div class="line"><span class="attr">side</span>=<span class="number">7</span></div><div class="line"><span class="attr">num</span>=<span class="number">2</span></div><div class="line"><span class="attr">softmax</span>=<span class="number">0</span></div><div class="line"><span class="attr">sqrt</span>=<span class="number">1</span></div><div class="line"><span class="attr">jitter</span>=.<span class="number">2</span></div><div class="line"></div><div class="line"><span class="attr">object_scale</span>=<span class="number">1</span></div><div class="line"><span class="attr">noobject_scale</span>=.<span class="number">5</span></div><div class="line"><span class="attr">class_scale</span>=<span class="number">1</span></div><div class="line"><span class="attr">coord_scale</span>=<span class="number">5</span></div></pre></td></tr></table></figure>
<p>最后, 如果使用了新的 class 的话, 需要在<code>data/labels</code>里修改<code>make_labels.py</code>并执行来生成新的 label image:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line">l = [<span class="string">"ball"</span>, <span class="string">"goal"</span>]</div><div class="line"></div><div class="line"><span class="keyword">for</span> word <span class="keyword">in</span> l:</div><div class="line">    os.system(<span class="string">"convert -fill black -background white -bordercolor white -border 4 -font ubuntu-mono -pointsize 18 label:\"%s\" \"%s.png\""</span>%(word, word))</div></pre></td></tr></table></figure>
<p>至此前期准备完成, 可以开始训练了.</p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>首先还需要下载 pre-trained weights.</p>
<ul>
<li>全尺寸的 YOLO 使用<a href="http://pjreddie.com/media/files/extraction.conv.weights" target="_blank" rel="external">这个</a></li>
<li>tiny-YOLO 使用<a href="http://pjreddie.com/media/files/darknet.conv.weights" target="_blank" rel="external">这个</a></li>
</ul>
<p>之后就是慢慢训练之路了:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">cd</span> darknet</div><div class="line">$ make -j8</div><div class="line">$ ./darknet yolo train cfg/tiny-yolo.train.cfg darknet.conv.weights</div></pre></td></tr></table></figure>
<p>我用上述的 dataset 训练 tiny-YOLO, 从 22:43 一直到 05:12, 总计 6 个小时左右, 最终得到<code>tiny-yolo_final.weights</code>文件.</p>
<p>之后就可以拿来 test 或者 demo 了:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./darknet yolo <span class="built_in">test</span> cfg/tiny-yolo.train.cfg tiny-yolo_final.weights</div></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./darknet yolo demo cfg/tiny-yolo.train.cfg tiny-yolo_final.weights</div></pre></td></tr></table></figure>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p><img src="/images/yolo-tiny_on_TX1_ball.png" alt="yolo-tiny_on_TX1_ball"></p>
<p><img src="/images/yolo-tiny_on_TX1_goal.png" alt="yolo-tiny_on_TX1_goal"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://guanghan.info/blog/en/my-works/train-yolo/" target="_blank" rel="external">Start Training YOLO with Our Own Data</a></li>
<li><a href="http://pjreddie.com/darknet/yolo/#train" target="_blank" rel="external">Training YOLO</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前到手 TX1 之后试了一下 YOLO 的 Demo, 感觉很是不错, 帧数勉强达到实时要求, 因此就萌生了使用自己的数据集来训练看看效果的想法. &lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="YOLO" scheme="http://www.yuthon.com/tags/YOLO/"/>
    
      <category term="Object detection" scheme="http://www.yuthon.com/tags/Object-detection/"/>
    
      <category term="Darknet" scheme="http://www.yuthon.com/tags/Darknet/"/>
    
  </entry>
  
  <entry>
    <title>YOLO on NVIDIA Jetson TX1</title>
    <link href="http://www.yuthon.com/2016/11/10/YOLO-on-NVIDIA-Jetson-TX1/"/>
    <id>http://www.yuthon.com/2016/11/10/YOLO-on-NVIDIA-Jetson-TX1/</id>
    <published>2016-11-10T12:36:34.000Z</published>
    <updated>2016-11-12T03:21:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>实验室昨天到了 NVIDIA 的 <a href="http://www.nvidia.com/object/jetson-tx1-module.html" target="_blank" rel="external">Jetson TX1</a>, 可以说是移动端比较好的带GPU的开发板子了, 于是可以试试在移动端上用YOLO (You Look Only Once) 来做目标识别.</p>
<a id="more"></a>
<h2 id="Specifications"><a href="#Specifications" class="headerlink" title="Specifications"></a>Specifications</h2><table>
<thead>
<tr>
<th>GPU</th>
<th>1 TFLOP/s 256-core with <a href="https://developer.nvidia.com/maxwell-compute-architecture" target="_blank" rel="external">NVIDIA Maxwell™ Architecture</a></th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>64-bit ARM® A57 CPUs</td>
</tr>
<tr>
<td>Memory</td>
<td>4 GB LPDDR4, 25.6 GB/s</td>
</tr>
<tr>
<td>Video decode</td>
<td>4K 60 Hz</td>
</tr>
<tr>
<td>Video encode</td>
<td>4K 30 Hz</td>
</tr>
<tr>
<td>CSI</td>
<td>Up to 6 cameras, 1400 Mpix/s</td>
</tr>
<tr>
<td>Display</td>
<td>2x DSI, 1x eDP 1.4, 1x DP 1.2/HDMI</td>
</tr>
<tr>
<td>Connectivity</td>
<td>Connects to 802.11ac Wi-Fi and Bluetooth-enabled devices</td>
</tr>
<tr>
<td>Networking</td>
<td>1 Gigabit Ethernet</td>
</tr>
<tr>
<td>PCIE</td>
<td>Gen 2 1x1 + 1x4</td>
</tr>
<tr>
<td>Storage</td>
<td>16 GB eMMC, SDIO, SATA</td>
</tr>
<tr>
<td>Other</td>
<td>3x UART, 3x SPI, 4x I2C, 4x I2S, GPIOs</td>
</tr>
</tbody>
</table>
<blockquote>
<p>标称1TFlops这个比较猛, 都快比得上XPS 15 9550的GTX960M了.</p>
</blockquote>
<h2 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h2><p>到手TX1之后发现是 Ubuntu 14.04 32-bit 的, 果断先用 <a href="https://developer.nvidia.com/embedded/jetpack" target="_blank" rel="external">JetPack 2.3</a> 升级到 Ubuntu 16.04 64bit. 用 JetPack 刷机的好处是能够顺便配置一大堆库, 比如说 CUDA, cuDNN, OpenCV4Terga 之类的.</p>
<ul>
<li><p>JetPack 在刷机之前需要下载一大堆 Package, 因此在国内的话最好在运行前配置好代理.</p>
</li>
<li><p>JetPack 刷完系统后会要求按 reset 键重启进 GUI, 之后就是不断地安装包安装依赖的过程, 因此在国内的话可以趁此机会修改<code>/etc/apt/source.list</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-updates main restricted universe multiverse</div><div class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-updates main restricted universe multiverse</div><div class="line">deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-security main restricted universe multiverse</div><div class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-security main restricted universe multiverse</div><div class="line">deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-backports main restricted universe multiverse</div><div class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-backports main restricted universe multiverse</div><div class="line">deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial main universe restricted</div><div class="line">deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial main universe restricted</div><div class="line">deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial universe</div></pre></td></tr></table></figure>
<blockquote>
<p>注意arm64的源与普通的x86-64的源是不一样的.</p>
</blockquote>
</li>
</ul>
<h2 id="Darknet"><a href="#Darknet" class="headerlink" title="Darknet"></a>Darknet</h2><p>为了用 Webcam demo, 所以需要 Compiling with CUDA and OpenCV:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/pjreddie/darknet.git</div><div class="line">$ <span class="built_in">cd</span> darknet</div><div class="line">$ sed <span class="string">'s/GPU=0/GPU=1/g'</span> Makefile</div><div class="line">$ sed <span class="string">'s/CUDNN=0/CUDNN=1/g'</span> Makefile</div><div class="line">$ sed <span class="string">'s/OPENCV=0/OPENCV=1/g'</span> Makefile</div><div class="line">$ make -j4</div></pre></td></tr></table></figure>
<p>上面编译完了之后输入以下指令, 与输出结果相对应, 那就说明成功了</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ ./darknet</div><div class="line">$ usage: ./darknet &lt;<span class="keyword">function</span>&gt;</div></pre></td></tr></table></figure>
<h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><p>先去下训练好的<a href="http://pjreddie.com/darknet/yolo/#models" target="_blank" rel="external">权重</a>, 建议选 yolo-tiny 的, 吃内存少. (毕竟 TX1 只有 4GB 内存, 还是 CPU 和 GPU 共用的)</p>
<p>之后运行一下命令即可测试 Real-Time Detection on a Webcam:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./darknet yolo demo cfg/tiny-yolo.cfg tiny-yolo.weights</div></pre></td></tr></table></figure>
<p>实际效果如下:</p>
<p> <img src="/images/yolo-tiny_on_TX1.png" alt="yolo-tiny_on_TX1"></p>
<p>左下为摄像头实拍屏幕的画面, 可以看出检测结果还是很不错的.</p>
<p>帧数有12fps左右, 基本上达到实时要求.</p>
<h2 id="Re-train"><a href="#Re-train" class="headerlink" title="Re-train"></a>Re-train</h2><p>重新训练 YOLO, 使其识别球与球门.</p>
<p>(To be continued…)</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="external">YOLO: Real-Time Object Detection</a></li>
<li><a href="http://guanghan.info/blog/en/my-works/train-yolo/" target="_blank" rel="external">Start Training YOLO with Our Own Data</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;实验室昨天到了 NVIDIA 的 &lt;a href=&quot;http://www.nvidia.com/object/jetson-tx1-module.html&quot;&gt;Jetson TX1&lt;/a&gt;, 可以说是移动端比较好的带GPU的开发板子了, 于是可以试试在移动端上用YOLO (You Look Only Once) 来做目标识别.&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="YOLO" scheme="http://www.yuthon.com/tags/YOLO/"/>
    
      <category term="Object detection" scheme="http://www.yuthon.com/tags/Object-detection/"/>
    
      <category term="Darknet" scheme="http://www.yuthon.com/tags/Darknet/"/>
    
      <category term="NVIDIA Jetson TX1" scheme="http://www.yuthon.com/tags/NVIDIA-Jetson-TX1/"/>
    
  </entry>
  
  <entry>
    <title>Solution for &#39;import tensorflow&#39; error in REPL on macOS</title>
    <link href="http://www.yuthon.com/2016/11/02/Solution-for-import-tensorflow-error-in-REPL-on-macOS/"/>
    <id>http://www.yuthon.com/2016/11/02/Solution-for-import-tensorflow-error-in-REPL-on-macOS/</id>
    <published>2016-11-02T03:32:01.000Z</published>
    <updated>2016-11-02T03:44:04.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>使用 pip 安装 Tensorflow 之后, 在 REPL 中执行<code>import tensorflow as tf</code>之后, 报出以下错误:</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">Python <span class="number">2.7</span><span class="number">.10</span> (default, Oct <span class="number">23</span> <span class="number">2015</span>, <span class="number">19</span>:<span class="number">19</span>:<span class="number">21</span>)</div><div class="line">[GCC <span class="number">4.2</span><span class="number">.1</span> Compatible Apple LLVM <span class="number">7.0</span><span class="number">.0</span> (clang<span class="number">-700.0</span><span class="number">.59</span><span class="number">.5</span>)] on darwin</div><div class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> <span class="keyword">or</span> <span class="string">"license"</span> <span class="keyword">for</span> more information.</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">Traceback (most recent call last):</div><div class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/__init__.py"</span>, line <span class="number">23</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    <span class="keyword">from</span> tensorflow.python <span class="keyword">import</span> *</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py"</span>, line <span class="number">53</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    <span class="keyword">from</span> tensorflow.core.framework.graph_pb2 <span class="keyword">import</span> *</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/core/framework/graph_pb2.py"</span>, line <span class="number">16</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    <span class="keyword">from</span> tensorflow.core.framework <span class="keyword">import</span> node_def_pb2 <span class="keyword">as</span> tensorflow_dot_core_dot_framework_dot_node__def__pb2</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/core/framework/node_def_pb2.py"</span>, line <span class="number">16</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    <span class="keyword">from</span> tensorflow.core.framework <span class="keyword">import</span> attr_value_pb2 <span class="keyword">as</span> tensorflow_dot_core_dot_framework_dot_attr__value__pb2</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py"</span>, line <span class="number">16</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    <span class="keyword">from</span> tensorflow.core.framework <span class="keyword">import</span> tensor_pb2 <span class="keyword">as</span> tensorflow_dot_core_dot_framework_dot_tensor__pb2</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/core/framework/tensor_pb2.py"</span>, line <span class="number">16</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    <span class="keyword">from</span> tensorflow.core.framework <span class="keyword">import</span> tensor_shape_pb2 <span class="keyword">as</span> tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2</div><div class="line">  File <span class="string">"/Library/Python/2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py"</span>, line <span class="number">22</span>, <span class="keyword">in</span> &lt;module&gt;</div><div class="line">    serialized_pb=_b(<span class="string">'\n,tensorflow/core/framework/tensor_shape.proto\x12\ntensorflow\"z\n\x10TensorShapeProto\x12-\n\x03\x64im\x18\x02 \x03(\x0b\x32 .tensorflow.TensorShapeProto.Dim\x12\x14\n\x0cunknown_rank\x18\x03 \x01(\x08\x1a!\n\x03\x44im\x12\x0c\n\x04size\x18\x01 \x01(\x03\x12\x0c\n\x04name\x18\x02 \x01(\tB2\n\x18org.tensorflow.frameworkB\x11TensorShapeProtosP\x01\xf8\x01\x01\x62\x06proto3'</span>)</div><div class="line">TypeError: __init__() got an unexpected keyword argument <span class="string">'syntax'</span></div></pre></td></tr></table></figure>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>经检查是<code>protobuf</code>的锅. TF 需要<code>protobuf&gt;=3.0.0a3</code>, 而macOS里似乎有两份<code>protobuf</code>, 一份是之前装的2.6.1, 另外一份是随着 TF 装的. 默认似乎是调用到了 2.6.1 的那个版本.</p>
<p>找到原因就好办了, 卸掉重装呗:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ sudo pip uninstall protobuf</div><div class="line">$ sudo pip uninstall tensorflow</div><div class="line">$ brew uninstall protobuf</div><div class="line">$ sudo pip install --upgrade $TF_BINARY_URL</div></pre></td></tr></table></figure>
<p>之后就好了, 确认一下是不是调用了3.0.0的版本:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> google.protobuf</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>google.protobuf.__file__</div><div class="line"><span class="string">'/Library/Python/2.7/site-packages/google/protobuf/__init__.pyc'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>google.protobuf.__version__</div><div class="line"><span class="string">'3.0.0'</span></div></pre></td></tr></table></figure>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://stackoverflow.com/questions/33622842/error-in-python-after-import-tensorflow-typeerror-init-got-an-unexpect" target="_blank" rel="external">Error in python after ‘import tensorflow’: TypeError: <strong>init</strong>() got an unexpected keyword argument ‘syntax’</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Description&quot;&gt;&lt;a href=&quot;#Description&quot; class=&quot;headerlink&quot; title=&quot;Description&quot;&gt;&lt;/a&gt;Description&lt;/h2&gt;&lt;p&gt;使用 pip 安装 Tensorflow 之后, 在 REPL 中执行&lt;code&gt;import tensorflow as tf&lt;/code&gt;之后, 报出以下错误:&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="macOS" scheme="http://www.yuthon.com/tags/macOS/"/>
    
      <category term="Tensorflow" scheme="http://www.yuthon.com/tags/Tensorflow/"/>
    
      <category term="Python" scheme="http://www.yuthon.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Solution for matplotlib importing error on macOS</title>
    <link href="http://www.yuthon.com/2016/11/01/Solution-for-matplotlib-importing-error-on-Mac-OS-X/"/>
    <id>http://www.yuthon.com/2016/11/01/Solution-for-matplotlib-importing-error-on-Mac-OS-X/</id>
    <published>2016-11-01T13:57:58.000Z</published>
    <updated>2016-11-02T03:34:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天在做 CS231n 的 Assignment #2 的时候遇到了导入 matplotlib.pyplot 的问题, 特此记录.</p>
<h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>打开 IPython Notebook 之后, 执行以下命令:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div></pre></td></tr></table></figure>
<p>出现错误:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ValueError: unknown locale: UTF<span class="number">-8</span></div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>把下面这些加到<code>~/.zshrc</code>或者是<code>~/.bash_profile</code>里面:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export LC_ALL=en_US.UTF-8</div><div class="line">export LANG=en_US.UTF-8</div></pre></td></tr></table></figure>
<p>同时, 如果用 iTerm 的话, 还需要在 Preference -&gt; Profiles -&gt; Terminal -&gt; Environment 中, 取消选择 <code>Set locale variables automatically</code>.</p>
<p><strong>然而</strong>又出现了下列报错:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ImportError: cannot <span class="keyword">import</span> name _thread</div></pre></td></tr></table></figure>
<p>这个问题已经在最新的<code>six</code>和<code>dateutil</code>库中解决了, 然而 macOS 本身却还在使用旧版本的库. 解决方法如下:</p>
<ul>
<li><p>执行以下命令安装最新版本的<code>six</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo pip install six -U</div></pre></td></tr></table></figure>
</li>
<li><p>开 python 看看是否还在使用旧版本的库:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> six</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>six.__file__</div><div class="line">/System/Library/Frameworks/Python.framework/Versions/<span class="number">2.7</span>/Extras/lib/python/six.pyc</div></pre></td></tr></table></figure>
<p>显然确实是这样</p>
</li>
<li><p>删除旧版本的库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ rm -rf /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/six.*</div></pre></td></tr></table></figure>
<p>这样就可以了, 之后 python 会使用我们之前新装的版本的<code>six</code>库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> six</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>six.__file__</div><div class="line"><span class="string">'/Library/Python/2.7/site-packages/six.pyc'</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p>之后再执行<code>import matplotlib.pyplot as plt</code>之后就没问题了.</p>
<p>说到底还是 macOS 的锅…</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><p><a href="https://coderwall.com/p/-k_93g/mac-os-x-valueerror-unknown-locale-utf-8-in-python" target="_blank" rel="external">Mac OS X: ValueError: unknown locale: UTF-8 in Python</a></p>
</li>
<li><p><a href="http://stackoverflow.com/questions/27630114/matplotlib-issue-on-os-x-importerror-cannot-import-name-thread" target="_blank" rel="external">Matplotlib issue on OS X (“ImportError: cannot import name _thread”)</a></p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天在做 CS231n 的 Assignment #2 的时候遇到了导入 matplotlib.pyplot 的问题, 特此记录.&lt;/p&gt;
&lt;h2 id=&quot;Description&quot;&gt;&lt;a href=&quot;#Description&quot; class=&quot;headerlink&quot; title=&quot;Description&quot;&gt;&lt;/a&gt;Description&lt;/h2&gt;&lt;p&gt;打开 IPython Notebook 之后, 执行以下命令:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;出现错误:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;ValueError: unknown locale: UTF&lt;span class=&quot;number&quot;&gt;-8&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="macOS" scheme="http://www.yuthon.com/tags/macOS/"/>
    
      <category term="Python" scheme="http://www.yuthon.com/tags/Python/"/>
    
      <category term="Matplotlib" scheme="http://www.yuthon.com/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>Thesis Notes for SLIC</title>
    <link href="http://www.yuthon.com/2016/11/01/Thesis-Notes-for-SLIC/"/>
    <id>http://www.yuthon.com/2016/11/01/Thesis-Notes-for-SLIC/</id>
    <published>2016-11-01T10:21:04.000Z</published>
    <updated>2016-11-19T07:22:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章介绍了当前 State-of-the-Art 的5种<strong>超像素 (Superpixel)</strong> 的算法, 并主要从其对于图像边缘信息的拟合程度 (their ability to adhere to image boundaries), 速度, 内存利用效率, 以及它们对于图像分割性能的影响 (their impact on segmentation performance) 来综合评价.</p>
<p>同时, 本文还提出了一种 <strong>SLIC (simple linear iterative clustering)</strong> 的算法, 用的是 k-means clustering 的方法.</p>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><strong>超像素算法</strong>主要是将一幅图像中, 具有相似颜色, 纹理或者亮度等信息的相邻的像素点聚集起来, 组成一些具有一定视觉意义的像素块, 为后续处理做准备. 这种算法用少量的超像素来代替原来的图像像素, 能够减少图像的冗余度, 为计算图像特征做了很好的铺垫, 并且显著地降低了随后的图像处理步骤的复杂度. 超像素算法作为一个<strong>预处理</strong>的步骤, 现在已经成为了很多计算机视觉算法中的重要一环, 比如说图像分割 (image segmentation), 深度估计(depth estimation), 姿态预估 (body model estimation) 和目标定位 (object localization) 这些领域.</p>
<p><img src="/images/Images_segmented_using_SLIC_into_superpixels.png" alt="Images_segmented_using_SLIC_into_superpixels"></p>
<p>生成超像素的方法有很多, 各自有各自的优点和缺陷. 在此主要考虑以下方面来评测对比这些算法:</p>
<ul>
<li>超像素必须<u>保持图像的边缘信息</u> (Superpixels should adhere well to image boundaries.)</li>
<li>如果超像素是作为一个预处理步骤, 来减少计算复杂度的, 那么就需要考虑<u>计算速度, 内存效率以及是否方便使用</u>等因素 (When used to reduce computational complexity as a pre- processing step, superpixels should be fast to compute, memory efficient, and simple to use.)</li>
<li>同时, 如果超像素算法是为了用来给之后的分割做准备的, 那么就要考虑它<u>是否既能加快速度, 又能提升分割的效果</u>. (When used for segmentation purposes, superpixels should both increase the speed and improve the quality of the results.)</li>
</ul>
<h2 id="Existing-Superpixel-Methods"><a href="#Existing-Superpixel-Methods" class="headerlink" title="Existing Superpixel Methods"></a>Existing Superpixel Methods</h2><h3 id="Graph-Based-Algorithms"><a href="#Graph-Based-Algorithms" class="headerlink" title="Graph-Based Algorithms"></a>Graph-Based Algorithms</h3><p>这类算法主要就是把整个图像看成一张图, 各个像素是节点, 相邻像素之间的相似度作为边上的权值. 超像素就是根据最小化损失函数来构建的.</p>
<p>相关的方法有:</p>
<ul>
<li><strong><a href="https://www.cs.sfu.ca/research/groups/VML/pubs/mori-model_search_segmentation-iccv05.pdf" target="_blank" rel="external">NC05</a></strong>: 对边缘的保持不好. 时间复杂度$O(N^{\frac{1}{2}})$<ul>
<li>The Normalized cuts algorithm recursively partitions a graph of all pixels in the image using contour and texture cues, globally minimizing a cost function defined on the edges at the partition boundaries.</li>
</ul>
</li>
<li><strong><a href="http://link.springer.com/article/10.1023/B:VISI.0000022288.19776.77" target="_blank" rel="external">GS04</a></strong>: 对边缘保持较好, 但是生成的超像素大小与形状不规则, 不能严格控制超像素的个数. 时间复杂度$O(NlogN)$<ul>
<li>It performs an agglomerative clustering of pixels as nodes on a graph such that each superpixel is the minimum spanning tree of the constituent pixels.</li>
</ul>
</li>
<li><strong><a href="http://ieeexplore.ieee.org/document/4587471/?arnumber=4587471&amp;tag=1" target="_blank" rel="external">SL08</a></strong>: 时间复杂度$O(N^{\frac{3}{2}}logN)$, 但是没有算上预先生成边缘图(boundary map)所耗的时间<ul>
<li>Moore et al. propose a method to generate superpixels that conform to a grid by finding optimal paths, or seams, that split the image into smaller vertical or horizontal regions. Optimal paths are found using a graph cuts method similar to Seam Carving.</li>
</ul>
</li>
<li><strong><a href="http://link.springer.com/chapter/10.1007/978-3-642-15555-0_16" target="_blank" rel="external">GCa10 &amp; GCb10</a></strong>: Veksler et al. use a global optimization approach similar to the texture synthesis work. Superpixels are obtained by stitching together overlapping image patches such that each pixel belongs to only one of the overlapping regions. They suggest two variants of their method, one for generating compact superpixels (GCa10) and one for constant- intensity superpixels (GCb10).</li>
</ul>
<h3 id="Gradient-Ascent-Based-Algorithms"><a href="#Gradient-Ascent-Based-Algorithms" class="headerlink" title="Gradient-Ascent-Based Algorithms"></a>Gradient-Ascent-Based Algorithms</h3><p>这类方法生对像素生成一个随机的初始聚类, 然后不断迭代优化, 直到满足收敛条件.</p>
<p>相关的方法有:</p>
<ul>
<li><strong><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1000236" target="_blank" rel="external">MS02</a></strong>: 一种比较老的方法, 生成的超像素形状不规整, 而且对于超像素的数量大小等均不能控制. 同时, 时间复杂度是$O(N^2)$, 非常慢.<ul>
<li>Mean shift, an iterative mode-seeking procedure for locating local maxima of a density function, is applied to find modes in the color or intensity feature space of an image. Pixels that converge to the same mode define the superpixels. MS02</li>
</ul>
</li>
<li><strong><a href="http://link.springer.com/chapter/10.1007/978-3-540-88693-8_52" target="_blank" rel="external">QS08</a></strong>: 对边界的保持比较好, 但是速度相当慢. 时间复杂度$O(dN^2)$.<ul>
<li>Quick shift also uses a mode-seeking segmentation scheme. It initializes the segmentation using a medoid shift procedure. It then moves each point in the feature space to the nearest neighbor that increases the Parzen density estimate.</li>
</ul>
</li>
<li><strong><a href="https://pdfs.semanticscholar.org/a381/9dda9a5f00dbb8cd3413ca7422e37a0d5794.pdf" target="_blank" rel="external">WS91</a></strong>: 对边界保持不好, 形状不规整, 不能严格控制超像素的数量, 但是速度快. 时间复杂度$O(NlogN)$.<ul>
<li>The watershed approach performs a gradient ascent starting from local minima to produce watersheds, lines that separate catchment basins. The </li>
</ul>
</li>
<li><strong><a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4912213" target="_blank" rel="external">TP09</a></strong>: 生成的超像素具有一致的大小, 紧凑性. 宣称时间复杂度$O(N)$, 但是实际上很慢, 而且对边界的保持不好. <ul>
<li>The Turbopixel method progressively dilates a set of seed locations using level-set-based geometric flow. The geometric flow relies on local image gradients, aiming to regularly distribute superpixels on the image plane.</li>
</ul>
</li>
</ul>
<h2 id="SLIC-Superpixels"><a href="#SLIC-Superpixels" class="headerlink" title="SLIC Superpixels"></a>SLIC Superpixels</h2><p>SLIC (simple linear iterative clustering) 算法是 k-means 在超像素生成方面的一个改写. 主要有以下特性:</p>
<ul>
<li>将搜索域限制在与超像素大小成比例的一个区域内, 从而显著减少优化过程中的距离计算量. 此举将复杂度降到了$N$的级别,并且与超像素数量$k$无关.</li>
<li>综合颜色以及空间上的临近关系来构造权边, 同时也能控制超像素的数量.</li>
</ul>
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><ol>
<li><p><strong>聚类初始化</strong>: 按照设定的超像素个数$k$, 采取步长$S=\sqrt{N/k}$, 在图像中均匀采样选取聚类中心$C_i = \begin{bmatrix} l_i &amp; a_i &amp; b_i &amp; x_i &amp; y_i \end{bmatrix}^T$. 然后在聚类中心的$3\times 3$领域内寻找梯度最小的点, 作为新的聚类中心的位置.</p>
<ul>
<li>图像色彩空间采用 CIELAB</li>
<li>步长$S=\sqrt{N/k}$是为了使得聚类中心分布均匀</li>
<li>移动聚类中心是为了避免把中心建在边缘或者噪点上</li>
</ul>
</li>
<li><p><strong>像素点分类</strong>: 将每个像素点$i$与离其最近并且搜索域覆盖到它的聚类中心相关联. 若是该像素点在多个聚类中心的搜索域内, 则取距离$D$最小的那个作为其关联的聚类中心.</p>
<ul>
<li><p>搜索域为$2S\times 2S$</p>
</li>
<li><p>将搜索域限制在一个较小的范围内, 避免了传统的 k-means 方法里每个像素点要和所有的聚类中心比较的缺点, 使得运行速度大大提高. 这也是本算法速度快的最重要的一个原因.</p>
<p> <img src="/images/reducing_the_superpixel_search_regions.png" alt="reducing_the_superpixel_search_regions"></p>
</li>
</ul>
</li>
<li><p><strong>分类更新</strong>: 在所有的像素都已经关联到聚类中心之后, 我们需要调整聚类中心的位置, 使得其位于与其关联的像素点的中心.</p>
<ul>
<li>这样的分配(assign)然后再更新(update)的步骤可以迭代多次, 直到满足收敛条件为止.</li>
<li>一般来说10次就差不多了</li>
</ul>
</li>
<li><p><strong>后续处理</strong>: 通过将不相交的点分配给相邻的超像素来增强联通性 (enforces connectivity by reassigning disjoint pixels to nearby superpixels).</p>
</li>
</ol>
<p>伪代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* Initialization */</span></div><div class="line">Initialize cluster centers C_k = [l_k, a_k, b_k, x_k, y_k] by sampling pixels at regular grid steps S.</div><div class="line">Move cluster centers to the lowest gradient position in a <span class="number">3</span>x3 neighborhood.</div><div class="line">Set label l(i) = <span class="number">-1</span> <span class="keyword">for</span> each pixel i.</div><div class="line">Set distance d(i) = \infty <span class="keyword">for</span> each pixel i</div><div class="line"></div><div class="line">repeat</div><div class="line">  <span class="comment">/* Assignment */</span></div><div class="line">  <span class="keyword">for</span> each cluster center C_k <span class="keyword">do</span></div><div class="line">    <span class="keyword">for</span> each pixel i in a <span class="number">2</span>Sx2S region around C_k <span class="keyword">do</span></div><div class="line">      Compare the distance between C_k and i.</div><div class="line">      <span class="keyword">if</span> D &lt; d_i then</div><div class="line">        <span class="built_in">set</span> d(i) = D</div><div class="line">        <span class="built_in">set</span> l(i) = k</div><div class="line">      end <span class="keyword">if</span></div><div class="line">    end <span class="keyword">for</span></div><div class="line">  end <span class="keyword">for</span></div><div class="line">  <span class="comment">/* Update */</span></div><div class="line">  Compute <span class="keyword">new</span> cluster centers.</div><div class="line">  Compute residual error E</div><div class="line">until E &lt;= threshold</div></pre></td></tr></table></figure>
<h3 id="Distance-Measure"><a href="#Distance-Measure" class="headerlink" title="Distance Measure"></a>Distance Measure</h3><p>SLIC 算法用的是 CIELAB 的色彩空间, 再加上像素本身的坐标, 因此一个像素的全部信息需要用一个5维的向量$\begin{bmatrix} l_i &amp; a_i &amp; b_i &amp; x_i &amp; y_i \end{bmatrix}^T$来表示, 也可以说是在一个$labxy$的图像空间内.</p>
<p>$(l, a, b)$表示颜色信息, $(x ,y)$表示位置信息, 因此不能简单地用5维空间的欧氏距离来衡量两个像素之间的距离. 为了统一色彩与空间上的接近程度, 需要引入归一化 (normalization).</p>
<p>$d_c = \sqrt{(l_j - l_i)^2 + (a_j - a_i)^2 + (b_j - b_i)^2}$</p>
<p>$d_s = \sqrt{(x_j - x_i)^2 + (y_j - y_i)^2}$</p>
<p>$D’ = \sqrt{(d_c / N_c)^2 + (d_s / N_s)^2}$</p>
<p>其中$N_c$与$N_s$分别代表最大的色彩与空间距离. $N_s$很好确定, 就是超像素的大小, $N_s = S = \sqrt{N/k}$. 然而$N_c$则根据每个聚类区域的不同而不同, 为了方便起见, 将其设定为一个常数值$m$. 因此距离$D$的定义即可表示为:</p>
<p>$D’ = \sqrt{(d_c / m)^2 + (d_s / S)^2}$</p>
<p>进而可简化为</p>
<p>$D = \sqrt{d_c^2 + (d_s / S)^2 m^2}$</p>
<blockquote>
<ul>
<li>$m$其实也是用来衡量色彩与空间相似度哪个更加重要的标志. $m$较大时, 表示空间相似度更为重要, 产生的超像素会更为紧凑, 区域/边缘比率较低; 当$m$较小时, 表示色彩相似度更为重要, 产生的超像素会紧贴边缘, 但是大小与形状就不会很规整. 一般来说, 使用 CIELAB 色彩空间时, $m \in [1,40]$.</li>
<li>对于灰阶图像, $d_c = \sqrt{(l_j - l_i)^2}$</li>
<li>对于三维的超体素, $d_s = \sqrt{(x_j - x_i)^2 + (y_j - y_i)^2 + (z_j - z_i)^2}$</li>
</ul>
</blockquote>
<h3 id="Postprocessing"><a href="#Postprocessing" class="headerlink" title="Postprocessing"></a>Postprocessing</h3><p>SLIC 和其他算法一样并不严格地强制保证连通性 (connectivity). 聚类完成时候, 会有一些与其聚类中心不属于同一个连通单元 (connected component) 的孤立像素存在. 为了对此做出校正, 这些像素会根据某种连通单元算法 (connected components algorithm) 来分配到另外一个最近的聚类中心上.</p>
<p>(To be continued…)</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文章介绍了当前 State-of-the-Art 的5种&lt;strong&gt;超像素 (Superpixel)&lt;/strong&gt; 的算法, 并主要从其对于图像边缘信息的拟合程度 (their ability to adhere to image boundaries), 速度, 内存利用效率, 以及它们对于图像分割性能的影响 (their impact on segmentation performance) 来综合评价.&lt;/p&gt;
&lt;p&gt;同时, 本文还提出了一种 &lt;strong&gt;SLIC (simple linear iterative clustering)&lt;/strong&gt; 的算法, 用的是 k-means clustering 的方法.&lt;/p&gt;
    
    </summary>
    
      <category term="Thesis Notes" scheme="http://www.yuthon.com/categories/Thesis-Notes/"/>
    
    
      <category term="Superpixel" scheme="http://www.yuthon.com/tags/Superpixel/"/>
    
      <category term="SLIC" scheme="http://www.yuthon.com/tags/SLIC/"/>
    
  </entry>
  
  <entry>
    <title>Notes for CS231n Recurrent Neural Network</title>
    <link href="http://www.yuthon.com/2016/10/30/Notes-for-CS231n-RNN/"/>
    <id>http://www.yuthon.com/2016/10/30/Notes-for-CS231n-RNN/</id>
    <published>2016-10-30T06:59:17.000Z</published>
    <updated>2016-11-20T13:22:44.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>从 RNN 开始, CS231n 的 Lecture Notes 就没有了, 因此我根据上课时的 Slides 整理了一些需要重视的知识点. 还可以参考这些文章或是视频来加深理解:</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/22107715?refer=intelligentunit" target="_blank" rel="external">[原创翻译]循环神经网络惊人的有效性（上）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22230074?refer=intelligentunit" target="_blank" rel="external">[原创翻译]循环神经网络惊人的有效性（下）</a></li>
<li><a href="https://www.youtube.com/watch?v=Ukgii7Yd_cU" target="_blank" rel="external">Recurrent Neural Networks (Video, recommend)</a></li>
<li><a href="http://james371507.wixsite.com/hylee/single-post/2016/03/20/Recurrent-Neural-Network-RNN-Note-of-Stanford-CS231n" target="_blank" rel="external">Recurrent Neural Network (RNN) (Note of Stanford CS231n)</a></li>
</ul>
</blockquote>
<a id="more"></a>
<h1 id="Lecture-10"><a href="#Lecture-10" class="headerlink" title="Lecture 10"></a>Lecture 10</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Recurrent Networks offer a lot of flexibility:</p>
<p> <img src="/images/RNN_flexibility.png" alt="RNN_flexibility"></p>
<ol>
<li><strong>one to one</strong>: Vanilla Neural Networks</li>
<li><strong>one to many</strong>: e.g. Image Captioning (image -&gt; sequence of words)</li>
<li><strong>many to one</strong>: e.g. Sentiment Classification (sequence of words -&gt; sentiment)</li>
<li><strong>many to many</strong>:<ul>
<li>e.g. Machine Translation (seq of words -&gt; seq of words)</li>
<li>e.g. Video classification on frame level</li>
</ul>
</li>
</ol>
<p>RNN can also do sequential precessing of fix inputs (Multiple Object Recognition with Visual Attention, Ba et al.) or fixed outputs (DRAW: A Recurrent Neural Network For Image Generation, Gregor et al.).</p>
<h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><h3 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h3><p>Usually we want to predict a vector at some time steps. To achieve this goal, we can process a sequence of vectors $x$ by applying a recurrence formula at every time step:</p>
<p> <img src="/images/RNN_concept.png" alt="RNN_concept"></p>
<blockquote>
<p>Notice: the same function and the same set of parameters are used at every time step. That’s to say, we use <strong>shared weights</strong>.</p>
</blockquote>
<p><strong>(Vanilla) Recurrent Neural Network</strong></p>
<p>The state consists of a single “hidden” vector $h$:</p>
<ul>
<li>$h_t = tanh (W_{hh} h_{t-1} + W_{xh} x_t)$</li>
<li>$y_t = W_{hy} h_t$</li>
</ul>
<h3 id="Example-Character-level-language-model"><a href="#Example-Character-level-language-model" class="headerlink" title="Example: Character-level language model"></a>Example: Character-level language model</h3><p>We have a vocabulary of four characters $\begin{bmatrix} h &amp; e &amp; l &amp; o \end{bmatrix}$, and the example training sequence is “hello”.</p>
<p><img src="/images/character-level_language_model_example.png" alt="character-level_language_model_example"></p>
<p>And we can look its <a href="https://gist.github.com/karpathy/d4dee566867f8291f086" target="_blank" rel="external">the implement</a>.</p>
<p><strong>Data I/O</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)</div><div class="line">BSD License</div><div class="line">"""</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># data I/O</span></div><div class="line">data = open(<span class="string">'input.txt'</span>, <span class="string">'r'</span>).read() <span class="comment"># should be simple plain text file</span></div><div class="line">chars = list(set(data))</div><div class="line">data_size, vocab_size = len(data), len(chars)</div><div class="line"><span class="keyword">print</span> <span class="string">'data has %d characters, %d unique.'</span> % (data_size, vocab_size)</div><div class="line">char_to_ix = &#123; ch:i <span class="keyword">for</span> i,ch <span class="keyword">in</span> enumerate(chars) &#125;</div><div class="line">ix_to_char = &#123; i:ch <span class="keyword">for</span> i,ch <span class="keyword">in</span> enumerate(chars) &#125;</div></pre></td></tr></table></figure>
<p><strong>Initializations</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># hyperparameters</span></div><div class="line">hidden_size = <span class="number">100</span> <span class="comment"># size of hidden layer of neurons</span></div><div class="line">seq_length = <span class="number">25</span> <span class="comment"># number of steps to unroll the RNN for</span></div><div class="line">learning_rate = <span class="number">1e-1</span></div><div class="line"></div><div class="line"><span class="comment"># model parameters</span></div><div class="line">Wxh = np.random.randn(hidden_size, vocab_size)*<span class="number">0.01</span> <span class="comment"># input to hidden</span></div><div class="line">Whh = np.random.randn(hidden_size, hidden_size)*<span class="number">0.01</span> <span class="comment"># hidden to hidden</span></div><div class="line">Why = np.random.randn(vocab_size, hidden_size)*<span class="number">0.01</span> <span class="comment"># hidden to output</span></div><div class="line">bh = np.zeros((hidden_size, <span class="number">1</span>)) <span class="comment"># hidden bias</span></div><div class="line">by = np.zeros((vocab_size, <span class="number">1</span>)) <span class="comment"># output bias</span></div></pre></td></tr></table></figure>
<p><strong>Main Loop</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">n, p = <span class="number">0</span>, <span class="number">0</span></div><div class="line">mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)</div><div class="line">mbh, mby = np.zeros_like(bh), np.zeros_like(by) <span class="comment"># memory variables for Adagrad</span></div><div class="line">smooth_loss = -np.log(<span class="number">1.0</span>/vocab_size)*seq_length <span class="comment"># loss at iteration 0</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">  <span class="comment"># prepare inputs (we're sweeping from left to right in steps seq_length long)</span></div><div class="line">  <span class="keyword">if</span> p+seq_length+<span class="number">1</span> &gt;= len(data) <span class="keyword">or</span> n == <span class="number">0</span>:</div><div class="line">    hprev = np.zeros((hidden_size,<span class="number">1</span>)) <span class="comment"># reset RNN memory</span></div><div class="line">    p = <span class="number">0</span> <span class="comment"># go from start of data</span></div><div class="line">  inputs = [char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> data[p:p+seq_length]]</div><div class="line">  targets = [char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> data[p+<span class="number">1</span>:p+seq_length+<span class="number">1</span>]]</div><div class="line"></div><div class="line">  <span class="comment"># sample from the model now and then</span></div><div class="line">  <span class="keyword">if</span> n % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">    sample_ix = sample(hprev, inputs[<span class="number">0</span>], <span class="number">200</span>)</div><div class="line">    txt = <span class="string">''</span>.join(ix_to_char[ix] <span class="keyword">for</span> ix <span class="keyword">in</span> sample_ix)</div><div class="line">    <span class="keyword">print</span> <span class="string">'----\n %s \n----'</span> % (txt, )</div><div class="line"></div><div class="line">  <span class="comment"># forward seq_length characters through the net and fetch gradient</span></div><div class="line">  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)</div><div class="line">  smooth_loss = smooth_loss * <span class="number">0.999</span> + loss * <span class="number">0.001</span></div><div class="line">  <span class="keyword">if</span> n % <span class="number">100</span> == <span class="number">0</span>: <span class="keyword">print</span> <span class="string">'iter %d, loss: %f'</span> % (n, smooth_loss) <span class="comment"># print progress</span></div><div class="line"></div><div class="line">  <span class="comment"># perform parameter update with Adagrad</span></div><div class="line">  <span class="keyword">for</span> param, dparam, mem <span class="keyword">in</span> zip([Wxh, Whh, Why, bh, by],</div><div class="line">                                [dWxh, dWhh, dWhy, dbh, dby],</div><div class="line">                                [mWxh, mWhh, mWhy, mbh, mby]):</div><div class="line">    mem += dparam * dparam</div><div class="line">    param += -learning_rate * dparam / np.sqrt(mem + <span class="number">1e-8</span>) <span class="comment"># adagrad update</span></div><div class="line"></div><div class="line">  p += seq_length <span class="comment"># move data pointer</span></div><div class="line">  n += <span class="number">1</span> <span class="comment"># iteration counter</span></div></pre></td></tr></table></figure>
<p><strong>Loss function</strong></p>
<ul>
<li>forward pass (compute loss)</li>
<li>backward pass (compute param gradient)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lossFun</span><span class="params">(inputs, targets, hprev)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line">  inputs,targets are both list of integers.</div><div class="line">  hprev is Hx1 array of initial hidden state</div><div class="line">  returns the loss, gradients on model parameters, and last hidden state</div><div class="line">  """</div><div class="line">  xs, hs, ys, ps = &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;</div><div class="line">  hs[<span class="number">-1</span>] = np.copy(hprev)</div><div class="line">  loss = <span class="number">0</span></div><div class="line">  <span class="comment"># forward pass</span></div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(len(inputs)):</div><div class="line">    xs[t] = np.zeros((vocab_size,<span class="number">1</span>)) <span class="comment"># encode in 1-of-k representation</span></div><div class="line">    xs[t][inputs[t]] = <span class="number">1</span></div><div class="line">    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t<span class="number">-1</span>]) + bh) <span class="comment"># hidden state</span></div><div class="line">    ys[t] = np.dot(Why, hs[t]) + by <span class="comment"># unnormalized log probabilities for next chars</span></div><div class="line">    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) <span class="comment"># probabilities for next chars</span></div><div class="line">    loss += -np.log(ps[t][targets[t],<span class="number">0</span>]) <span class="comment"># softmax (cross-entropy loss)</span></div><div class="line">  <span class="comment"># backward pass: compute gradients going backwards</span></div><div class="line">  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)</div><div class="line">  dbh, dby = np.zeros_like(bh), np.zeros_like(by)</div><div class="line">  dhnext = np.zeros_like(hs[<span class="number">0</span>])</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(len(inputs))):</div><div class="line">    dy = np.copy(ps[t])</div><div class="line">    dy[targets[t]] -= <span class="number">1</span> <span class="comment"># backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here</span></div><div class="line">    dWhy += np.dot(dy, hs[t].T)</div><div class="line">    dby += dy</div><div class="line">    dh = np.dot(Why.T, dy) + dhnext <span class="comment"># backprop into h</span></div><div class="line">    dhraw = (<span class="number">1</span> - hs[t] * hs[t]) * dh <span class="comment"># backprop through tanh nonlinearity</span></div><div class="line">    dbh += dhraw</div><div class="line">    dWxh += np.dot(dhraw, xs[t].T)</div><div class="line">    dWhh += np.dot(dhraw, hs[t<span class="number">-1</span>].T)</div><div class="line">    dhnext = np.dot(Whh.T, dhraw)</div><div class="line">  <span class="keyword">for</span> dparam <span class="keyword">in</span> [dWxh, dWhh, dWhy, dbh, dby]:</div><div class="line">    np.clip(dparam, <span class="number">-5</span>, <span class="number">5</span>, out=dparam) <span class="comment"># clip to mitigate exploding gradients</span></div><div class="line">  <span class="keyword">return</span> loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)<span class="number">-1</span>]</div></pre></td></tr></table></figure>
<p><strong>Sampling</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(h, seed_ix, n)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line">  sample a sequence of integers from the model</div><div class="line">  h is memory state, seed_ix is seed letter for first time step</div><div class="line">  """</div><div class="line">  x = np.zeros((vocab_size, <span class="number">1</span>))</div><div class="line">  x[seed_ix] = <span class="number">1</span></div><div class="line">  ixes = []</div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> xrange(n):</div><div class="line">    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)</div><div class="line">    y = np.dot(Why, h) + by</div><div class="line">    p = np.exp(y) / np.sum(np.exp(y))</div><div class="line">    ix = np.random.choice(range(vocab_size), p=p.ravel())</div><div class="line">    x = np.zeros((vocab_size, <span class="number">1</span>))</div><div class="line">    x[ix] = <span class="number">1</span></div><div class="line">    ixes.append(ix)</div><div class="line">  <span class="keyword">return</span> ixes</div></pre></td></tr></table></figure>
<p><strong>Gradient Check</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># gradient checking</span></div><div class="line"><span class="keyword">from</span> random <span class="keyword">import</span> uniform</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradCheck</span><span class="params">(inputs, target, hprev)</span>:</span></div><div class="line">  <span class="keyword">global</span> Wxh, Whh, Why, bh, by</div><div class="line">  num_checks, delta = <span class="number">10</span>, <span class="number">1e-5</span></div><div class="line">  _, dWxh, dWhh, dWhy, dbh, dby, _ = lossFun(inputs, targets, hprev)</div><div class="line">  <span class="keyword">for</span> param,dparam,name <span class="keyword">in</span> zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [<span class="string">'Wxh'</span>, <span class="string">'Whh'</span>, <span class="string">'Why'</span>, <span class="string">'bh'</span>, <span class="string">'by'</span>]):</div><div class="line">    s0 = dparam.shape</div><div class="line">    s1 = param.shape</div><div class="line">    <span class="keyword">assert</span> s0 == s1, <span class="string">'Error dims dont match: %s and %s.'</span> % (`s0`, `s1`)</div><div class="line">    <span class="keyword">print</span> name</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_checks):</div><div class="line">      ri = int(uniform(<span class="number">0</span>,param.size))</div><div class="line">      <span class="comment"># evaluate cost at [x + delta] and [x - delta]</span></div><div class="line">      old_val = param.flat[ri]</div><div class="line">      param.flat[ri] = old_val + delta</div><div class="line">      cg0, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)</div><div class="line">      param.flat[ri] = old_val - delta</div><div class="line">      cg1, _, _, _, _, _, _ = lossFun(inputs, targets, hprev)</div><div class="line">      param.flat[ri] = old_val <span class="comment"># reset old value for this parameter</span></div><div class="line">      <span class="comment"># fetch both numerical and analytic gradient</span></div><div class="line">      grad_analytic = dparam.flat[ri]</div><div class="line">      grad_numerical = (cg0 - cg1) / ( <span class="number">2</span> * delta )</div><div class="line">      rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)</div><div class="line">      <span class="keyword">print</span> <span class="string">'%f, %f =&gt; %e '</span> % (grad_numerical, grad_analytic, rel_error)</div><div class="line">      <span class="comment"># rel_error should be on order of 1e-7 or less</span></div></pre></td></tr></table></figure>
<p><strong>Results</strong></p>
<p>Using Shakespeare’s sonnet as input:</p>
<p> <img src="/images/RNN_text_generator.png" alt="RNN_text_generator"></p>
<h3 id="Example-Image-Captioning"><a href="#Example-Image-Captioning" class="headerlink" title="Example: Image Captioning"></a>Example: Image Captioning</h3><p>We use CNN to recognize objects and use RNN to generate captions.</p>
<p><img src="/images/image_captioning_structure.png" alt="image_captioning_structure"></p>
<p>Cut the last two layers from CNN and connect it to RNN:</p>
<p><img src="/images/image_captioning_structure_1.png" alt="image_captioning_structure_1"></p>
<p>And smaple the output from previous layer to next layer as input:</p>
<p><img src="/images/image_captioning_structure_2.png" alt="image_captioning_structure_2"></p>
<p>Sampling is stoped when meeting an END</p>
<p><img src="/images/image_captioning_structure_3.png" alt="image_captioning_structure_3"></p>
<p>Finally, we’ll get a complete sentence (using <a href="http://mscoco.org" target="_blank" rel="external">Microsoft COCO dataset</a>). The first row are good, but the second row may be not satisfactory.</p>
<p><img src="/images/image_captioning_result.png" alt="image_captioning_result"></p>
<blockquote>
<p><strong>Reference</strong>:</p>
<ul>
<li>Explain Images with Multimodal Recurrent Neural Networks, Mao et al.</li>
<li>Deep Visual-Semantic Alignments for Generating Image Descriptions, Karpathy and Fei-Fei</li>
<li>Show and Tell: A Neural Image Caption Generator, Vinyals et al.</li>
<li>Long-term Recurrent Convolutional Networks for Visual Recognition andDescription, Donahue et al.</li>
<li>Learning a Recurrent Visual Representation for Image CaptionGeneration, Chen and Zitnick</li>
</ul>
</blockquote>
<h3 id="More-examples"><a href="#More-examples" class="headerlink" title="More examples"></a>More examples</h3><p>We can also use RNN to generate open source textbooks written in LaTex, or generate C code from Linux source code, or searching for interpretable cells.</p>
<h2 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long Short Term Memory (LSTM)"></a>Long Short Term Memory (LSTM)</h2><h3 id="Vanishing-Exploding-gradients"><a href="#Vanishing-Exploding-gradients" class="headerlink" title="Vanishing/Exploding gradients"></a>Vanishing/Exploding gradients</h3><ul>
<li>Exploding gradients<ul>
<li>Truncated BPTT</li>
<li><strong>Clip gradients at threshold</strong> (something like anti-windup in control science LOL)</li>
<li>RMSProp to adjust learning rate</li>
</ul>
</li>
<li>Vanishing gradients<ul>
<li>Harder to detect</li>
<li>Weight Initialization</li>
<li>ReLU activation functions</li>
<li>RMSProp</li>
<li><strong>LSTM, GRUs</strong> (&lt;– That’s why we use LSTM)</li>
</ul>
</li>
</ul>
<h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><p>LSTM is proposed in [Hochreiter et al., 1997]. GRU is a knid of simplified LSTM.</p>
<p> <img src="/images/LSTM_diagram.png" alt="LSTM_diagram"></p>
<blockquote>
<p>ResNet is to PlainNet what LSTM is to RNN, kind of.</p>
<p> <img src="/images/plainnet_vs_resnet.png" alt="plainnet_vs_resnet"></p>
</blockquote>
<h3 id="Concept-1"><a href="#Concept-1" class="headerlink" title="Concept"></a>Concept</h3><p> <img src="/images/LSTM_structure.png" alt="LSTM_structure"></p>
<p>LSTM have two states, one is <strong>cell state</strong> ($c$), another is <strong>hidden state</strong> ($h$):</p>
<ul>
<li>$i$: input gate, “add to memory”, decides whether do we want to add value to this cell.</li>
<li>$f$: forget gate, “flush the memory”, decides whether to shut off the cell and reset the counter.</li>
<li>$o$: output gate, “get from memory”, decides how much do we want to get from this cell.</li>
<li>$g$: input, decides how much do we want to add to this cell.</li>
</ul>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>RNNs allow a lot of flexibility inarchitecture design</li>
<li>Vanilla RNNs are simple but don’twork very well</li>
<li>Common to use LSTM or GRU: theiradditive interactions improve gradient flow</li>
<li>Backward flow of gradients in RNNcan explode or vanish. Exploding is controlled with gradient clipping.Vanishing is controlled with additive interactions (LSTM)</li>
<li>Better/simpler architectures are ahot topic of current research</li>
<li>Better understanding (boththeoretical and empirical) is needed.</li>
</ul>
<p>(To be improved by adding extra materials…)</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;从 RNN 开始, CS231n 的 Lecture Notes 就没有了, 因此我根据上课时的 Slides 整理了一些需要重视的知识点. 还可以参考这些文章或是视频来加深理解:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/22107715?refer=intelligentunit&quot;&gt;[原创翻译]循环神经网络惊人的有效性（上）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/22230074?refer=intelligentunit&quot;&gt;[原创翻译]循环神经网络惊人的有效性（下）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Ukgii7Yd_cU&quot;&gt;Recurrent Neural Networks (Video, recommend)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://james371507.wixsite.com/hylee/single-post/2016/03/20/Recurrent-Neural-Network-RNN-Note-of-Stanford-CS231n&quot;&gt;Recurrent Neural Network (RNN) (Note of Stanford CS231n)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="CS231n" scheme="http://www.yuthon.com/tags/CS231n/"/>
    
      <category term="Recurrent Neural Network" scheme="http://www.yuthon.com/tags/Recurrent-Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Traffic Prediction Using LSTM</title>
    <link href="http://www.yuthon.com/2016/10/30/Traffic-Prediction-Using-LSTM/"/>
    <id>http://www.yuthon.com/2016/10/30/Traffic-Prediction-Using-LSTM/</id>
    <published>2016-10-30T05:53:32.000Z</published>
    <updated>2016-11-01T12:16:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近上的一门课 “无线传感器网络” 快要结束了, 于是所谓的大作业的 DDL 也压上来了. TAT</p>
<p>不过这门课虽然说是讲无线传感器网络的, 但是大作业的要求却额外的宽松, 只要是和数据分析有关的就好了. 老师还给了些数据集, 比如说公共自行车的出借与归入记录啊, 出租车在各个路段的行驶速度啊, 或者是顺丰快递途径各个城市需要的时间啊这类的. 当然也可以自己选题. </p>
<p>我当然是想自己选题的, 然而想了一圈没想到什么好的方案, 于是只好回到了老师给的题目上面来, 选了道路速度预测这样的题目. 刚好之前在 CS231n 上看了 RNN 和 LSTM, 心想这总比传统方法好点吧, 于是就开始干了. (于是就有了之前的那篇装 CUDA 和 TF)</p>
<p>I wanna traffic prediction, I learn LSTM.</p>
<p>ugh, Traffic prediction using LSTM!</p>
<p>(此处应有 PPAP)</p>
<a id="more"></a>
<h2 id="RNN-与-LSTM-基本原理"><a href="#RNN-与-LSTM-基本原理" class="headerlink" title="RNN 与 LSTM 基本原理"></a>RNN 与 LSTM 基本原理</h2><p>直接看我 CS231n 相关的课程笔记吧</p>
<p><a href="http://www.yuthon.com/2016/10/30/Notes-for-CS231n-RNN/">Notes for CS231n Recurrent Neural Network</a></p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>详细代码可见我Github上的项目 <strong><a href="https://github.com/corenel/traffic-prediction" target="_blank" rel="external">traffic-prediction</a></strong>. 为了课堂展示我还做了一个pdf, 可以从此处下载.</p>
<p>本次用了两层的 LSTM, 中间加了 Dropout:</p>
<p><img src="/images/traffic_prediction_model.png" alt="traffic_prediction_model"></p>
<p>输入是一个 4 元素的向量, 分别是星期几, 是否周末, 小时与分钟. </p>
<p>$Input = \begin{bmatrix}Weekday &amp; isWeekend &amp; Hour &amp; Minute\end{bmatrix}$</p>
<p>输出自然是道路上此刻的速度</p>
<p>$Output = \begin{bmatrix} Velocity \end{bmatrix}$</p>
<blockquote>
<p>话说 Keras 竟然能用 graphviz 直接输出模型的结构图, 真是方便</p>
</blockquote>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>老师给的数据集简直弱爆了, 一条路上总共2000+条数据, 还是按照小时计的, 训练出来的结果惨不忍睹.</p>
<p>于是在网上找到了 <a href="http://pems.dot.ca.gov/" target="_blank" rel="external">Caltrans Performance Measurement System (PeMS)</a> 这个网站, 里面数据是每 5 分钟采样一次的, 比前面的那个不知高到哪里去了.</p>
<p>此次选取的是 16444 路段, 时间是 2016-05-01 到 2016-10-26 总共 6 个月 5W+ 条数据.</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>一天(2016-10-26)和一周(2016.10.20 - 2016.10.26)的预测如下:</p>
<p><img src="/images/traffic_prediction_result_1.png" alt="traffic_prediction_result_1"></p>
<p><img src="/images/traffic_prediction_result_2.png" alt="traffic_prediction_result_2"></p>
<p>可以看出, 总体的趋势还是不错的, 但是高峰的部分还是有些够不上. 同时, 也确实预测到了周末与工作日的速度的区别.</p>
<blockquote>
<p>matplotlib 可以用 ggplot 的样式, 好看多了</p>
</blockquote>
<h2 id="Deeper"><a href="#Deeper" class="headerlink" title="Deeper"></a>Deeper</h2><p>使用了3层LSTM, MSE有一定下降, 但是高峰期跟不上的问题还是没有解决</p>
<p> <img src="/images/traffic_prediction_result_4.png" alt="traffic_prediction_result_4"></p>
<p> <img src="/images/traffic_prediction_result_5.png" alt="traffic_prediction_result_5"></p>
<p> <img src="/images/traffic_prediction_result_6.png" alt="traffic_prediction_result_6"></p>
<h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><ul>
<li>加深层数</li>
<li>仔细考虑输入向量的长度和内容, 还可加入假日, 天气等(老师给的数据集有, 但是PeMS没)</li>
<li>使用 Stateful LSTM 的尝试失败了</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近上的一门课 “无线传感器网络” 快要结束了, 于是所谓的大作业的 DDL 也压上来了. TAT&lt;/p&gt;
&lt;p&gt;不过这门课虽然说是讲无线传感器网络的, 但是大作业的要求却额外的宽松, 只要是和数据分析有关的就好了. 老师还给了些数据集, 比如说公共自行车的出借与归入记录啊, 出租车在各个路段的行驶速度啊, 或者是顺丰快递途径各个城市需要的时间啊这类的. 当然也可以自己选题. &lt;/p&gt;
&lt;p&gt;我当然是想自己选题的, 然而想了一圈没想到什么好的方案, 于是只好回到了老师给的题目上面来, 选了道路速度预测这样的题目. 刚好之前在 CS231n 上看了 RNN 和 LSTM, 心想这总比传统方法好点吧, 于是就开始干了. (于是就有了之前的那篇装 CUDA 和 TF)&lt;/p&gt;
&lt;p&gt;I wanna traffic prediction, I learn LSTM.&lt;/p&gt;
&lt;p&gt;ugh, Traffic prediction using LSTM!&lt;/p&gt;
&lt;p&gt;(此处应有 PPAP)&lt;/p&gt;
    
    </summary>
    
      <category term="Projects" scheme="http://www.yuthon.com/categories/Projects/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="Traffic Prediction" scheme="http://www.yuthon.com/tags/Traffic-Prediction/"/>
    
      <category term="LSTM" scheme="http://www.yuthon.com/tags/LSTM/"/>
    
      <category term="RNN" scheme="http://www.yuthon.com/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>CUDA and Tensorflow Installation on Ubuntu 16.04</title>
    <link href="http://www.yuthon.com/2016/10/25/CUDA-and-Tensorflow-Installation-on-Ubuntu-16-04/"/>
    <id>http://www.yuthon.com/2016/10/25/CUDA-and-Tensorflow-Installation-on-Ubuntu-16-04/</id>
    <published>2016-10-25T12:53:50.000Z</published>
    <updated>2016-10-30T07:01:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>昨天折腾了一个下午开发环境的配置，记录一下其中遇到的坑。</p>
<a id="more"></a>
<h2 id="硬件配置"><a href="#硬件配置" class="headerlink" title="硬件配置"></a>硬件配置</h2><p>我的硬件配置(XPS 15 9550):</p>
<ul>
<li>CPU: i5 6300HQ</li>
<li>GPU: GTX960M 2G</li>
<li>内存: 16G DDR4 2133</li>
<li>硬盘: 512G SM951 NVMe</li>
</ul>
<blockquote>
<p>基本上就只处于玩票的状态, 实验室快给我配1080啊~~~</p>
</blockquote>
<p>上一个 NVIDIA 钦定的 DevBox <a href="http://www.nvidia.com/object/deep-learning-system.html" target="_blank" rel="external">配置</a>:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">GPUs</td>
<td>8x Tesla P100</td>
</tr>
<tr>
<td style="text-align:left">TFLOPS (GPU FP16 /CPU FP32)</td>
<td>170/3</td>
</tr>
<tr>
<td style="text-align:left">GPU Memory</td>
<td>16 GB per GPU</td>
</tr>
<tr>
<td style="text-align:left">CPU</td>
<td>Dual 20-core Intel® Xeon®E5-2698 v4 2.2 GHz</td>
</tr>
<tr>
<td style="text-align:left">NVIDIA CUDA® Cores</td>
<td>28672</td>
</tr>
<tr>
<td style="text-align:left">System Memory</td>
<td>512 GB 2133 MHz DDR4</td>
</tr>
<tr>
<td style="text-align:left">Storage</td>
<td>4x 1.92 TB SSD RAID 0</td>
</tr>
<tr>
<td style="text-align:left">Network</td>
<td>Dual 10 GbE, 4 IB EDR</td>
</tr>
<tr>
<td style="text-align:left">Software</td>
<td>Ubuntu Server Linux OSDGX-1 Recommended GPUDriver</td>
</tr>
<tr>
<td style="text-align:left">System Weight</td>
<td>134 lbs</td>
</tr>
<tr>
<td style="text-align:left">System Dimensions</td>
<td>866 D x 444 W x 131 H (mm)</td>
</tr>
<tr>
<td style="text-align:left">Packing Dimensions</td>
<td>1180 D x 730 W x 284 H (mm)</td>
</tr>
<tr>
<td style="text-align:left">Maximum Power Requirements</td>
<td>3200W</td>
</tr>
<tr>
<td style="text-align:left">Operating Temperature Range</td>
<td>10 - 30°C</td>
</tr>
</tbody>
</table>
<p>这简直是吾辈梦想神机啊… 然而要 <strong>$129000</strong> !</p>
<h2 id="系统环境"><a href="#系统环境" class="headerlink" title="系统环境"></a>系统环境</h2><p>推荐是用 Ubuntu 最新的 LTS 版本 16.04.1, 对 Skylake 系列的 CPU 和主板的支持都很不错. 关于在 XPS 15 9550 上的详细配置过程, 我计划稍后专门写一篇.</p>
<p>本来想在 macOS 上跑的, 奈何黑苹果不支持独显.</p>
<h2 id="安装-CUDA"><a href="#安装-CUDA" class="headerlink" title="安装 CUDA"></a>安装 CUDA</h2><p><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="external">CUDA</a> 与 <a href="https://developer.nvidia.com/cudnn" target="_blank" rel="external">cuDNN</a> 的安装在 NVIDIA 与 Tensorflow的官网上都有<a href="https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#optional-install-cuda-gpus-on-linux" target="_blank" rel="external">详细说明</a>, 此处仅就一些关键环节作出说明.</p>
<ul>
<li><p>禁用开源的 Nouveau 驱动</p>
<ul>
<li><p>首先看看有没有在使用这个开源驱动:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ lsmod | grep nouveau</div></pre></td></tr></table></figure>
</li>
<li><p>创建<code>/etc/modprobe.d/blacklist-nouveau.conf</code>文件, 并写入以下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">blacklist nouveau</div><div class="line">options nouveau modeset=0</div></pre></td></tr></table></figure>
</li>
<li><p>重启kernel initramfs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo update-initramfs -u</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>从ppa源下载最新版的驱动(&gt;364), 或者使用 CUDA-Toolkit 自带的驱动:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get purge nvidia-*</div><div class="line">$ sudo add-apt-repository ppa:graphics-drivers/ppa</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install nvidia-370</div></pre></td></tr></table></figure>
</li>
<li><p>下载 CUDA 的 runfile (local) 版本, 不要使用apt-get 的方式, 保证获取到的是最新的版本 (目前是8.0).</p>
</li>
<li><p>安装过程中, 需要<code>Ctrl+Alt+F1</code>切换到 tty 界面, 然后关闭 X server:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo service lightdm stop</div></pre></td></tr></table></figure>
<p>之后再执行安装过程</p>
</li>
<li><p>安装过程中, 如果是像我一样的 Intel 核显 + NVIDIA 独显的, <strong>绝对不要装 OpenGL</strong>, 否则重启后会陷入 login loop.</p>
</li>
<li><p>安装完成之后, 设置环境变量:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export LD_LIBRARY_PATH=&quot;$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64&quot;</div><div class="line">export CUDA_HOME=/usr/local/cuda</div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="安装-cuDNN"><a href="#安装-cuDNN" class="headerlink" title="安装 cuDNN"></a>安装 cuDNN</h2><p>从<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="external">官网</a>下载之后, 执行以下命令 (目前最新版本5.1):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">tar xvzf cudnn-8.0-linux-x64-v5.1-ga.tgz</div><div class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda/include</div><div class="line">sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</div><div class="line">sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*</div></pre></td></tr></table></figure>
<h2 id="安装Tensorflow"><a href="#安装Tensorflow" class="headerlink" title="安装Tensorflow"></a>安装Tensorflow</h2><p>这是最纠结的一步, 之前按照别人教程说最好从源码编译支持 GPU. 但是由于国内的网络环境, 源码编译需要下一堆的依赖包, 速度超级慢. 因此还是使用官网推荐的 pip 安装方式:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># Get pip</div><div class="line">$ sudo apt-get install python-pip python-dev</div><div class="line"></div><div class="line"># Ubuntu/Linux 64-bit, GPU enabled, Python 2.7</div><div class="line"># Requires CUDA toolkit 8.0 and CuDNN v5. For other versions, see &quot;Install from sources&quot; below.</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc1-cp27-none-linux_x86_64.whl</div><div class="line"></div><div class="line"># Python 2</div><div class="line">$ sudo pip install --upgrade $TF_BINARY_URL</div></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>可以先把<code>tensorflow-0.11.0rc1-cp27-none-linux_x86_64.whl</code>下下来, 本地执行安装命令</li>
<li>可以使用国内的 pip 源, 加快安装依赖的速度</li>
</ul>
</blockquote>
<h2 id="测试-Tensorflow"><a href="#测试-Tensorflow" class="headerlink" title="测试 Tensorflow"></a>测试 Tensorflow</h2><p>用下面的小例子来测试下 Tensorflow 安装得成不成功:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$ python</div><div class="line">...</div><div class="line">&gt;&gt;&gt; import tensorflow as tf</div><div class="line">&gt;&gt;&gt; hello = tf.constant(&apos;Hello, TensorFlow!&apos;)</div><div class="line">&gt;&gt;&gt; sess = tf.Session()</div><div class="line">&gt;&gt;&gt; print(sess.run(hello))</div><div class="line">Hello, TensorFlow!</div><div class="line">&gt;&gt;&gt; a = tf.constant(10)</div><div class="line">&gt;&gt;&gt; b = tf.constant(32)</div><div class="line">&gt;&gt;&gt; print(sess.run(a + b))</div><div class="line">42</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure>
<p>在 <code>import</code>的同时, 还会显示 CUDA 方面与 GPU 方面的信息.</p>
<p>至此大功告成!</p>
<h2 id="Docker-for-tensorflow"><a href="#Docker-for-tensorflow" class="headerlink" title="Docker for tensorflow"></a>Docker for tensorflow</h2><p>其实在这期间还试过直接用装了 Tensorflow 的 Docker 镜像, 也有支持 GPU 的版本. 各位如有兴趣可以试试:</p>
<ul>
<li><a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker" target="_blank" rel="external">Github - Using TensorFlow via Docker</a></li>
<li><a href="https://github.com/saiprashanths/dl-docker" target="_blank" rel="external">Github - An all-in-one Docker image for deep learning. Contains all the popular DL frameworks (TensorFlow, Theano, Torch, Caffe, etc.)</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;昨天折腾了一个下午开发环境的配置，记录一下其中遇到的坑。&lt;/p&gt;
    
    </summary>
    
      <category term="Experience" scheme="http://www.yuthon.com/categories/Experience/"/>
    
    
      <category term="Tensorflow" scheme="http://www.yuthon.com/tags/Tensorflow/"/>
    
      <category term="Ubuntu" scheme="http://www.yuthon.com/tags/Ubuntu/"/>
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="CUDA" scheme="http://www.yuthon.com/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>Notes for CS231n Convolutional Neural Network</title>
    <link href="http://www.yuthon.com/2016/10/19/Notes-for-CS231n-CNN/"/>
    <id>http://www.yuthon.com/2016/10/19/Notes-for-CS231n-CNN/</id>
    <published>2016-10-19T03:06:30.000Z</published>
    <updated>2016-10-20T12:23:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:</p>
<ul>
<li><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">Convolutional Neural Networks: Architectures, Convolution / Pooling Layers</a></li>
</ul>
<p>或者可以看知乎专栏中的中文翻译:</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：卷积神经网络笔记 </a></li>
</ul>
<p>另外, 本文主要根据讲课的 Slides 上的顺序来, 与 Lecture Notes 的顺序略有不同.</p>
</blockquote>
<a id="more"></a>
<h1 id="Lecture-7"><a href="#Lecture-7" class="headerlink" title="Lecture 7"></a>Lecture 7</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>CNN 主要有以下的层(layer):</p>
<ul>
<li>卷积层 (Conv Layer): 通过不同的 filter 进行卷积操作, 来增加 depth</li>
<li>ReLU 层</li>
<li>汇聚层 / 池化层 (Pooling Layer): 进行 down-sampling, 减小空间尺寸</li>
<li>全连接层 (Full-connected Layer): 放在最后进行 classification, 相当于普通的 NN</li>
</ul>
<p><img src="/images/CNN_structure.png" alt="CNN_structure"></p>
<blockquote>
<p>CNN 相对于 NN 来说, 其结构基于输入数据是图像这么一个假设. 基于该假设, 我们就向结构中添加了一些特有的性质. 这些特有属性使得前向传播函数实现起来更高效, 并且大幅度降低了网络中参数的数量. 这也是 CNN 更适用于图像方面的原因.</p>
</blockquote>
<h3 id="Conv-layer"><a href="#Conv-layer" class="headerlink" title="Conv layer"></a>Conv layer</h3><p>主要需要了解以下几个概念:</p>
<ul>
<li><p><strong>滤波器(Filter)</strong>: 又叫卷积核 (Kernel), 尺寸较小 (例如5x5x3). 通过在输入数据上滑动来生成新的 Activation Map / Feature Map.</p>
<p><img src="/images/conv_layer_filter.png" alt="conv_layer_filter"></p>
<ul>
<li>滤波器的深度须与输入数据的深度一致. 也就是说输入 32x32x3 的图像, 其对应的滤波器的尺寸必须是 FxFx3.</li>
<li>下一层的深度取决于这层用了几个滤波器</li>
<li>滤波器的尺寸又称感受野 (Receptive Field)</li>
</ul>
</li>
<li><p><strong>步长 (Stride)</strong>: 即指滤波器每次移动几个像素. 通常步长为奇数.</p>
<p><img src="/images/CNN_stride.png" alt="CNN_stride"></p>
</li>
<li><p><strong>零填充 (Zero-padding)</strong>: 用来保证滤波器完整平滑地划过输入数据, 不出现非整数的问题. 同时还能够用来保持输入与输出数据具有相同的尺寸, 即令$P=(F-1)/2$.</p>
<p><img src="/images/CNN_padding.png" alt="CNN_padding"></p>
</li>
</ul>
<p>如是这般, 宽度与高度不断缩小, 深度不断增加, 信息提取得更为抽象.</p>
<p><img src="/images/CNN_layers.png" alt="CNN_layers"></p>
<p><strong>总结</strong></p>
<ul>
<li>输入数据尺寸$W_1 \times H_1 \times D_1$</li>
<li>需要的超参数<ul>
<li>滤波器数量$K$, 通常是2的几次幂, 例如32, 64, 128, 512等</li>
<li>滤波器尺寸$F$, 通常为1, 3, 5等</li>
<li>步长$S$, 通常为1或2</li>
<li>零填充数量$P$</li>
</ul>
</li>
<li>输出数据尺寸$W_2 \times H_2 \times D_2$<ul>
<li>$W_2 = (W_1 - F + 2P) / S + 1$</li>
<li>$H_2 = (H_1 - F + 2P) / S + 1$ (通常$W_1=H_1,W_2=H_2$)</li>
<li>$D_2 = K$</li>
</ul>
</li>
<li>就参数共享来说, 每个滤波器有$F\cdot F\cdot D_1$个权重参数, 总共有$(F\cdot F\cdot D_1)\cdot K$个权重参数 (weights) 和$K$个偏差参数 (biases).</li>
<li>在输出数据体中, 第$d$层 (尺寸$W_2\times H_2$) 深度切片(depth slice)是由第$d$个滤波器在输入数据体上以$S$为补偿进行有效的卷积, 并且偏移了第$d$个偏差之后得到的.</li>
<li>有时候会有$1\times 1\times D$的滤波器, 其也是有效的. 因为它有深度, 实际上进行的是一个$D$维的点积.</li>
</ul>
<h2 id="Pooling-layer"><a href="#Pooling-layer" class="headerlink" title="Pooling layer"></a>Pooling layer</h2><ul>
<li>makes the representations smaller and more manageable </li>
<li><p>operates over each activation map independently</p>
<p><img src="/images/pooling_layer.png" alt="pooling_layer"></p>
</li>
</ul>
<p><strong>总结</strong></p>
<ul>
<li>输入数据尺寸$W_1 \times H_1 \times D_1$</li>
<li>需要的超参数<ul>
<li>滤波器尺寸$F$, 通常为2或3</li>
<li>步长$S$, 通常为2</li>
</ul>
</li>
<li>输出数据尺寸$W_2 \times H_2 \times D_2$<ul>
<li>$W_2 = (W_1 - F) / S + 1$</li>
<li>$H_2 = (H_1 - F) / S + 1$ (通常$W_1=H_1,W_2=H_2$)</li>
<li>$D_2 = D_1$</li>
</ul>
</li>
<li>由于是固定的计算, 因此没有引入参数.</li>
<li>通常不在汇聚层中使用零填充</li>
</ul>
<h2 id="Case-study"><a href="#Case-study" class="headerlink" title="Case study"></a>Case study</h2><p>To be continued…</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;Convolutional Neural Networks: Architectures, Convolution / Pooling Layers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;或者可以看知乎专栏中的中文翻译:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit&quot;&gt;CS231n课程笔记翻译：卷积神经网络笔记 &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另外, 本文主要根据讲课的 Slides 上的顺序来, 与 Lecture Notes 的顺序略有不同.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="CS231n" scheme="http://www.yuthon.com/tags/CS231n/"/>
    
      <category term="Convolutional Neural Network" scheme="http://www.yuthon.com/tags/Convolutional-Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Notes for CS231n Neural Network</title>
    <link href="http://www.yuthon.com/2016/10/16/Notes-for-CS231n-NN/"/>
    <id>http://www.yuthon.com/2016/10/16/Notes-for-CS231n-NN/</id>
    <published>2016-10-16T13:04:26.000Z</published>
    <updated>2016-11-20T13:23:59.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:</p>
<ul>
<li><a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="external">Neural Networks Part 1: Setting up the Architecture</a></li>
<li><a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="external">Neural Networks Part 2: Setting up the Data and the Loss</a></li>
<li><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">Neural Networks Part 3: Learning and Evaluation</a></li>
</ul>
<p>或者可以看知乎专栏中的中文翻译:</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记1（上）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记1（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记2 </a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记3（上）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记3（下）</a></li>
</ul>
<p>另外, 本文主要根据讲课的 Slides 上的顺序来, 与 Lecture Notes 的顺序略有不同.</p>
</blockquote>
<a id="more"></a>
<h1 id="Lecture-5"><a href="#Lecture-5" class="headerlink" title="Lecture 5"></a>Lecture 5</h1><h2 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h2><p>课程中主要讲了Sigmoid, tanh, ReLU, Leaky ReLU, Maxout 以及 ELU 这几种激活函数.</p>
<p><img src="/images/activation_functions.png" alt="activation_functions"></p>
<ul>
<li>Sigmoid 由于以下原因, 基本不使用<ul>
<li>函数饱和使得梯度消失(Saturated neurons “kill” the gradients)</li>
<li>函数并非以零为中心(zero-centered)</li>
<li>指数运算消耗大量计算资源</li>
</ul>
</li>
<li>tanh 相对于 Sigmoid 来说, 多了零中心这一个特性, 但还是不常用</li>
<li>重头戏 ReLU (Rectified Linear Unit):<ul>
<li>在正半轴上没有饱和现象</li>
<li>线性结构省下了很多计算资源, 可以直接对矩阵进行阈值计算来实现, 速度是 sigmoid/tanh 的6倍</li>
<li>然而由于负半轴直接是0, 训练的时候会”死掉”(die), 因此就有了 Leaky ReLU 和 ELU (Exponential Linear Units), 以及更加通用的 Maxout (代价是消耗两倍的计算资源)</li>
</ul>
</li>
</ul>
<p><strong>实践中一般就直接选 ReLU, 同时注意 Learning Rate 的调整. 实在不行用 Leaky ReLU 或者 Mahout 碰碰运气. 还可以试试 tanh. 坚决别用 Sigmoid.</strong></p>
<h2 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h2><p>有很多数据预处理的方法, 比如零中心化(zero-centering), 归一化(normalization), PCA(Principal Component Analysis, 主成分分析)和白化(Whitening).</p>
<ul>
<li><p>零中心化(zero-centering): 主要方法就是均值减法, 将数据的中心移到原点上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Assume X [NxD] is data matrix, each example in a row</span></div><div class="line">X -= np.mean(X, axis=<span class="number">0</span>)</div></pre></td></tr></table></figure>
<p>零中心化主要有两种做法(e.g. consider CIFAR-10 example with [32,32,3] images):</p>
<ul>
<li><p>Subtract the mean image (e.g.AlexNet)  (mean image = [32,32,3] array)</p>
</li>
<li><p>Subtract per-channel mean (e.g.VGGNet)  (mean along each channel = 3 numbers)</p>
</li>
</ul>
</li>
<li><p>归一化(normalization): 使得数据所有维度的范围基本相等, 当然由于图像像素的数值范围本身基本是一致的(一般为0-255), 所以不一定要用.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">X /= np.std(X, axis=<span class="number">0</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>PCA 和白化在 CNN 中并没有什么用, 就不介绍了.</p>
<p><img src="/images/data_preprocessing.png" alt="data_preprocessing"></p>
</li>
</ul>
<p><strong>实践中一般就只做零中心化, 其他几样基本都不用做.</strong></p>
<blockquote>
<p>以下引自知乎专栏[智能单元]所翻译的课程讲义:</p>
<p><strong>常见错误。</strong>进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。<strong>应该怎么做呢？应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。</strong></p>
<p><strong>译者注：此处确为初学者常见错误，请务必注意！</strong></p>
</blockquote>
<h2 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h2><p>由于各种原因, 将 Weight 全部初始化为0, 或者是小随机数的方法都不大好(一个是由于对称性, 另一个是由于梯度信号太小). 建议使用的是下面这个(配合 ReLU):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">w = np.random.randn(n) * sqrt(<span class="number">2.0</span>/n)</div></pre></td></tr></table></figure>
<p>或者 Xavier initialization:</p>
<p> <img src="/images/xavier_init.png" alt="xavier_init"></p>
<p>另外就是还推荐 <strong>Batch Normalization</strong> (批量归一化), 通常应用在全连接层之后, 激活函数之前. 具体参见论文[Ioffe and Szegedy, 2015].</p>
<p><img src="/images/batch_normalizaition.png" alt="batch_normalizaition"></p>
<ul>
<li>Improves gradient flow through thenetwork</li>
<li>Allows higher learning rates</li>
<li>Reduces the strong dependence on initialization</li>
<li>Acts as a form of regularization in afunny way, and slightly reduces the need for dropout, maybe</li>
</ul>
<h2 id="Babysitting-the-Learning-Process"><a href="#Babysitting-the-Learning-Process" class="headerlink" title="Babysitting the Learning Process"></a>Babysitting the Learning Process</h2><h3 id="Double-check-that-the-loss-is-reasonable"><a href="#Double-check-that-the-loss-is-reasonable" class="headerlink" title="Double check that the loss is reasonable"></a>Double check that the loss is reasonable</h3><ul>
<li>首先不使用 regularization, 观察 loss 是否合理(下例中对于 CIFAR-10 的初始 loss 应近似等于$log(0.1)=2.31$)</li>
<li><p>然后再开启 regularization, 观察 loss 是否上升</p>
<p><img src="/images/loss_double_check.png" alt="loss_double_check"></p>
</li>
</ul>
<h3 id="Other-sanity-check-tips"><a href="#Other-sanity-check-tips" class="headerlink" title="Other sanity check tips"></a>Other sanity check tips</h3><ul>
<li><p>首先在一个小数据集上进行训练(可先设 regualrization 为0), 看看是否过拟合, 确保算法的正确性.</p>
<p><img src="/images/overfit_on_a_small_portion_of_training_data.png" alt="overfit_on_a_small_portion_of_training_data"></p>
</li>
<li><p>之后再从一个小的 regularization 开始, 寻找合适的能够使 loss 下降的 learning rate.</p>
<ul>
<li><p>如果几次 epoch 后, loss 没没有下降, 说明 learning rate 太小了</p>
<p> <img src="/images/loss_barely_changing.png" alt="loss_barely_changing"></p>
</li>
<li><p>如果 loss 爆炸了, 那么说明 learning rate 太大了</p>
<p> <img src="/images/loss_exploding.png" alt="loss_exploding"></p>
</li>
<li><p>通常 learning rate 的范围是$[1e-3, 1e-5]$</p>
</li>
</ul>
</li>
</ul>
<h2 id="Hyperparameter-Optimization"><a href="#Hyperparameter-Optimization" class="headerlink" title="Hyperparameter Optimization"></a>Hyperparameter Optimization</h2><ul>
<li><p><strong>从粗放(coarse)到细致(fine)地分段搜索</strong>, 先大范围小周期(1-5 epoch足矣), 然后再根据结果小范围长周期</p>
<ul>
<li>First stage: only a few epochs to get rough idea of what params work</li>
<li>Second stage: longer running time, finer search</li>
<li>… (repeat as necessary)</li>
</ul>
<blockquote>
<p>If the cost is ever &gt; 3 * original cost, break out early</p>
</blockquote>
</li>
<li><p><strong>在对数尺度上进行搜索</strong>, 例如<code>learning_rate = 10 ** uniform(-6, 1)</code>. 当然有些超参数还是按原来的, 比如 <code>dropout = uniform(0,1)</code></p>
</li>
<li><p><strong>小心边界上的最优值</strong>, 否则可能会错过更好的参数搜索范围.</p>
<p> <img src="/images/coarse_search.png" alt="coarse_search"></p>
<p> <img src="/images/finer_search.png" alt="finer_search"></p>
</li>
<li><p><strong>随机搜索优于网格搜索</strong></p>
<p> <img src="/images/random_search_vs_grid_search .png" alt="random_search_vs_grid_search "></p>
</li>
</ul>
<h1 id="Lecture-6"><a href="#Lecture-6" class="headerlink" title="Lecture 6"></a>Lecture 6</h1><h2 id="Parameter-Updates"><a href="#Parameter-Updates" class="headerlink" title="Parameter Updates"></a>Parameter Updates</h2><p> 参数更新有很多种方法, 常见的如下图:<img src="/images/parameter_update.png" alt="parameter_update"></p>
<ul>
<li><p>最普通的就是SGD, 仅仅按照负梯度来更新</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x += - learning_rate * dx</div></pre></td></tr></table></figure>
<p><img src="/images/sgd.png" alt="sgd"></p>
</li>
<li><p>其次就是各种动量方法, 比如 <strong>Momentum</strong>,  以及其衍生的 <strong>Nesterov</strong> 方法. 其主要思想就是在任何具有持续梯度的方向上保持一个会慢慢消失的动量, 使得梯度下降更为圆滑.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Momentum update</span></div><div class="line">v = mu * v - learning_rate * dx <span class="comment"># integrate velocity</span></div><div class="line">x += v <span class="comment"># integrate position</span></div><div class="line"></div><div class="line"><span class="comment"># Mesterov momentum update rewrite</span></div><div class="line">v_prev = v</div><div class="line">v = mu * v - learning_rate * dx</div><div class="line">x += -mu * v_prev + (<span class="number">1</span> + mu) * v</div></pre></td></tr></table></figure>
<p><img src="/images/momentum_and_Nesterov.png" alt="momentum_and_Nesterov"></p>
<blockquote>
<ul>
<li>v 初始为 0</li>
<li>mu 一般取 0.5, 0.9 或 0.99. 有时候可以先 0.5, 然后慢慢变成 0.99</li>
</ul>
</blockquote>
</li>
<li><p>然后就是逐步改 learning rate 的方法, 比如 AdaGrad 或者 RMSProp (Hinton 大神在 Coursera 课上提出的改进方法)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># AdaGrad</span></div><div class="line">cache += dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div><div class="line"></div><div class="line"><span class="comment"># RMSProp</span></div><div class="line">cache = decay_rate * cache + (<span class="number">1</span> - decay_rate) * dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>cache 尺寸与 dx 相同</li>
<li>eps 取值在 1e-4 到 1e-8 之间, 主要是为了防止分母为 0.</li>
<li>AdaGrad 通常过早停止学习, RMSProp 通过引入一个梯度平方的滑动平均改善了它.</li>
</ul>
</blockquote>
</li>
<li><p>最后就是集上述方法之大成的 <strong>Adam</strong>, 在大多数的实践中都是一个很好的选择.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Adam</span></div><div class="line">m ,v = <span class="comment"># ... initialize cacahe to zeros</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> xrange(<span class="number">1</span>, big_number):</div><div class="line">    dx = <span class="comment"># ... evaluate gradient</span></div><div class="line">    m = beta1 * m + (<span class="number">1</span> - beta1) * dx <span class="comment"># update first momentum</span></div><div class="line">    v = beta2 * v + (<span class="number">1</span> - beta2) * (dx ** <span class="number">2</span>) <span class="comment"># update second momentum</span></div><div class="line">    mb = m / (<span class="number">1</span> - beta1 ** t) <span class="comment"># bias correction</span></div><div class="line">    vb = v / (<span class="number">1</span> - beta2 ** t) <span class="comment"># bias correction</span></div><div class="line">    x += - learning_rate * mb / (np.sqrt(vb) + <span class="number">1e-7</span>) <span class="comment"># RMSProp-like</span></div></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>The bias correction compensates for the fact that m,v are initialized at zero and need<br>some time to “warm up”. Only relevant in first few iterations when t is small.</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="Learning-rate-decay"><a href="#Learning-rate-decay" class="headerlink" title="Learning rate decay"></a>Learning rate decay</h3><p>主要是为了让 learning rate 随着训练时间的推移慢慢变小, 防止系统动能太大, 到最后在最优点旁边跳来跳去.</p>
<ul>
<li><strong>step decay</strong>: e.g. decay learning rate by half every few epochs.</li>
<li><strong>exponential decay</strong>: $\alpha = \alpha_0 e^{-kt}$</li>
<li><strong>1/t decay</strong>: $\alpha = \alpha_0 / (1+kt)$</li>
</ul>
<h3 id="Second-order-optimization-methods"><a href="#Second-order-optimization-methods" class="headerlink" title="Second order optimization methods"></a>Second order optimization methods</h3><p>主要是一些基于牛顿法的二阶最优化方法, 包括 L-BGFS 之类的. 其优点是根本就没有 learning rate 这个超参数, 而缺点则是 Hessian 矩阵实在是太大了, 非常耗费时间与空间, 因此在 DL 和 CNN 中基本不使用.</p>
<h2 id="Evaluation-Model-Ensembles"><a href="#Evaluation-Model-Ensembles" class="headerlink" title="Evaluation: Model Ensembles"></a>Evaluation: Model Ensembles</h2><ul>
<li><p>训练多个独立的模型, 然后在测试的时候对其结果进行平均, 一般能得到 2% 的额外性能提升;</p>
</li>
<li><p>平均单个模型的多个记录点 (check point) 上的参数, 也能获得一些提升</p>
</li>
<li><p>训练的时候对参数进行平滑操作, 并用于测试集 (keep track of (and use at test time) a running average</p>
<p>parameter vector)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">While <span class="keyword">True</span>:</div><div class="line">    data_batch = dataset.sample_data_batch()</div><div class="line">    loss = network.forward(data_batch)</div><div class="line">    dx = network.backward()</div><div class="line">    x += - learning_rate * dx</div><div class="line">    x_test = <span class="number">0.995</span> * x_test + <span class="number">0.005</span> * x <span class="comment"># use for test set</span></div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Regularization-dropout"><a href="#Regularization-dropout" class="headerlink" title="Regularization (dropout)"></a>Regularization (dropout)</h2><p>Dropout 算是很常用的一种方法了, 主要就是在前向传播的时候随机设置某些神经元为零 (“randomly set some neurons to zero in the forward pass”).</p>
<p> <img src="/images/dropout.png" alt="dropout"></p>
<p>其主要想法是让网络具有一定的冗余能力 (Forces the network to have a<br>redundant representation), 或者说是训练出了一个大的集成网络 (Dropout is training a large ensemble of models (that share parameters), each binary mask is one model, gets trained on only ~one datapoint.)</p>
<p> <img src="/images/dropout_a_good_idea.png" alt="dropout_a_good_idea"></p>
<p>具体实现如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">p = <span class="number">0.5</span> <span class="comment"># probability of keeping a unit active. higher = less dropout</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="string">""" X contains the datat """</span></div><div class="line"></div><div class="line">    <span class="comment"># forward pass for example 3-layer neural network</span></div><div class="line">    H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</div><div class="line">    U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># First dropout mask. Notice /p!</span></div><div class="line">    H1 *= U1 <span class="comment"># drop!</span></div><div class="line">    H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</div><div class="line">    U2 = (np.random.rand(*H2.shape) &lt; p) / p <span class="comment"># Second dropout mask. Notice /p!  </span></div><div class="line">    H2 *= U2 <span class="comment"># drop!</span></div><div class="line">    out = np.dot(W3, H2) + b3</div><div class="line"></div><div class="line">    <span class="comment"># backward pass: compute geadients ... (not shown)</span></div><div class="line">    <span class="comment"># parameter update... (not shown)</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="comment"># ensembled forward pass</span></div><div class="line">    H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) <span class="comment"># no scaling necessary</span></div><div class="line">    H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</div><div class="line">    out = np.dot(W3, H2) + b3</div></pre></td></tr></table></figure>
<h2 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h2><p>主要就是通过数值法计算梯度, 然后和通过后向传播得到的解析梯度比较, 看看误差大不大, 防止手贱算错梯度导致后面算法全乱了.</p>
<ol>
<li>用中心化公式$\frac{df(x)}{dx} = \frac{f(x+h - f(x-h)}{2h}$计算数值梯度, $h$取 $1e-5$ 左右.</li>
<li>使用相对误差$\frac{|f^{‘}_a - f^{‘}_n|}{max(|f^{‘}_a|, |f^{‘}_n|)}$</li>
</ol>
<p>同时还有些注意事项, 参见 <a href="http://cs231n.github.io/neural-networks-3/#gradcheck" target="_blank" rel="external">Gradient Checks</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://cs231n.github.io/neural-networks-1/&quot;&gt;Neural Networks Part 1: Setting up the Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cs231n.github.io/neural-networks-2/&quot;&gt;Neural Networks Part 2: Setting up the Data and the Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cs231n.github.io/neural-networks-3/&quot;&gt;Neural Networks Part 3: Learning and Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;或者可以看知乎专栏中的中文翻译:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit&quot;&gt;CS231n课程笔记翻译：神经网络笔记1（上）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit&quot;&gt;CS231n课程笔记翻译：神经网络笔记1（下）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit&quot;&gt;CS231n课程笔记翻译：神经网络笔记2 &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit&quot;&gt;CS231n课程笔记翻译：神经网络笔记3（上）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit&quot;&gt;CS231n课程笔记翻译：神经网络笔记3（下）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另外, 本文主要根据讲课的 Slides 上的顺序来, 与 Lecture Notes 的顺序略有不同.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Notes" scheme="http://www.yuthon.com/categories/Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://www.yuthon.com/tags/Deep-Learning/"/>
    
      <category term="CS231n" scheme="http://www.yuthon.com/tags/CS231n/"/>
    
      <category term="Neural Network" scheme="http://www.yuthon.com/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>B-Human Code 浅析 - ScanGridProvider</title>
    <link href="http://www.yuthon.com/2016/10/08/B-Human-Code-Brief-Analysis-2/"/>
    <id>http://www.yuthon.com/2016/10/08/B-Human-Code-Brief-Analysis-2/</id>
    <published>2016-10-08T11:18:19.000Z</published>
    <updated>2016-10-09T08:36:32.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>本系列的文章主要是根据 B-Human 的开源代码库<code>CodeRelease2015</code>以及历年的 Team Description Paper 写成. 由于个人的 C++ 水平有限, 难免有缺漏之处, 望发现后不吝赐教.</p>
</blockquote>
<h1 id="B-Human-Code-浅析"><a href="#B-Human-Code-浅析" class="headerlink" title="B-Human Code 浅析"></a>B-Human Code 浅析</h1><h2 id="Perception"><a href="#Perception" class="headerlink" title="Perception"></a>Perception</h2><h3 id="ScanGrid"><a href="#ScanGrid" class="headerlink" title="ScanGrid"></a>ScanGrid</h3><p>首先来看一个基础的类<code>ScanGrid</code>, 里面定义了由扫描线组成的网格. 这个类在<code>ScanGridProvider</code>以及<code>LineScanner</code> 中都有用到. </p>
<a id="more"></a>
<p>头文件如下:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">STREAMABLE(ScanGrid,</div><div class="line">&#123;</div><div class="line">  STREAMABLE(Line,</div><div class="line">  &#123;</div><div class="line">    Line() = <span class="keyword">default</span>;</div><div class="line">    Line(<span class="keyword">int</span> x, <span class="keyword">int</span> y, <span class="keyword">unsigned</span> yMaxIndex),</div><div class="line"></div><div class="line">    (<span class="keyword">int</span>) x, <span class="comment">/**&lt; x coordinate of the scanline. */</span></div><div class="line">    (<span class="keyword">int</span>) yMax, <span class="comment">/**&lt; Maximum y coordinate (exclusive). */</span></div><div class="line">    (<span class="keyword">unsigned</span>) yMaxIndex, <span class="comment">/**&lt; Index of the lowest y coordinate relevant for this scanline. */</span></div><div class="line">  &#125;);</div><div class="line"></div><div class="line">  <span class="keyword">void</span> draw() <span class="keyword">const</span>,</div><div class="line"></div><div class="line">  (<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;) y, <span class="comment">/**&lt; All possible y coordinates of pixels to be scanned. */</span></div><div class="line">  (<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Line&gt;) lines, <span class="comment">/**&lt; Decription of all scanlines. */</span></div><div class="line">  (<span class="keyword">int</span>)(<span class="number">0</span>) fieldLimit, <span class="comment">/**&lt; Upper bound for all scanlines (exclusive). */</span></div><div class="line">  (<span class="keyword">unsigned</span>)(<span class="number">0</span>) lowResStart, <span class="comment">/**&lt; First index of low res grid. */</span></div><div class="line">  (<span class="keyword">unsigned</span>)(<span class="number">1</span>) lowResStep, <span class="comment">/**&lt; Steps between low res grid lines. */</span></div><div class="line">&#125;);</div><div class="line"></div><div class="line"><span class="keyword">inline</span> ScanGrid::Line::Line(<span class="keyword">int</span> x, <span class="keyword">int</span> yMax, <span class="keyword">unsigned</span> yMaxIndex) :</div><div class="line">  x(x), yMax(yMax), yMaxIndex(yMaxIndex)</div><div class="line">&#123;&#125;</div></pre></td></tr></table></figure>
<p>原本的注释已经很清楚了, 我就不再多说什么.</p>
<h3 id="ScanGridProvider"><a href="#ScanGridProvider" class="headerlink" title="ScanGridProvider"></a>ScanGridProvider</h3><p>这个模块主要是提供了一个由一堆竖线组成的扫描图像的网格. 生成的网格可供<code>LineScanner</code>使用. 头文件如下:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">MODULE(ScanGridProvider,</div><div class="line">&#123;,</div><div class="line">  REQUIRES(BodyContour),</div><div class="line">  REQUIRES(CameraInfo),</div><div class="line">  REQUIRES(CameraMatrix),</div><div class="line">  REQUIRES(FieldDimensions),</div><div class="line">  PROVIDES(ScanGrid),</div><div class="line">  DEFINES_PARAMETERS(</div><div class="line">  &#123;,</div><div class="line">    (<span class="keyword">int</span>)(<span class="number">3</span>) minStepSize, <span class="comment">/**&lt; The minimum pixel distance between two neigboring scanlines. */</span></div><div class="line">    (<span class="keyword">int</span>)(<span class="number">25</span>) minNumOfLowResScanlines, <span class="comment">/**&lt; The minimum number of scanlines for low resolution. */</span></div><div class="line">    (<span class="keyword">float</span>)(<span class="number">0.9f</span>) lineWidthRatio, <span class="comment">/**&lt; The ratio of field line width that is sampled when scanning the image. */</span></div><div class="line">    (<span class="keyword">float</span>)(<span class="number">0.8f</span>) ballWidthRatio, <span class="comment">/**&lt; The ratio of ball width that is sampled when scanning the image. */</span></div><div class="line">  &#125;),</div><div class="line">&#125;);</div><div class="line"></div><div class="line"><span class="keyword">class</span> ScanGridProvider : <span class="keyword">public</span> ScanGridProviderBase</div><div class="line">&#123;</div><div class="line">  <span class="function"><span class="keyword">void</span> <span class="title">update</span><span class="params">(ScanGrid&amp; scanGrid)</span></span>;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>看看具体实现.</p>
<p>首先是一段初始化以及是否继续生成网格的判断:</p>
<ul>
<li>摄像头参数矩阵未给定</li>
<li>场地边界上的最远点不在视野内</li>
<li>场地边界高度超过图像高度(视野完全在场地之内?)</li>
<li>不能将图像左下与右下两点射影到场地上(视野完全在场地之外?)</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">void</span> ScanGridProvider::update(ScanGrid&amp; scanGrid)</div><div class="line">&#123;</div><div class="line">  <span class="comment">// 初始化网格信息</span></div><div class="line">  scanGrid.y.clear();</div><div class="line">  scanGrid.lines.clear();</div><div class="line"></div><div class="line">  <span class="keyword">if</span>(!theCameraMatrix.isValid)</div><div class="line">    <span class="keyword">return</span>; <span class="comment">// Cannot compute grid without camera matrix</span></div><div class="line"></div><div class="line">  <span class="comment">// Compute the furthest point away that could be part of the field given an unknown own position.</span></div><div class="line">  Vector2f pointInImage;</div><div class="line">  <span class="keyword">const</span> <span class="keyword">float</span> fieldDiagional = Vector2f(theFieldDimensions.boundary.x.getSize(), theFieldDimensions.boundary.y.getSize()).norm();</div><div class="line">  <span class="keyword">if</span>(!Transformation::robotWithCameraRotationToImage(Vector2f(fieldDiagional, <span class="number">0</span>), theCameraMatrix, theCameraInfo, pointInImage))</div><div class="line">    <span class="keyword">return</span>; <span class="comment">// Cannot project furthest possible point to image -&gt; no grid in image</span></div><div class="line"></div><div class="line">  scanGrid.fieldLimit = <span class="built_in">std</span>::max(<span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(pointInImage.y()), <span class="number">-1</span>);</div><div class="line">  <span class="keyword">if</span>(scanGrid.fieldLimit &gt;= theCameraInfo.height)</div><div class="line">    <span class="keyword">return</span>; <span class="comment">// Image is above field limit -&gt; no grid in image</span></div><div class="line"></div><div class="line">  <span class="comment">// Determine the maximum distance between scanlines at the bottom of the image not to miss the ball.</span></div><div class="line">  Vector2f leftOnField;</div><div class="line">  Vector2f rightOnField;</div><div class="line">  <span class="keyword">if</span>(!Transformation::imageToRobotWithCameraRotation(Vector2i(<span class="number">0</span>, theCameraInfo.height - <span class="number">1</span>), theCameraMatrix, theCameraInfo, leftOnField) ||</div><div class="line">     !Transformation::imageToRobotWithCameraRotation(Vector2i(theCameraInfo.width, theCameraInfo.height - <span class="number">1</span>), theCameraMatrix, theCameraInfo, rightOnField))</div><div class="line">    <span class="keyword">return</span>; <span class="comment">// Cannot project lower image border to field -&gt; no grid</span></div></pre></td></tr></table></figure>
<p>接下来是要设置 x 方向的最大步长, 主要的考虑因素是扫描线的最小数量, 以及底部预计球的大小.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> xStepUpperBound = theCameraInfo.width / minNumOfLowResScanlines;</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxXStep = <span class="built_in">std</span>::min(xStepUpperBound,</div><div class="line">                              <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(theCameraInfo.width *</div><div class="line">                                               theFieldDimensions.ballRadius * <span class="number">2.f</span> * </div><div class="line">                                               ballWidthRatio / </div><div class="line">                                               (leftOnField - rightOnField).norm()));</div></pre></td></tr></table></figure>
<p>之后自下而上选取能够用来采样的 y 的值</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">Vector2f pointOnField = (leftOnField + rightOnField) / <span class="number">2.f</span>;</div><div class="line"></div><div class="line"><span class="comment">// Determine vertical sampling points of the grid</span></div><div class="line">scanGrid.y.reserve(theCameraInfo.height);</div><div class="line"><span class="keyword">const</span> <span class="keyword">float</span> fieldStep = theFieldDimensions.fieldLinesWidth * lineWidthRatio;</div><div class="line"><span class="keyword">bool</span> singleSteps = <span class="literal">false</span>;</div><div class="line"><span class="comment">// scanGrid.fieldLimit: Upper bound for all scanlines (exclusive).</span></div><div class="line"><span class="keyword">for</span>(<span class="keyword">int</span> y = theCameraInfo.height - <span class="number">1</span>; y &gt; scanGrid.fieldLimit;)</div><div class="line">&#123;</div><div class="line">  scanGrid.y.emplace_back(y);</div><div class="line">  <span class="comment">// Calc next vertical position for all scanlines.</span></div><div class="line">  <span class="keyword">if</span>(singleSteps)</div><div class="line">    --y;</div><div class="line">  <span class="keyword">else</span></div><div class="line">  &#123;</div><div class="line">    pointOnField.x() += fieldStep;</div><div class="line">    <span class="keyword">if</span>(!Transformation::robotWithCameraRotationToImage(pointOnField, theCameraMatrix, theCameraInfo, pointInImage))</div><div class="line">      <span class="keyword">break</span>;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> y2 = y;</div><div class="line">    y = <span class="built_in">std</span>::min(y2 - <span class="number">1</span>, <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(pointInImage.y() + <span class="number">0.5</span>));</div><div class="line">    singleSteps = y2 - <span class="number">1</span> == y;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>接下来是要设置 x 方向的最小步长, 主要的考虑因素是图像顶部预计球的大小.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Determine the maximum distance between scanlines at the top of the image not to miss the ball. Do not go below minStepSize.</span></div><div class="line"><span class="keyword">int</span> minXStep = minStepSize;</div><div class="line"><span class="keyword">if</span>(Transformation::imageToRobotWithCameraRotation(Vector2i(<span class="number">0</span>, <span class="number">0</span>), theCameraMatrix, theCameraInfo, leftOnField) &amp;&amp;</div><div class="line">   Transformation::imageToRobotWithCameraRotation(Vector2i(theCameraInfo.width, <span class="number">0</span>), theCameraMatrix, theCameraInfo, rightOnField))</div><div class="line">  minXStep = <span class="built_in">std</span>::max(minXStep, <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(theCameraInfo.width *</div><div class="line">                                                 theFieldDimensions.ballRadius *</div><div class="line">                                                 <span class="number">2.f</span> * ballWidthRatio / </div><div class="line">                                                 (leftOnField - rightOnField).norm()));</div><div class="line">minXStep = <span class="built_in">std</span>::min(xStepUpperBound, minXStep);</div></pre></td></tr></table></figure>
<p>然后是确认一个 x 方向的次大步长, 主要是满足$maxXStep2 = minXStep * 2^n, maxXStep2 &lt;= maxXStep$</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Determine a max step size that fulfills maxXStep2 = minXStep * 2^n, maxXStep2 &lt;= maxXStep.</span></div><div class="line"><span class="comment">// Also compute lower y coordinates for the different lengths of scanlines.</span></div><div class="line"><span class="keyword">int</span> maxXStep2 = minXStep;</div><div class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; yStarts;</div><div class="line"><span class="keyword">while</span>(maxXStep2 * <span class="number">2</span> &lt;= maxXStep)</div><div class="line">&#123;</div><div class="line">  <span class="keyword">float</span> distance = Geometry::getDistanceBySize(theCameraInfo, theFieldDimensions.ballRadius * ballWidthRatio, <span class="keyword">static_cast</span>&lt;<span class="keyword">float</span>&gt;(maxXStep2));</div><div class="line">  VERIFY(Transformation::robotWithCameraRotationToImage(Vector2f(distance, <span class="number">0</span>), theCameraMatrix, theCameraInfo, pointInImage));</div><div class="line">  yStarts.push_back(<span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(pointInImage.y() + <span class="number">0.5f</span>));</div><div class="line">  maxXStep2 *= <span class="number">2</span>;</div><div class="line">&#125;</div><div class="line">yStarts.push_back(theCameraInfo.height);</div></pre></td></tr></table></figure>
<p>根据上一步计算出的<code>maxXStep2</code>, 建立一个2为比值的等比数列作为扫描线的长度.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Determine a pattern with the different lengths of scan lines, in which the longest appears once,</span></div><div class="line"><span class="comment">// the second longest twice, etc. The pattern starts with the longest.</span></div><div class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; yStarts2(maxXStep2 / minXStep);</div><div class="line"><span class="keyword">for</span>(<span class="keyword">size_t</span> i = <span class="number">0</span>, step = <span class="number">1</span>; i &lt; yStarts.size(); ++i, step *= <span class="number">2</span>)</div><div class="line">  <span class="keyword">for</span>(<span class="keyword">size_t</span> j = <span class="number">0</span>; j &lt; yStarts2.size(); j += step)</div><div class="line">    yStarts2[j] = yStarts[i];</div></pre></td></tr></table></figure>
<p>初始化扫描线与颜色区域的信息</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Initialize the scan states and the regions.</span></div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> xStart = theCameraInfo.width % (theCameraInfo.width / minXStep - <span class="number">1</span>) / <span class="number">2</span>;</div><div class="line">scanGrid.lines.reserve((theCameraInfo.width - xStart) / minXStep);</div><div class="line"><span class="keyword">size_t</span> i = yStarts2.size() / <span class="number">2</span>; <span class="comment">// Start with the second longest scanline.</span></div><div class="line"><span class="keyword">for</span>(<span class="keyword">int</span> x = xStart; x &lt; theCameraInfo.width; x += minXStep)</div><div class="line">&#123;</div><div class="line">  <span class="keyword">int</span> yMax = <span class="built_in">std</span>::min(yStarts2[i++], theCameraInfo.height);</div><div class="line">  i %= yStarts2.size();</div><div class="line">  theBodyContour.clipBottom(x, yMax);</div><div class="line">  <span class="keyword">const</span> <span class="keyword">size_t</span> yMaxIndex = <span class="built_in">std</span>::upper_bound(scanGrid.y.begin(), scanGrid.y.end(), yMax + <span class="number">1</span>, <span class="built_in">std</span>::greater&lt;<span class="keyword">int</span>&gt;()) - scanGrid.y.begin();</div><div class="line">  scanGrid.lines.emplace_back(x, yMax, <span class="keyword">static_cast</span>&lt;<span class="keyword">unsigned</span>&gt;(yMaxIndex));</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>设置低分辨率的扫描线的信息</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Set low resolution scanline info</span></div><div class="line">scanGrid.lowResStep = maxXStep2 / minXStep;</div><div class="line">scanGrid.lowResStart = scanGrid.lowResStep / <span class="number">2</span>;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本系列的文章主要是根据 B-Human 的开源代码库&lt;code&gt;CodeRelease2015&lt;/code&gt;以及历年的 Team Description Paper 写成. 由于个人的 C++ 水平有限, 难免有缺漏之处, 望发现后不吝赐教.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;B-Human-Code-浅析&quot;&gt;&lt;a href=&quot;#B-Human-Code-浅析&quot; class=&quot;headerlink&quot; title=&quot;B-Human Code 浅析&quot;&gt;&lt;/a&gt;B-Human Code 浅析&lt;/h1&gt;&lt;h2 id=&quot;Perception&quot;&gt;&lt;a href=&quot;#Perception&quot; class=&quot;headerlink&quot; title=&quot;Perception&quot;&gt;&lt;/a&gt;Perception&lt;/h2&gt;&lt;h3 id=&quot;ScanGrid&quot;&gt;&lt;a href=&quot;#ScanGrid&quot; class=&quot;headerlink&quot; title=&quot;ScanGrid&quot;&gt;&lt;/a&gt;ScanGrid&lt;/h3&gt;&lt;p&gt;首先来看一个基础的类&lt;code&gt;ScanGrid&lt;/code&gt;, 里面定义了由扫描线组成的网格. 这个类在&lt;code&gt;ScanGridProvider&lt;/code&gt;以及&lt;code&gt;LineScanner&lt;/code&gt; 中都有用到. &lt;/p&gt;
    
    </summary>
    
      <category term="Code Anaylsis" scheme="http://www.yuthon.com/categories/Code-Anaylsis/"/>
    
    
      <category term="RoboCup" scheme="http://www.yuthon.com/tags/RoboCup/"/>
    
      <category term="C++" scheme="http://www.yuthon.com/tags/C/"/>
    
  </entry>
  
</feed>
