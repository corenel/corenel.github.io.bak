<!doctype html><html class="theme-next muse use-motion"><head><!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="BGEL63KNRW25AkpCy3shpRdMWDHE9LZTAFS3XSHtFK8"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.0.2" rel="stylesheet" type="text/css"><meta name="keywords" content="Deep Learning,ScribbleSup,Scene Segmentation,"><link rel="alternate" href="/atom.xml" title="Yuthon's blog" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.2"><meta name="description" content="毕设需要写一个图像标注的软件, 来给场景分割的数据集做标注. 经学长推荐, 看了今年的这篇文章, 作者中竟然还有 Kaiming He 大神, 给微软膜一秒.
这篇文章讲了一个弱监督的场景分割的算法 ScribbleSup, 主要是先通过 Graph Cut 将输入的 scribble 信息广播到没有标注的像素, 然后用 FCN 来做像素级别的预测. 令人遗憾的是 Github 上并没有人实现 ("><meta property="og:type" content="article"><meta property="og:title" content="Thesis Notes for ScribbleSup"><meta property="og:url" content="http://www.yuthon.com/2016/11/20/Thesis-Notes-for-ScribbleSup/index.html"><meta property="og:site_name" content="Yuthon's blog"><meta property="og:description" content="毕设需要写一个图像标注的软件, 来给场景分割的数据集做标注. 经学长推荐, 看了今年的这篇文章, 作者中竟然还有 Kaiming He 大神, 给微软膜一秒.
这篇文章讲了一个弱监督的场景分割的算法 ScribbleSup, 主要是先通过 Graph Cut 将输入的 scribble 信息广播到没有标注的像素, 然后用 FCN 来做像素级别的预测. 令人遗憾的是 Github 上并没有人实现 ("><meta property="og:image" content="http://www.yuthon.com/images/ScribbleSup_grapgical_model.png"><meta property="og:image" content="http://www.yuthon.com/images/ScribbleSup_training.png"><meta property="og:updated_time" content="2016-11-21T14:01:47.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Thesis Notes for ScribbleSup"><meta name="twitter:description" content="毕设需要写一个图像标注的软件, 来给场景分割的数据集做标注. 经学长推荐, 看了今年的这篇文章, 作者中竟然还有 Kaiming He 大神, 给微软膜一秒.
这篇文章讲了一个弱监督的场景分割的算法 ScribbleSup, 主要是先通过 Graph Cut 将输入的 scribble 信息广播到没有标注的像素, 然后用 FCN 来做像素级别的预测. 令人遗憾的是 Github 上并没有人实现 ("><meta name="twitter:image" content="http://www.yuthon.com/images/ScribbleSup_grapgical_model.png"><script type="text/javascript" id="hexo.configuration">var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    }
  };</script><link rel="canonical" href="http://www.yuthon.com/2016/11/20/Thesis-Notes-for-ScribbleSup/"><title>Thesis Notes for ScribbleSup | Yuthon's blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body itemscope itemtype="//schema.org/WebPage" lang="en"><!-- hexo-inject:begin --><!-- hexo-inject:end --><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-76233259-1', 'auto');
  ga('send', 'pageview');</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="//schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Yuthon's blog</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section">Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section">Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section">Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section">Tags</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="//schema.org/Article"><header class="post-header"><h1 class="post-title" itemprop="name headline">Thesis Notes for ScribbleSup</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time itemprop="dateCreated" datetime="2016-11-20T18:26:24+08:00" content="2016-11-20">2016-11-20 </time></span><span class="post-updated">&nbsp; | &nbsp; Updated on <time itemprop="dateUpdated" datetime="2016-11-21T22:01:47+08:00" content="2016-11-21">2016-11-21 </time></span><span class="post-category">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a href="/categories/Thesis-Notes/" itemprop="url" rel="index"><span itemprop="name">Thesis Notes</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><p>毕设需要写一个图像标注的软件, 来给场景分割的数据集做标注. 经学长推荐, 看了今年的这篇文章, 作者中竟然还有 Kaiming He 大神, 给微软膜一秒.</p><p>这篇文章讲了一个弱监督的场景分割的算法 ScribbleSup, 主要是先通过 Graph Cut 将输入的 scribble 信息广播到没有标注的像素, 然后用 FCN 来做像素级别的预测. 令人遗憾的是 Github 上并没有人实现 (不能偷懒了TAT).</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>TBD</p><h2 id="Scribble-Supervised-Learning"><a href="#Scribble-Supervised-Learning" class="headerlink" title="Scribble-Supervised Learning"></a>Scribble-Supervised Learning</h2><h3 id="Objective-Functions"><a href="#Objective-Functions" class="headerlink" title="Objective Functions"></a>Objective Functions</h3><p>主要用到的记号如下:</p><table><thead><tr><th>Symbol</th><th>Name</th><th>Note</th></tr></thead><tbody><tr><td>$X$</td><td>training image</td><td></td></tr><tr><td>${x_i}$</td><td>set of non-overlapping superpixles</td><td>$\cup_i x_i = X; x_i \cap x_j = \varnothing, \forall i,j$</td></tr><tr><td>$S$</td><td>scribble annotations of image</td><td>$S={s_k, c_k}$</td></tr><tr><td>$s_k$</td><td>the pixels of a scribble $k$</td><td></td></tr><tr><td>$c_k$</td><td>the scribble’s category label</td><td>$0 \le c_k \le C$; $c_k=0$ for background</td></tr><tr><td>$Y$ or ${y_i}$</td><td>the category label of ${x_i}$</td><td>provides full annotations of the image</td></tr></tbody></table><p>定义目标函数为</p><p>$$\sum_i \psi_i (y_i | X,S) + \sum_{i,j} \psi_{ij} (y_i, y_j | X)$$</p><p>其中$\psi_i$是一个关于$x_i$的一元项 (unary term), 而$\psi\ _{ij}$是关于$x_i$与$x_j$的成对项 (pairwise term).</p><ul><li><p>$\psi _i$由两个部分组成, 一个是$\psi ^{scr}_i$, 另一个是$\psi^{net}_i$.两者权重相同, $\psi _i = \psi^{scr} _i + \psi^{net} _i$.</p><ul><li><p>$\psi ^{scr}_i$ 基于 scribble, 定义如下:<br>$$<br>\psi ^{scr}_i=<br>\begin{aligned}<br>&amp;0 &amp; \text{if $y_i=c_k$ and $x_i \cap s_k \ne \varnothing$}\\<br>&amp;-log(\frac{1}{|c_k|}) &amp; \text{if $y_i \in {c_k}$ and $x_i \cap S = \varnothing$} \\<br>&amp;\infty &amp; \text{otherwise} \\<br>\end{aligned}<br>$$</p><ul><li>当$x_i$与$s_k$有交集, 且标签是分到$c_k$时, 则$cost=0$</li><li>当$x_i$与所有 scribble 都没有交集, 则它可以被等概率地分给任何标签. 当然, $y_i$需要在${c_k}$之内. 此处$|{c_k}|$表示标签集内元素个数.</li><li>如果不是以上两种情况, 则$cost= \infty$</li></ul></li><li><p>$\psi ^{net}_i$基于 FCN 的输出, 定义为<br>$$<br>\psi^{net}_i (y_i) = -log P(y_i|X, \Theta)<br>$$</p><ul><li>$\Theta$表示网络的参数</li><li>$log P(y_i|X, \Theta)$表示了$x_i$属于标签$y_i$的对数概率, 实际上是$x_i$内所有像素的对数概率之和.</li></ul></li></ul></li><li><p>$\psi_{ij}$用以衡量相邻的两个超像素的相似程度, 主要是用色彩直方图与纹理直方图来量化 (均已归一化).<br>$$<br>\psi_{ij} (y_i, y_j | X) = [y_i \ne y_j] exp \left( -\frac{||h_c(x_i) - h_c(x_j)||^2_2}{\delta_c^2} - \frac{||h_t(x_i) - h_t(x_j)||^2_2}{\delta_t^2} \right)<br>$$</p><ul><li>$h_c(x_i)$ 表示RGB三个 channel 每个 channel 分成 25 bins 的色彩直方图</li><li>$h_t(x_i)$ 表示横向与纵向的梯度直方图, 每个方向 10 bins</li><li>$[\cdot]$表示一个符号函数, 条件为真则为$1$, 否则为$0$</li><li>$\delta_c=5, \delta_t = 10$</li><li>对于不是同一个标签的临近超像素来说, 它们间的外观越相似, 则 cost 越大</li></ul></li></ul><p>最后把上边这些合起来, 就成了一个对于以下式子进行最优化的问题:<br>$$<br>\sum_i \psi^{scr}_i (y_i |X, S) + \sum_i -log P(y_i | X, \Theta) + \sum_{i,j} \psi_{ij} (y_i, y_j | X)<br>$$<br>其中有两组变量, 一个是所有超像素的标签$Y={y_i}$, 另一个是 FCN 的参数 $\Theta$.</p><p><img src="/images/ScribbleSup_grapgical_model.png" alt="ScribbleSup_grapgical_model"></p><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>论文里采用的是一种交替优化的方法:</p><ul><li>$\Theta$固定, 优化$Y$, slover 基于 scribbles, appearance 以及 FCN 网络的预测, 将标签传播到未标记的像素中</li><li>$Y$固定, 优化$\Theta$, slover 对 pixel-wise 的语义分割的 FCN 进行学习</li></ul><p>具体的来说就是</p><ul><li><p><strong>Propagating scribble information to unmarked pixels</strong></p><p>当$\Theta$固定时, 一元项$\psi _i = \psi^{scr} _i + \psi^{net} _i$能够用列举所有可能的标签$0 \le y_i \le C$得到, 成对项也能够预先计算生成一个 look-up table. 因此, 优化问题就能用 graph cut 的方法来解决. 论文里用的是<a href="http://www.csd.uwo.ca/faculty/yuri/Papers/pami04.pdf" target="_blank" rel="external">这一篇文章</a>的<a href="http://vision.csd.uwo.ca/code/gco- v3.0.zip" target="_blank" rel="external">现成代码</a>.</p></li><li><p><strong>Optimizing network parameters</strong></p><p>前一步做完后, 所有超像素的标签都已经定好了, 也就是说$Y$固定了. 之后优化$\Theta$就相当于用$Y$做为监督来训练 FCN. $Y$有了, 那么每个像素的标签就有了, 然后 FCN 面对的就是一个 pixel-wise 的回归问题. FCN 的最后一层输出的就是每个像素的分类的对数概率, 可以用来更新 graph 上的一元项.</p></li></ul><p>训练的时候有几点需要注意:</p><ul><li>初始化的时候没有 network prediction, 因此就是直接用 graph cut 初始化的. 之后则是在两步之间不断迭代.</li><li>每次 network optimizing step 的时候, 前50k次用0.0003的 learning rate, 后10k次用0.0001的 learning rate, batch size 为 8.</li><li>每次 network optimizing step 都是从一个 pre-trained 的 model (比如 VGG-16) 重新初始化的. 作者也试过复用上一次迭代后的权重, 但是效果不是很理想. 似乎是由于本来标签就不可靠, 导致训练的时候参数被调到了不太好的局部最优里面.</li><li>基本上3次迭代就能得到比较好的效果了, 再多得到的提升微乎其微.</li><li>做验证的时候只要用 FCN 就好了, 超像素和 graph model 之类的都只是用来训练用的.</li><li>Post-process 用了 CRF.</li></ul><p>迭代结果如下:</p><p><img src="/images/ScribbleSup_training.png" alt="ScribbleSup_training"></p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="Graphical-models-for-segmentation"><a href="#Graphical-models-for-segmentation" class="headerlink" title="Graphical models for segmentation"></a>Graphical models for segmentation</h4><p>Graphical model 在交互式的图像分割和语义分割领域是很常见的, 通常是目标函数包含了一元项和成对项, 特别适用于对局部和全局的空间约束的建模.</p><p>有趣的是, FCN 作为目前最成功的语义分割的方法之一, 由于做的是 pixel-wise 的 regression, 因此其目标函数只有一元项. 不过像 CRF/MRF 这样给 FCN 做 post-processing 或是 joint-training 的方法在之后也发展起来了.</p><p>但是这一类 graph model 都是强监督的, 主要工作是在优化 mask 的边缘, 而 ScribbleSup 里面的 graph model 主要是用来把标签传播到其他未标注的像素上. 同时, 这类方法是 pixel-based, 而 ScribbleSup 是 super-pixel-based.</p><h4 id="Weakly-supervised-semantic-segmentation"><a href="#Weakly-supervised-semantic-segmentation" class="headerlink" title="Weakly-supervised semantic segmentation"></a>Weakly-supervised semantic segmentation</h4><p>用 CNN/FCN 来做弱监督的语义分割的方法很多, 用的标注方法也有很多种.</p><ul><li>Image-level 的标注很容易获取, 但是只用这个的话精度远低于强监督的结果</li><li>Box-level 的相比较而言结果与强监督的接近了不少. 由于 Box annotations 本身就提供了物体边缘以及可信的背景区域的信息, 因此就不需要 graph model 来传播标签.</li></ul><p>这些方法和本篇论文里面讲的 ScribbleSup 比起来到底哪个更胜一筹, 姿势水平更高, 就看下面的实验了.</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Annotating-Scribbles"><a href="#Annotating-Scribbles" class="headerlink" title="Annotating Scribbles"></a>Annotating Scribbles</h3><p>主要使用了 PASCAL VOC 2012 (20个分类) 以及 PASCAL-CONTEXT (59个分类) 这两个数据集, 同时也标注了 PASCAL VOC 2007 (标注了59个分类). 不过 2007 没有 mask-level 的标注.</p><p>总共有10个人在标注, 每张图片一人标注一人检查. 平均下来20分类的话每张图片25秒, 59分类的话每张图片50秒, 算是相当快的了.</p><p>同时, 保证每个 object 上的 scribble 至少有其 bounding box 长边的 70% 以上的长度.</p><h3 id="Experiments-on-PASCAL-VOC-2012"><a href="#Experiments-on-PASCAL-VOC-2012" class="headerlink" title="Experiments on PASCAL VOC 2012"></a>Experiments on PASCAL VOC 2012</h3><h4 id="Strategies-of-utilizing-scribbles"><a href="#Strategies-of-utilizing-scribbles" class="headerlink" title="Strategies of utilizing scribbles"></a>Strategies of utilizing scribbles</h4><p>ScribbleSup 是将标签的扩散与网络的训练合起来考虑的, 但是一个更为简单的方案是把这两步分开来, 先用一些现成的工具 (比如说 GrabCut 或者是 LazySnapping) 把 scribble 转换成 mask, 然后再来训练 FCN 网络. 这个方案听起来也是很吼的, 那么中央到底兹不兹瓷呢, 我们来看看实验结果</p><table><thead><tr><th>Method</th><th>mIoU(%)</th></tr></thead><tbody><tr><td>GrabCut + FCN</td><td>49.1</td></tr><tr><td>LazySnapping + FCN</td><td>53.8</td></tr><tr><td>ours, w/o pairwise terms</td><td>60.5</td></tr><tr><td>ours, w/ pairwise terms</td><td>63.1</td></tr></tbody></table><p>所以说不要听风就是雨, 可以看出分两步走的方案是一个错误的道路, mIoU显著低于 ScribbleSup. 其中的原因主要是这些传统的方法仅仅针对 low-level 的空间或者是色彩信息建模, 并没有考虑到语义的层面. 也就是说, 这些方法得到的 mask 是不值得信赖的, 不能作为 ground truth 来用.</p><p>而 ScribbleSup 就不同了, 通过不断的迭代, FCN 能够逐渐学习到 high-level 的语义特征, 这些特征又能反哺给 graph-based scribble propagation. 这样就形成了一个良性循环, 自然 mIoU 就不知比传统方法高到哪里去了.</p><p>(To be continued…)</p></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Deep-Learning/" rel="tag">#Deep Learning</a> <a href="/tags/ScribbleSup/" rel="tag">#ScribbleSup</a> <a href="/tags/Scene-Segmentation/" rel="tag">#Scene Segmentation</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2016/11/18/Thesis-Notes-for-YOLO/" rel="next" title="Thesis Notes for YOLO"><i class="fa fa-chevron-left"></i> Thesis Notes for YOLO</a></div><div class="post-nav-prev post-nav-item"></div></div></footer></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person"><img class="site-author-image" itemprop="image" src="/uploads/avatar.png" alt="Yusu Pan"><p class="site-author-name" itemprop="name">Yusu Pan</p><p class="site-description motion-element" itemprop="description">We've been gaining one good thing through losing another.</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">30</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories"><span class="site-state-item-count">5</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">37</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/corenel" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="https://twitter.com/corenel" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i> Twitter </a></span><span class="links-of-author-item"><a href="http://www.zhihu.com/people/pan-yu-su" target="_blank" title="Zhihu"><i class="fa fa-fw fa-zhihu"></i> Zhihu</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Scribble-Supervised-Learning"><span class="nav-number">2.</span> <span class="nav-text">Scribble-Supervised Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Objective-Functions"><span class="nav-number">2.1.</span> <span class="nav-text">Objective Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimization"><span class="nav-number">2.2.</span> <span class="nav-text">Optimization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Related-Work"><span class="nav-number">2.3.</span> <span class="nav-text">Related Work</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Graphical-models-for-segmentation"><span class="nav-number">2.3.1.</span> <span class="nav-text">Graphical models for segmentation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Weakly-supervised-semantic-segmentation"><span class="nav-number">2.3.2.</span> <span class="nav-text">Weakly-supervised semantic segmentation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Experiment"><span class="nav-number">3.</span> <span class="nav-text">Experiment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Annotating-Scribbles"><span class="nav-number">3.1.</span> <span class="nav-text">Annotating Scribbles</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Experiments-on-PASCAL-VOC-2012"><span class="nav-number">3.2.</span> <span class="nav-text">Experiments on PASCAL VOC 2012</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Strategies-of-utilizing-scribbles"><span class="nav-number">3.2.1.</span> <span class="nav-text">Strategies of utilizing scribbles</span></a></li></ol></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 - <span itemprop="copyrightYear">2016</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Yusu Pan</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.0.2"></script><script type="text/javascript" src="/js/src/motion.js?v=5.0.2"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.2"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.0.2"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.2"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>