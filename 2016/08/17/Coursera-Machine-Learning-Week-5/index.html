<!doctype html><html class="theme-next mist use-motion"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="BGEL63KNRW25AkpCy3shpRdMWDHE9LZTAFS3XSHtFK8"><link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css"><meta name="keywords" content="Coursera,Machine Learning,"><link rel="alternate" href="/atom.xml" title="Yuthon's blog" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1"><meta name="description" content="Neural Networks: LearningCost Function and BackpropagationCost FunctionLet’s first define a few variables that we will need to use:

$L$ = total number of layers in the network
$s_l$ = number of units"><meta property="og:type" content="article"><meta property="og:title" content="Notes for Machine Learning - Week 5"><meta property="og:url" content="http://www.yuthon.com/2016/08/17/Coursera-Machine-Learning-Week-5/index.html"><meta property="og:site_name" content="Yuthon's blog"><meta property="og:description" content="Neural Networks: LearningCost Function and BackpropagationCost FunctionLet’s first define a few variables that we will need to use:

$L$ = total number of layers in the network
$s_l$ = number of units"><meta property="og:image" content="http://www.yuthon.com/images/gradient_computation.png"><meta property="og:image" content="http://www.yuthon.com/images/forward_propagation_intuition.png"><meta property="og:image" content="http://www.yuthon.com/images/backward_propagation_intuition.png"><meta property="og:updated_time" content="2016-08-28T02:13:08.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Notes for Machine Learning - Week 5"><meta name="twitter:description" content="Neural Networks: LearningCost Function and BackpropagationCost FunctionLet’s first define a few variables that we will need to use:

$L$ = total number of layers in the network
$s_l$ = number of units"><meta name="twitter:image" content="http://www.yuthon.com/images/gradient_computation.png"><script type="text/javascript" id="hexo.configuration">var NexT=window.NexT||{},CONFIG={scheme:"Mist",sidebar:{position:"left",display:"hide"},fancybox:!0,motion:!0,duoshuo:{userId:0,author:"Author"}}</script><link rel="canonical" href="http://www.yuthon.com/2016/08/17/Coursera-Machine-Learning-Week-5/"><title> Notes for Machine Learning - Week 5 | Yuthon's blog</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script>!function(e,a,t,n,c,o,s){e.GoogleAnalyticsObject=c,e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},e[c].l=1*new Date,o=a.createElement(t),s=a.getElementsByTagName(t)[0],o.async=1,o.src=n,s.parentNode.insertBefore(o,s)}(window,document,"script","//www.google-analytics.com/analytics.js","ga"),ga("create","UA-76233259-1","auto"),ga("send","pageview")</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Yuthon's blog</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> Tags</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><header class="post-header"><h1 class="post-title" itemprop="name headline"> Notes for Machine Learning - Week 5</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">Posted on</span> <time itemprop="dateCreated" datetime="2016-08-17T11:57:03+08:00" content="2016-08-17">2016-08-17</time></span> <span class="post-updated">&nbsp; | &nbsp; Updated on <time itemprop="dateUpdated" datetime="2016-08-28T10:13:08+08:00" content="2016-08-28">2016-08-28</time></span> <span class="post-category">&nbsp; | &nbsp;<span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a href="/categories/Notes/" itemprop="url" rel="index"><span itemprop="name">Notes</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="Neural-Networks-Learning"><a href="#Neural-Networks-Learning" class="headerlink" title="Neural Networks: Learning"></a>Neural Networks: Learning</h1><h2 id="Cost-Function-and-Backpropagation"><a href="#Cost-Function-and-Backpropagation" class="headerlink" title="Cost Function and Backpropagation"></a>Cost Function and Backpropagation</h2><h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><p>Let’s first define a few variables that we will need to use:</p><ul><li>$L$ = total number of layers in the network</li><li>$s_l$ = number of units (not counting bias unit) in layer $l$</li><li>$K$ = number of output units/classes</li></ul><p>Recall that the cost function for regularized logistic regression was:</p><p>$J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$</p><p>For neural networks, it is going to be slightly more complicated:</p><p>$J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2$</p><a id="more"></a><ul><li>$h_\Theta (x) \in R^K$, $(h_\Theta (x))_i$ = $i^{th}$ output</li><li><u>In the first part of the equation</u>, the double sum simply adds up the logistic regression costs calculated for each cell in the output layer</li><li><u>In the regularization part</u>, the triple sum simply adds up the squares of all the individual $\Theta$s in the entire network.<ul><li><u>The number of columns</u> in our current theta matrix is equal to the number of nodes in our current layer (<u>including</u> the bias unit).</li><li><u>The number of rows</u> in our current theta matrix is equal to the number of nodes in the next layer (<u>excluding</u> the bias unit).<ul><li>This is like a bias unit and by analogy to what we were doing for logistic progression, we won’t sum over those terms in our regularization term because <u>we don’t want to regularize them</u> and string their values as zero.</li></ul></li></ul></li></ul><h3 id="Backpropagation-Algorithm"><a href="#Backpropagation-Algorithm" class="headerlink" title="Backpropagation Algorithm"></a>Backpropagation Algorithm</h3><p><strong>“Backpropagation” (后向搜索)</strong> is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression.</p><p>Our goal is try to find parameters $\Theta$ to try to minimize $J(\Theta)$.</p><p>In order to use either gradient descent or one of the advance optimization algorithms. What we need to do therefore is to write code that takes this input the parameters theta and computes $J(\Theta)$ and $\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)$.</p><h4 id="Gradient-compuation"><a href="#Gradient-compuation" class="headerlink" title="Gradient compuation"></a>Gradient compuation</h4><p>Given one training example $(x,y)$</p><p><img src="/images/gradient_computation.png" alt="gradient_computation"></p><p>Forward propagation:</p><ul><li>$a^{(1)} = x$</li><li>$z^{(2)} = \Theta ^{(1)} a ^{(1)}$</li><li>$a^{(2)} = g(z^{(2)})\ (add\ a^{(2)}_0)$</li><li>$z^{(3)} = \Theta ^{(1)} a ^{(2)}$</li><li>$a^{(3)} = g(z^{(3)})\ (add\ a^{(3)}_0)$</li><li>$z^{(4)} = \Theta ^{(3)} a ^{(3)}$</li><li>$a^{(2)} = h_\Theta (x) = g(z^{(3)})$</li></ul><p>In backpropagation we’re going to compute for every node:</p><p>$\delta_j^{(l)}$ = “error” of node j in layer $l$ ($s_{l+1}$ elements vector)</p><p>For each output unit (layer $L = 4$):</p><p>$\delta ^{(4)} = a^{(4)} - y$</p><p>To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left:</p><p>$\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ g’(z^{(l)})$</p><p>The g-prime derivative terms can also be written out as:</p><p>$g’(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})$</p><p>There is no $\delta ^{(1)}$ term, because the first layer corresponds to the input layer and that’s just the feature we observed in our training sets, so that doesn’t have any error associated with that.</p><p>It’s possible to prove that if you ignore regularation, then the partial derivative terms you want are exactly given by the activations and these delta terms.</p><p>$\dfrac{\partial J(\Theta)}{\partial \Theta_{i,j}^{(l)}} = a^{(i)}_j \delta^{(l+1)}_i\ (\text{ignoring }\lambda)$</p><h4 id="Backpropagation-Algorithm-1"><a href="#Backpropagation-Algorithm-1" class="headerlink" title="Backpropagation Algorithm"></a>Backpropagation Algorithm</h4><ul><li>Training set $\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace$</li><li>Set $\Delta^{(l)}_{i,j} := 0$ (for all $l, i, j$)</li><li>For $i=1$ to $m$<ul><li>Set $a^{(1)} := x^{(t)}$</li><li>Perform forward propagation to compute $a^{(l)}$ for $l = 2,3,\dots ,L$</li><li>Using $y^{(i)}$, compute $\delta^{(L)} = a^{(L)} - y^{(t)}$</li><li>Compute $\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}$</li><li>$\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}$ or with vectorization, $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$</li></ul></li><li>$D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)$ <strong>If</strong> $j\ne 0$</li><li>$D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}$ <strong>If</strong> $j = 0$</li></ul><p>The capital-delta matrix is used as an “accumulator” to add up our values as we go along and eventually compute our partial derivative.</p><p> the $D_{i,j}^{(l)}$ terms are the partial derivatives and the results we are looking for:</p><p>$\dfrac{\partial J(\Theta)}{\partial \Theta_{i,j}^{(l)}} = D_{i,j}^{(l)}$</p><h3 id="Backpropagation-Intuition"><a href="#Backpropagation-Intuition" class="headerlink" title="Backpropagation Intuition"></a>Backpropagation Intuition</h3><h4 id="Forward-propagation"><a href="#Forward-propagation" class="headerlink" title="Forward propagation"></a>Forward propagation</h4><p><img src="/images/forward_propagation_intuition.png" alt="forward_propagation_intuition.png"></p><h4 id="What’s-backpropagation-doing"><a href="#What’s-backpropagation-doing" class="headerlink" title="What’s backpropagation doing?"></a>What’s backpropagation doing?</h4><p>The cost function is:</p><p>$J(\theta) = - \frac{1}{m} \sum_{t=1}^m\sum_{k=1}^K \left[ y^{(t)}_k \ \log (h_\theta (x^{(t)}))_k + (1 - y^{(t)}_k)\ \log (1 - h_\theta(x^{(t)})_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} ( \theta_{j,i}^{(l)})^2$</p><p>Focusing on a single example $x^{(i)}, y^{(i)}$, the case of 1 output unit, and ignoring regularization ($\lambda = 0$),</p><p>$cost(t) =y^{(t)} \ \log (h_\theta (x^{(t)})) + (1 - y^{(t)})\ \log (1 - h_\theta(x^{(t)}))$</p><p>Intuitively, $\theta ^{(l)}_j$ is the “error” for $a ^{(l)}_j$ (unit $j$ in layer $l$). More formally, the delta values are actually the derivative of the cost function:</p><p>$\delta_j^{(l)} = \dfrac{\partial}{\partial z_j^{(l)}} cost(t)$</p><p><img src="/images/backward_propagation_intuition.png" alt="backward_propagation_intuition.png"></p><p>In above, we can compute</p><p>$\delta ^{(4)}_1 = y^{(i)} - a^{(4)}_1$</p><p>$\delta ^{(3)}_2 = \Theta ^{(3)}_{12} \delta^{(4)}_1$</p><p>$\delta ^{(2)}_2 = \Theta ^{(2)}_{12} \delta^{(3)}_1 + \Theta ^{(2)}_{22} \delta^{(3)}_2$</p><h2 id="Backpropagation-in-Practice"><a href="#Backpropagation-in-Practice" class="headerlink" title="Backpropagation in Practice"></a>Backpropagation in Practice</h2><h3 id="Implementation-Note-Unrolling-Parameters"><a href="#Implementation-Note-Unrolling-Parameters" class="headerlink" title="Implementation Note: Unrolling Parameters"></a>Implementation Note: Unrolling Parameters</h3><p>We use following code to get the optimisation theta.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">function [jVal, gradient] = costFunction(theta)</div><div class="line">  ...</div><div class="line">optTheta = fminunc(@costFunction, initialTheta, options)</div></pre></td></tr></table></figure><p>Where <code>gradient</code>, <code>theta</code>, <code>initialTheta</code> are vectors of $n+1$ dimension.</p><p>In order to use optimizing functions such as <code>fminunc()</code>, we will want to “unroll” all the elements and put them into one long vector:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">thetaVec = [Theta1(:); Theta2(:); Theta3(:)];</div><div class="line">DVec = [D1(:); D2(:); D3(:)];</div></pre></td></tr></table></figure><p>If the dimensions of <code>Theta1</code> is $10\times11$, <code>Theta2</code> is $101\times 1$ and <code>Theta3</code> is $1\times 11$, then we can get back our original matrices from the “unrolled” versions as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Theta1 = reshape(thetaVector(1:110),10,11)</div><div class="line">Theta2 = reshape(thetaVector(111:220),10,11)</div><div class="line">Theta3 = reshape(thetaVector(221:231),1,11)</div></pre></td></tr></table></figure></div><div></div><div></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/Coursera/" rel="tag">#Coursera</a> <a href="/tags/Machine-Learning/" rel="tag">#Machine Learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2016/08/15/Coursera-Machine-Learning-Week-4/" rel="next" title="Notes for Machine Learning - Week 4"><i class="fa fa-chevron-left"></i> Notes for Machine Learning - Week 4</a></div><div class="post-nav-prev post-nav-item"></div></div></footer></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview"> Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="/uploads/avatar.png" alt="Yusu Pan"><p class="site-author-name" itemprop="name">Yusu Pan</p><p class="site-description motion-element" itemprop="description">We've been gaining one good thing through losing another.</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives"><span class="site-state-item-count">13</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories"><span class="site-state-item-count">1</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags"><span class="site-state-item-count">8</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/corenel" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github</a></span><span class="links-of-author-item"><a href="https://twitter.com/corenel" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i> Twitter</a></span><span class="links-of-author-item"><a href="http://www.zhihu.com/people/pan-yu-su" target="_blank" title="Zhihu"><i class="fa fa-fw fa-zhihu"></i> Zhihu</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Neural-Networks-Learning"><span class="nav-number">1.</span> <span class="nav-text">Neural Networks: Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cost-Function-and-Backpropagation"><span class="nav-number">1.1.</span> <span class="nav-text">Cost Function and Backpropagation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cost-Function"><span class="nav-number">1.1.1.</span> <span class="nav-text">Cost Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Backpropagation-Algorithm"><span class="nav-number">1.1.2.</span> <span class="nav-text">Backpropagation Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient-compuation"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">Gradient compuation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Backpropagation-Algorithm-1"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">Backpropagation Algorithm</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Backpropagation-Intuition"><span class="nav-number">1.1.3.</span> <span class="nav-text">Backpropagation Intuition</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Forward-propagation"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">Forward propagation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#What’s-backpropagation-doing"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">What’s backpropagation doing?</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Backpropagation-in-Practice"><span class="nav-number">1.2.</span> <span class="nav-text">Backpropagation in Practice</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Implementation-Note-Unrolling-Parameters"><span class="nav-number">1.2.1.</span> <span class="nav-text">Implementation Note: Unrolling Parameters</span></a></li></ol></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright"> &copy; 2015 - <span itemprop="copyrightYear">2016</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Yusu Pan</span></div><div class="powered-by"> Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info"> Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script><script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>