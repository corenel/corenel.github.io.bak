<!doctype html><html class="theme-next muse use-motion"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="BGEL63KNRW25AkpCy3shpRdMWDHE9LZTAFS3XSHtFK8"><link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.0.2" rel="stylesheet" type="text/css"><meta name="keywords" content="Coursera,Machine Learning,"><link rel="alternate" href="/atom.xml" title="Yuthon's blog" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.2"><meta name="description" content="Linear Regression with One VariableModel and Cost FunctionModel Representation
Supervised Learning (监督学习): Given the “right answer” for each example in the data.
Regression Problem (回归问题): Predict rea"><meta property="og:type" content="article"><meta property="og:title" content="Notes for Machine Learning - Week 1"><meta property="og:url" content="http://www.yuthon.com/2016/07/26/Coursera-Machine-Learning-Week-1/index.html"><meta property="og:site_name" content="Yuthon's blog"><meta property="og:description" content="Linear Regression with One VariableModel and Cost FunctionModel Representation
Supervised Learning (监督学习): Given the “right answer” for each example in the data.
Regression Problem (回归问题): Predict rea"><meta property="og:updated_time" content="2016-07-29T08:21:27.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Notes for Machine Learning - Week 1"><meta name="twitter:description" content="Linear Regression with One VariableModel and Cost FunctionModel Representation
Supervised Learning (监督学习): Given the “right answer” for each example in the data.
Regression Problem (回归问题): Predict rea"><script type="text/javascript" id="hexo.configuration">var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    }
  };</script><link rel="canonical" href="http://www.yuthon.com/2016/07/26/Coursera-Machine-Learning-Week-1/"><title>Notes for Machine Learning - Week 1 | Yuthon's blog</title></head><body itemscope itemtype="//schema.org/WebPage" lang="en"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-76233259-1', 'auto');
  ga('send', 'pageview');</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="//schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Yuthon's blog</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="//schema.org/Article"><header class="post-header"><h1 class="post-title" itemprop="name headline">Notes for Machine Learning - Week 1</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time itemprop="dateCreated" datetime="2016-07-26T11:55:36+08:00" content="2016-07-26">2016-07-26 </time></span><span class="post-updated">&nbsp; | &nbsp; Updated on <time itemprop="dateUpdated" datetime="2016-07-29T16:21:27+08:00" content="2016-07-29">2016-07-29 </time></span><span class="post-category">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a href="/categories/Notes/" itemprop="url" rel="index"><span itemprop="name">Notes</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="Linear-Regression-with-One-Variable"><a href="#Linear-Regression-with-One-Variable" class="headerlink" title="Linear Regression with One Variable"></a>Linear Regression with One Variable</h1><h2 id="Model-and-Cost-Function"><a href="#Model-and-Cost-Function" class="headerlink" title="Model and Cost Function"></a>Model and Cost Function</h2><h3 id="Model-Representation"><a href="#Model-Representation" class="headerlink" title="Model Representation"></a>Model Representation</h3><ul><li><strong>Supervised Learning (监督学习)</strong>: Given the “right answer” for each example in the data.<ul><li><strong>Regression Problem (回归问题)</strong>: Predict real-valued output.</li><li><strong>Classification Problem (分类问题)</strong>: Predict discrete-valued output.</li></ul></li><li><strong>Training set (训练集)</strong><ul><li><strong>m</strong>: number of training examples</li><li><strong>x</strong>‘s: “input” variable / features</li><li><strong>y</strong>‘s: “output” variable / “target” variable</li><li><strong>$(x, y)$</strong>: one training example</li><li><strong>$(x^i, y^i)$</strong>: $i^{th}$ training example</li></ul></li></ul><ul><li>Training Set -&gt; Learning Algorithm -&gt; <strong>h(hypothesis, 假设)</strong><ul><li>h is a function maps from x’s to y’s</li><li>e.g. Size of house -&gt; h -&gt; Estimated price</li></ul></li></ul><ul><li><strong>Linear regression with one variable</strong><ul><li>$h_\theta (x) = \theta_0 + \theta_1 x$<ul><li>Shorthand: $h(x)$</li></ul></li><li>Or named Univariate linear regression (单变量线性回归)</li></ul></li></ul><a id="more"></a><h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><ul><li><p><strong>Hypothesis</strong>: $h_\theta (x) = \theta_0 + \theta_1 x$</p><ul><li>$\theta_i$’s: Parameters (模型参数)</li><li>How to choose $\theta_i$’s ?<ul><li>Idea: Choose $\theta_0, \theta_1$ so that $h_\theta (x)$ is close to $y$ for our training example $(x,y)$</li></ul></li></ul></li></ul><ul><li><strong>Cost function (代价函数)</strong><ul><li>$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m \left(h_\theta(x^{(i)})-y^{(i)}\right)^2$</li><li>Sometimes called Square error function (平方误差代价函数)</li></ul></li></ul><ul><li>Goal: minimise $J(\theta_0, \theta_1)$</li></ul><h2 id="Parameter-Learning"><a href="#Parameter-Learning" class="headerlink" title="Parameter Learning"></a>Parameter Learning</h2><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><ul><li><p><strong>Gradient Descent (梯度下降)</strong></p><ul><li>Goal<ul><li>Have some function $J(\theta_0, \theta_1)$</li><li>Want $\theta_0, \theta_1$ of $min J(\theta_0, \theta_1)$</li></ul></li><li>Outline<ul><li>Start with some $\theta_0, \theta_1$, usually all set to $0$.</li><li>Keep changing $\theta_0, \theta_1$ to reduce $J(\theta_0, \theta_1)$ until we hopefully end up at minimum</li></ul></li></ul></li><li><p>Gradient descent algorithm</p><blockquote><p>repeat until convergence (收敛) {<br>​ $\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_j)$ (for $j=0$ and $j=1$)<br>}</p></blockquote><ul><li><p><code>:=</code> denotes assignment</p></li><li><p>$\alpha$ denotes learning rate</p><ul><li>if too small, gradient descent can be slow</li><li>If too large, gradient descent can overshoot the minimum. It may fail to converge or even diverge.</li></ul></li><li><p>You should <u>simultaneously</u> update $\theta_0$ and $\theta_1$</p><ul><li><p>That is, you should compute the right-hand sides of $\theta_0$ and $\theta_1$, then save them to temporary variables, and finally update $\theta_0$ and $\theta_1$.</p><blockquote><p>$temp0 := \theta_0 - \alpha \frac{\partial}{\partial \theta_0} J(\theta_0, \theta_j)$</p><p>$temp1 := \theta_1 - \alpha \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_j)$</p><p>$\theta_0 := temp0$</p><p>$\theta_1 :=temp1$</p></blockquote></li></ul></li></ul></li></ul><h3 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h3><ul><li><p>If $\theta_1$ at local optima, it leaves $\theta_1$ unchanged.</p></li><li><p><u>gradient descent can converge to a local minimum, even with the learning rate $\alpha$ fixed.</u></p><ul><li>As we approach a local minimum, gradient descent will automatically take smaller steps. So, no need to decrease $\alpha$ over time.</li></ul></li></ul><h3 id="Gradient-Descent-For-Linear-Regression"><a href="#Gradient-Descent-For-Linear-Regression" class="headerlink" title="Gradient Descent For Linear Regression"></a>Gradient Descent For Linear Regression</h3><p>We can compute that</p><p>$\frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1) = \frac{1}{m} \sum^m_{i=1}\left(h_\theta(x^{(i)})-y^{(i)}\right)$</p><p>$\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) = \frac{1}{m} \sum^m_{i=1}\left(h_\theta(x^{(i)})-y^{(i)}\right) \cdot x^{(i)}$</p><p>Thus the Gradient descent algorithm can be expressed as</p><blockquote><p>repeat until convergence {<br>$\theta_0 := \theta_0 - \alpha \frac{1}{m} \sum^m_{i=1}\left(h_\theta(x^{(i)})-y^{(i)}\right)​$</p><p>$\theta_1 := \theta_1 - \alpha \frac{1}{m} \sum^m_{i=1}\left(h_\theta(x^{(i)})-y^{(i)}\right) \cdot x^{(i)}$<br>}</p></blockquote><p>And the cost funciton of linear refression is always a convex function (凸函数), or called Bowl-shaped function (弓形函数). <u>It doesn’t have any local optima except for the one global optimum.</u></p><h4 id="“Batch”-Gradient-Descent"><a href="#“Batch”-Gradient-Descent" class="headerlink" title="“Batch” Gradient Descent"></a>“Batch” Gradient Descent</h4><ul><li>The algorithm that we just went over is sometimes called <strong>Batch Gradient Descent (批量梯度下降)</strong>.</li><li>“Batch”: Each step of gradient descent uses all th etraining examples.</li></ul></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Coursera/" rel="tag">#Coursera</a> <a href="/tags/Machine-Learning/" rel="tag">#Machine Learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2016/05/16/OS-X-10-11-4-on-XPS-15-9550/" rel="next" title="OS X 10.11.4 on XPS 15 9550"><i class="fa fa-chevron-left"></i> OS X 10.11.4 on XPS 15 9550</a></div><div class="post-nav-prev post-nav-item"><a href="/2016/07/27/Coursera-Machine-Learning-Week-2/" rel="prev" title="Notes for Machine Learning - Week 2">Notes for Machine Learning - Week 2 <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person"><img class="site-author-image" itemprop="image" src="/uploads/avatar.png" alt="Yusu Pan"><p class="site-author-name" itemprop="name">Yusu Pan</p><p class="site-description motion-element" itemprop="description">We've been gaining one good thing through losing another.</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">24</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories"><span class="site-state-item-count">4</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">27</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/corenel" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="https://twitter.com/corenel" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i> Twitter </a></span><span class="links-of-author-item"><a href="http://www.zhihu.com/people/pan-yu-su" target="_blank" title="Zhihu"><i class="fa fa-fw fa-zhihu"></i> Zhihu</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Linear-Regression-with-One-Variable"><span class="nav-number">1.</span> <span class="nav-text">Linear Regression with One Variable</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-and-Cost-Function"><span class="nav-number">1.1.</span> <span class="nav-text">Model and Cost Function</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-Representation"><span class="nav-number">1.1.1.</span> <span class="nav-text">Model Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cost-Function"><span class="nav-number">1.1.2.</span> <span class="nav-text">Cost Function</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Parameter-Learning"><span class="nav-number">1.2.</span> <span class="nav-text">Parameter Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Descent"><span class="nav-number">1.2.1.</span> <span class="nav-text">Gradient Descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Intuition"><span class="nav-number">1.2.2.</span> <span class="nav-text">Intuition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Descent-For-Linear-Regression"><span class="nav-number">1.2.3.</span> <span class="nav-text">Gradient Descent For Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#“Batch”-Gradient-Descent"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">“Batch” Gradient Descent</span></a></li></ol></li></ol></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 - <span itemprop="copyrightYear">2016</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Yusu Pan</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }</script><script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.0.2"></script><script type="text/javascript" src="/js/src/motion.js?v=5.0.2"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.2"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.0.2"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.2"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>