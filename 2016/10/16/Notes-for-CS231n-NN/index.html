<!doctype html><html class="theme-next muse use-motion"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="BGEL63KNRW25AkpCy3shpRdMWDHE9LZTAFS3XSHtFK8"><link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css"><meta name="keywords" content="CS231n,Neural Network,Deep Learning,"><link rel="alternate" href="/atom.xml" title="Yuthon's blog" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1"><meta name="description" content="本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:

Neural Networks Part 1: Setting up the Architecture
Neural Networks Part 2: Setting up the Data and the Loss
Neural Networks Part 3"><meta property="og:type" content="article"><meta property="og:title" content="Notes for CS231n Neural Network"><meta property="og:url" content="http://www.yuthon.com/2016/10/16/Notes-for-CS231n-NN/index.html"><meta property="og:site_name" content="Yuthon's blog"><meta property="og:description" content="本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:

Neural Networks Part 1: Setting up the Architecture
Neural Networks Part 2: Setting up the Data and the Loss
Neural Networks Part 3"><meta property="og:image" content="http://www.yuthon.com/images/activation_functions.png"><meta property="og:image" content="http://www.yuthon.com/images/data_preprocessing.png"><meta property="og:image" content="http://www.yuthon.com/images/batch_normalizaition.png"><meta property="og:image" content="http://www.yuthon.com/images/loss_double_check.png"><meta property="og:image" content="http://www.yuthon.com/images/overfit_on_a_small_portion_of_training_data.png"><meta property="og:image" content="http://www.yuthon.com/images/loss_barely_changing.png"><meta property="og:image" content="http://www.yuthon.com/images/loss_exploding.png"><meta property="og:updated_time" content="2016-10-17T05:35:39.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Notes for CS231n Neural Network"><meta name="twitter:description" content="本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:

Neural Networks Part 1: Setting up the Architecture
Neural Networks Part 2: Setting up the Data and the Loss
Neural Networks Part 3"><meta name="twitter:image" content="http://www.yuthon.com/images/activation_functions.png"><script type="text/javascript" id="hexo.configuration">var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };</script><link rel="canonical" href="http://www.yuthon.com/2016/10/16/Notes-for-CS231n-NN/"><title>Notes for CS231n Neural Network | Yuthon's blog</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-76233259-1', 'auto');
  ga('send', 'pageview');</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Yuthon's blog</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><header class="post-header"><h1 class="post-title" itemprop="name headline">Notes for CS231n Neural Network</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time itemprop="dateCreated" datetime="2016-10-16T21:04:26+08:00" content="2016-10-16">2016-10-16 </time></span><span class="post-updated">&nbsp; | &nbsp; Updated on <time itemprop="dateUpdated" datetime="2016-10-17T13:35:39+08:00" content="2016-10-17">2016-10-17 </time></span><span class="post-category">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a href="/categories/Notes/" itemprop="url" rel="index"><span itemprop="name">Notes</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:</p><ul><li><a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="external">Neural Networks Part 1: Setting up the Architecture</a></li><li><a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="external">Neural Networks Part 2: Setting up the Data and the Loss</a></li><li><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">Neural Networks Part 3: Learning and Evaluation</a></li></ul><p>或者可以看知乎专栏中的中文翻译:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记1（上）</a></li><li><a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记1（下）</a></li><li><a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记2 </a></li><li><a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记3（上）</a></li><li><a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记3（下）</a></li></ul><p>另外, 本文主要根据讲课的 Slides 上的顺序来, 与 Lecture Notes 的顺序略有不同.</p></blockquote><a id="more"></a><h1 id="Lecture-5"><a href="#Lecture-5" class="headerlink" title="Lecture 5"></a>Lecture 5</h1><h2 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h2><p>课程中主要讲了Sigmoid, tanh, ReLU, Leaky ReLU, Maxout 以及 ELU 这几种激活函数.</p><p><img src="/images/activation_functions.png" alt="activation_functions"></p><ul><li>Sigmoid 由于以下原因, 基本不使用<ul><li>函数饱和使得梯度消失(Saturated neurons “kill” the gradients)</li><li>函数并非以零为中心(zero-centered)</li><li>指数运算消耗大量计算资源</li></ul></li><li>tanh 相对于 Sigmoid 来说, 多了零中心这一个特性, 但还是不常用</li><li>重头戏 ReLU (Rectified Linear Unit):<ul><li>在正半轴上没有饱和现象</li><li>线性结构省下了很多计算资源, 可以直接对矩阵进行阈值计算来实现, 速度是 sigmoid/tanh 的6倍</li><li>然而由于负半轴直接是0, 训练的时候会”死掉”(die), 因此就有了 Leaky ReLU 和 ELU (Exponential Linear Units), 以及更加通用的 Maxout (代价是消耗两倍的计算资源)</li></ul></li></ul><p><strong>实践中一般就直接选 ReLU, 同时注意 Learning Rate 的调整. 实在不行用 Leaky ReLU 或者 Mahout 碰碰运气. 还可以试试 tanh. 坚决别用 Sigmoid.</strong></p><h2 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h2><p>有很多数据预处理的方法, 比如零中心化(zero-centering), 归一化(normalization), PCA(Principal Component Analysis, 主成分分析)和白化(Whitening).</p><ul><li><p>零中心化(zero-centering): 主要方法就是均值减法, 将数据的中心移到原点上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Assume X [NxD] is data matrix, each example in a row</span></div><div class="line">X -= np.mean(X, axis=<span class="number">0</span>)</div></pre></td></tr></table></figure><p>零中心化主要有两种做法(e.g. consider CIFAR-10 example with [32,32,3] images):</p><ul><li>Subtract the mean image (e.g.AlexNet) (mean image = [32,32,3] array)</li></ul></li></ul><ul><li><p>Subtract per-channel mean (e.g.VGGNet) (mean along each channel = 3 numbers)</p></li><li><p>归一化(normalization): 使得数据所有维度的范围基本相等, 当然由于图像像素的数值范围本身基本是一致的(一般为0-255), 所以不一定要用.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">X /= np.std(X, axis=<span class="number">0</span>)</div></pre></td></tr></table></figure></li><li><p>PCA 和白化在 CNN 中并没有什么用, 就不介绍了.</p><p><img src="/images/data_preprocessing.png" alt="data_preprocessing"></p></li></ul><p><strong>实践中一般就只做零中心化, 其他几样基本都不用做.</strong></p><blockquote><p>以下引自知乎专栏[智能单元]所翻译的课程讲义:</p><p><strong>常见错误。</strong>进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。<strong>应该怎么做呢？应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。</strong></p><p><strong>译者注：此处确为初学者常见错误，请务必注意！</strong></p></blockquote><h2 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h2><p>由于各种原因, 将 Weight 全部初始化为0, 或者是小随机数的方法都不大好(一个是由于对称性, 另一个是由于梯度信号太小). 建议使用的是下面这个(配合 ReLU):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">w = np.random.randn(n) * sqrt(<span class="number">2.0</span>/n)</div></pre></td></tr></table></figure><p>另外就是还推荐 <strong>Batch Normalization</strong> (批量归一化), 通常应用在全连接层之后, 激活函数之前. 具体参见论文([Ioffe and Szegedy, 2015]).</p><p><img src="/images/batch_normalizaition.png" alt="batch_normalizaition"></p><ul><li>Improves gradient flow through thenetwork</li><li>Allows higher learning rates</li><li>Reduces the strong dependence oninitialization</li><li>Acts as a form of regularization in afunny way, and slightly reduces the need for dropout, maybe</li></ul><h2 id="Babysitting-the-Learning-Process"><a href="#Babysitting-the-Learning-Process" class="headerlink" title="Babysitting the Learning Process"></a>Babysitting the Learning Process</h2><h3 id="Double-check-that-the-loss-is-reasonable"><a href="#Double-check-that-the-loss-is-reasonable" class="headerlink" title="Double check that the loss is reasonable"></a>Double check that the loss is reasonable</h3><ul><li>首先不使用 regularization, 观察 loss 是否合理(下例中对于 CIFAR-10 的初始 loss 应近似等于$log(0.1)=2.31$)</li><li><p>然后再开启 regularization, 观察 loss 是否上升</p><p><img src="/images/loss_double_check.png" alt="loss_double_check"></p></li></ul><h3 id="Other-sanity-check-tips"><a href="#Other-sanity-check-tips" class="headerlink" title="Other sanity check tips"></a>Other sanity check tips</h3><ul><li><p>首先在一个小数据集上进行训练(可先设 regualrization 为0), 看看是否过拟合, 确保算法的正确性.</p><p><img src="/images/overfit_on_a_small_portion_of_training_data.png" alt="overfit_on_a_small_portion_of_training_data"></p></li><li><p>之后再从一个小的 regularization 开始, 寻找合适的能够使 loss 下降的 learning rate.</p><ul><li><p>如果几次 epoch 后, loss 没没有下降, 说明 learning rate 太小了</p><p><img src="/images/loss_barely_changing.png" alt="loss_barely_changing"></p></li><li><p>如果 loss 爆炸了, 那么说明 learning rate 太大了</p><p><img src="/images/loss_exploding.png" alt="loss_exploding"></p></li><li><p>通常 learning rate 的范围是$[1e-3, 1e-5]$</p></li></ul></li></ul><h2 id="Hyperparameter-Optimization"><a href="#Hyperparameter-Optimization" class="headerlink" title="Hyperparameter Optimization"></a>Hyperparameter Optimization</h2><p>(To be continued…)</p></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/CS231n/" rel="tag">#CS231n</a> <a href="/tags/Neural-Network/" rel="tag">#Neural Network</a> <a href="/tags/Deep-Learning/" rel="tag">#Deep Learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2016/10/08/B-Human-Code-Brief-Analysis-2/" rel="next" title="B-Human Code 浅析 - ScanGridProvider"><i class="fa fa-chevron-left"></i> B-Human Code 浅析 - ScanGridProvider</a></div><div class="post-nav-prev post-nav-item"></div></div></footer></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/uploads/avatar.png" alt="Yusu Pan"><p class="site-author-name" itemprop="name">Yusu Pan</p><p class="site-description motion-element" itemprop="description">We've been gaining one good thing through losing another.</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">19</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories"><span class="site-state-item-count">3</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">18</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/corenel" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="https://twitter.com/corenel" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i> Twitter </a></span><span class="links-of-author-item"><a href="http://www.zhihu.com/people/pan-yu-su" target="_blank" title="Zhihu"><i class="fa fa-fw fa-zhihu"></i> Zhihu</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-5"><span class="nav-number">1.</span> <span class="nav-text">Lecture 5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Activation-Functions"><span class="nav-number">1.1.</span> <span class="nav-text">Activation Functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Preprocessing"><span class="nav-number">1.2.</span> <span class="nav-text">Data Preprocessing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Weight-Initialization"><span class="nav-number">1.3.</span> <span class="nav-text">Weight Initialization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Babysitting-the-Learning-Process"><span class="nav-number">1.4.</span> <span class="nav-text">Babysitting the Learning Process</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Double-check-that-the-loss-is-reasonable"><span class="nav-number">1.4.1.</span> <span class="nav-text">Double check that the loss is reasonable</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-sanity-check-tips"><span class="nav-number">1.4.2.</span> <span class="nav-text">Other sanity check tips</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hyperparameter-Optimization"><span class="nav-number">1.5.</span> <span class="nav-text">Hyperparameter Optimization</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 - <span itemprop="copyrightYear">2016</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Yusu Pan</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }</script><script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>