<!doctype html><html class="theme-next muse use-motion"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="BGEL63KNRW25AkpCy3shpRdMWDHE9LZTAFS3XSHtFK8"><link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.0.2" rel="stylesheet" type="text/css"><meta name="keywords" content="Deep Learning,CS231n,Neural Network,"><link rel="alternate" href="/atom.xml" title="Yuthon's blog" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.2"><meta name="description" content="本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:

Neural Networks Part 1: Setting up the Architecture
Neural Networks Part 2: Setting up the Data and the Loss
Neural Networks Part 3"><meta property="og:type" content="article"><meta property="og:title" content="Notes for CS231n Neural Network"><meta property="og:url" content="http://www.yuthon.com/2016/10/16/Notes-for-CS231n-NN/index.html"><meta property="og:site_name" content="Yuthon's blog"><meta property="og:description" content="本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:

Neural Networks Part 1: Setting up the Architecture
Neural Networks Part 2: Setting up the Data and the Loss
Neural Networks Part 3"><meta property="og:image" content="http://www.yuthon.com/images/activation_functions.png"><meta property="og:image" content="http://www.yuthon.com/images/data_preprocessing.png"><meta property="og:image" content="http://www.yuthon.com/images/xavier_init.png"><meta property="og:image" content="http://www.yuthon.com/images/batch_normalizaition.png"><meta property="og:image" content="http://www.yuthon.com/images/loss_double_check.png"><meta property="og:image" content="http://www.yuthon.com/images/overfit_on_a_small_portion_of_training_data.png"><meta property="og:image" content="http://www.yuthon.com/images/loss_barely_changing.png"><meta property="og:image" content="http://www.yuthon.com/images/loss_exploding.png"><meta property="og:image" content="http://www.yuthon.com/images/coarse_search.png"><meta property="og:image" content="http://www.yuthon.com/images/finer_search.png"><meta property="og:image" content="http://www.yuthon.com/images/random_search_vs_grid_search .png"><meta property="og:image" content="http://www.yuthon.com/images/parameter_update.png"><meta property="og:image" content="http://www.yuthon.com/images/sgd.png"><meta property="og:image" content="http://www.yuthon.com/images/momentum_and_Nesterov.png"><meta property="og:image" content="http://www.yuthon.com/images/dropout.png"><meta property="og:image" content="http://www.yuthon.com/images/dropout_a_good_idea.png"><meta property="og:updated_time" content="2016-10-18T11:45:51.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Notes for CS231n Neural Network"><meta name="twitter:description" content="本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:

Neural Networks Part 1: Setting up the Architecture
Neural Networks Part 2: Setting up the Data and the Loss
Neural Networks Part 3"><meta name="twitter:image" content="http://www.yuthon.com/images/activation_functions.png"><script type="text/javascript" id="hexo.configuration">var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"hide"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    }
  };</script><link rel="canonical" href="http://www.yuthon.com/2016/10/16/Notes-for-CS231n-NN/"><title>Notes for CS231n Neural Network | Yuthon's blog</title></head><body itemscope itemtype="//schema.org/WebPage" lang="en"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-76233259-1', 'auto');
  ga('send', 'pageview');</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="//schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Yuthon's blog</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="//schema.org/Article"><header class="post-header"><h1 class="post-title" itemprop="name headline">Notes for CS231n Neural Network</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time itemprop="dateCreated" datetime="2016-10-16T21:04:26+08:00" content="2016-10-16">2016-10-16 </time></span><span class="post-updated">&nbsp; | &nbsp; Updated on <time itemprop="dateUpdated" datetime="2016-10-18T19:45:51+08:00" content="2016-10-18">2016-10-18 </time></span><span class="post-category">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a href="/categories/Notes/" itemprop="url" rel="index"><span itemprop="name">Notes</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>本文主要对于 CS231n 课程自带的 Lecture Notes 的一些补充与总结. 建议先看原版的 Lecture Notes:</p><ul><li><a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="external">Neural Networks Part 1: Setting up the Architecture</a></li><li><a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="external">Neural Networks Part 2: Setting up the Data and the Loss</a></li><li><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">Neural Networks Part 3: Learning and Evaluation</a></li></ul><p>或者可以看知乎专栏中的中文翻译:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记1（上）</a></li><li><a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记1（下）</a></li><li><a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记2 </a></li><li><a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记3（上）</a></li><li><a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记3（下）</a></li></ul><p>另外, 本文主要根据讲课的 Slides 上的顺序来, 与 Lecture Notes 的顺序略有不同.</p></blockquote><a id="more"></a><h1 id="Lecture-5"><a href="#Lecture-5" class="headerlink" title="Lecture 5"></a>Lecture 5</h1><h2 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h2><p>课程中主要讲了Sigmoid, tanh, ReLU, Leaky ReLU, Maxout 以及 ELU 这几种激活函数.</p><p><img src="/images/activation_functions.png" alt="activation_functions"></p><ul><li>Sigmoid 由于以下原因, 基本不使用<ul><li>函数饱和使得梯度消失(Saturated neurons “kill” the gradients)</li><li>函数并非以零为中心(zero-centered)</li><li>指数运算消耗大量计算资源</li></ul></li><li>tanh 相对于 Sigmoid 来说, 多了零中心这一个特性, 但还是不常用</li><li>重头戏 ReLU (Rectified Linear Unit):<ul><li>在正半轴上没有饱和现象</li><li>线性结构省下了很多计算资源, 可以直接对矩阵进行阈值计算来实现, 速度是 sigmoid/tanh 的6倍</li><li>然而由于负半轴直接是0, 训练的时候会”死掉”(die), 因此就有了 Leaky ReLU 和 ELU (Exponential Linear Units), 以及更加通用的 Maxout (代价是消耗两倍的计算资源)</li></ul></li></ul><p><strong>实践中一般就直接选 ReLU, 同时注意 Learning Rate 的调整. 实在不行用 Leaky ReLU 或者 Mahout 碰碰运气. 还可以试试 tanh. 坚决别用 Sigmoid.</strong></p><h2 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h2><p>有很多数据预处理的方法, 比如零中心化(zero-centering), 归一化(normalization), PCA(Principal Component Analysis, 主成分分析)和白化(Whitening).</p><ul><li><p>零中心化(zero-centering): 主要方法就是均值减法, 将数据的中心移到原点上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Assume X [NxD] is data matrix, each example in a row</span></div><div class="line">X -= np.mean(X, axis=<span class="number">0</span>)</div></pre></td></tr></table></figure><p>零中心化主要有两种做法(e.g. consider CIFAR-10 example with [32,32,3] images):</p><ul><li><p>Subtract the mean image (e.g.AlexNet) (mean image = [32,32,3] array)</p></li><li><p>Subtract per-channel mean (e.g.VGGNet) (mean along each channel = 3 numbers)</p></li></ul></li><li><p>归一化(normalization): 使得数据所有维度的范围基本相等, 当然由于图像像素的数值范围本身基本是一致的(一般为0-255), 所以不一定要用.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">X /= np.std(X, axis=<span class="number">0</span>)</div></pre></td></tr></table></figure></li><li><p>PCA 和白化在 CNN 中并没有什么用, 就不介绍了.</p><p><img src="/images/data_preprocessing.png" alt="data_preprocessing"></p></li></ul><p><strong>实践中一般就只做零中心化, 其他几样基本都不用做.</strong></p><blockquote><p>以下引自知乎专栏[智能单元]所翻译的课程讲义:</p><p><strong>常见错误。</strong>进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。<strong>应该怎么做呢？应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。</strong></p><p><strong>译者注：此处确为初学者常见错误，请务必注意！</strong></p></blockquote><h2 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h2><p>由于各种原因, 将 Weight 全部初始化为0, 或者是小随机数的方法都不大好(一个是由于对称性, 另一个是由于梯度信号太小). 建议使用的是下面这个(配合 ReLU):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">w = np.random.randn(n) * sqrt(<span class="number">2.0</span>/n)</div></pre></td></tr></table></figure><p>或者 Xavier initialization:</p><p><img src="/images/xavier_init.png" alt="xavier_init"></p><p>另外就是还推荐 <strong>Batch Normalization</strong> (批量归一化), 通常应用在全连接层之后, 激活函数之前. 具体参见论文[Ioffe and Szegedy, 2015].</p><p><img src="/images/batch_normalizaition.png" alt="batch_normalizaition"></p><ul><li>Improves gradient flow through thenetwork</li><li>Allows higher learning rates</li><li>Reduces the strong dependence on initialization</li><li>Acts as a form of regularization in afunny way, and slightly reduces the need for dropout, maybe</li></ul><h2 id="Babysitting-the-Learning-Process"><a href="#Babysitting-the-Learning-Process" class="headerlink" title="Babysitting the Learning Process"></a>Babysitting the Learning Process</h2><h3 id="Double-check-that-the-loss-is-reasonable"><a href="#Double-check-that-the-loss-is-reasonable" class="headerlink" title="Double check that the loss is reasonable"></a>Double check that the loss is reasonable</h3><ul><li>首先不使用 regularization, 观察 loss 是否合理(下例中对于 CIFAR-10 的初始 loss 应近似等于$log(0.1)=2.31$)</li><li><p>然后再开启 regularization, 观察 loss 是否上升</p><p><img src="/images/loss_double_check.png" alt="loss_double_check"></p></li></ul><h3 id="Other-sanity-check-tips"><a href="#Other-sanity-check-tips" class="headerlink" title="Other sanity check tips"></a>Other sanity check tips</h3><ul><li><p>首先在一个小数据集上进行训练(可先设 regualrization 为0), 看看是否过拟合, 确保算法的正确性.</p><p><img src="/images/overfit_on_a_small_portion_of_training_data.png" alt="overfit_on_a_small_portion_of_training_data"></p></li><li><p>之后再从一个小的 regularization 开始, 寻找合适的能够使 loss 下降的 learning rate.</p><ul><li><p>如果几次 epoch 后, loss 没没有下降, 说明 learning rate 太小了</p><p><img src="/images/loss_barely_changing.png" alt="loss_barely_changing"></p></li><li><p>如果 loss 爆炸了, 那么说明 learning rate 太大了</p><p><img src="/images/loss_exploding.png" alt="loss_exploding"></p></li><li><p>通常 learning rate 的范围是$[1e-3, 1e-5]$</p></li></ul></li></ul><h2 id="Hyperparameter-Optimization"><a href="#Hyperparameter-Optimization" class="headerlink" title="Hyperparameter Optimization"></a>Hyperparameter Optimization</h2><ul><li><p><strong>从粗放(coarse)到细致(fine)地分段搜索</strong>, 先大范围小周期(1-5 epoch足矣), 然后再根据结果小范围长周期</p><ul><li>First stage: only a few epochs to get rough idea of what params work</li><li>Second stage: longer running time, finer search</li><li>… (repeat as necessary)</li></ul><blockquote><p>If the cost is ever &gt; 3 * original cost, break out early</p></blockquote></li><li><p><strong>在对数尺度上进行搜索</strong>, 例如<code>learning_rate = 10 ** uniform(-6, 1)</code>. 当然有些超参数还是按原来的, 比如 <code>dropout = uniform(0,1)</code></p></li><li><p><strong>小心边界上的最优值</strong>, 否则可能会错过更好的参数搜索范围.</p><p><img src="/images/coarse_search.png" alt="coarse_search"></p><p><img src="/images/finer_search.png" alt="finer_search"></p></li><li><p><strong>随机搜索优于网格搜索</strong></p><p><img src="/images/random_search_vs_grid_search .png" alt="random_search_vs_grid_search "></p></li></ul><h1 id="Lecture-6"><a href="#Lecture-6" class="headerlink" title="Lecture 6"></a>Lecture 6</h1><h2 id="Parameter-Updates"><a href="#Parameter-Updates" class="headerlink" title="Parameter Updates"></a>Parameter Updates</h2><p>参数更新有很多种方法, 常见的如下图:<img src="/images/parameter_update.png" alt="parameter_update"></p><ul><li><p>最普通的就是SGD, 仅仅按照负梯度来更新</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x += - learning_rate * dx</div></pre></td></tr></table></figure><p><img src="/images/sgd.png" alt="sgd"></p></li><li><p>其次就是各种动量方法, 比如 <strong>Momentum</strong>, 以及其衍生的 <strong>Nesterov</strong> 方法. 其主要思想就是在任何具有持续梯度的方向上保持一个会慢慢消失的动量, 使得梯度下降更为圆滑.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Momentum update</span></div><div class="line">v = mu * v - learning_rate * dx <span class="comment"># integrate velocity</span></div><div class="line">x += v <span class="comment"># integrate position</span></div><div class="line"></div><div class="line"><span class="comment"># Mesterov momentum update rewrite</span></div><div class="line">v_prev = v</div><div class="line">v = mu * v - learning_rate * dx</div><div class="line">x += -mu * v_prev + (<span class="number">1</span> + mu) * v</div></pre></td></tr></table></figure><p><img src="/images/momentum_and_Nesterov.png" alt="momentum_and_Nesterov"></p><blockquote><ul><li>v 初始为 0</li><li>mu 一般取 0.5, 0.9 或 0.99. 有时候可以先 0.5, 然后慢慢变成 0.99</li></ul></blockquote></li><li><p>然后就是逐步改 learning rate 的方法, 比如 AdaGrad 或者 RMSProp (Hinton 大神在 Coursera 课上提出的改进方法)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># AdaGrad</span></div><div class="line">cache += dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div><div class="line"></div><div class="line"><span class="comment"># RMSProp</span></div><div class="line">cache = decay_rate * cache + (<span class="number">1</span> - decay_rate) * dx**<span class="number">2</span></div><div class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</div></pre></td></tr></table></figure><blockquote><ul><li>cache 尺寸与 dx 相同</li><li>eps 取值在 1e-4 到 1e-8 之间, 主要是为了防止分母为 0.</li><li>AdaGrad 通常过早停止学习, RMSProp 通过引入一个梯度平方的滑动平均改善了它.</li></ul></blockquote></li><li><p>最后就是集上述方法之大成的 <strong>Adam</strong>, 在大多数的实践中都是一个很好的选择.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Adam</span></div><div class="line">m ,v = <span class="comment"># ... initialize cacahe to zeros</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> xrange(<span class="number">1</span>, big_number):</div><div class="line">    dx = <span class="comment"># ... evaluate gradient</span></div><div class="line">    m = beta1 * m + (<span class="number">1</span> - beta1) * dx <span class="comment"># update first momentum</span></div><div class="line">    v = beta2 * v + (<span class="number">1</span> - beta2) * (dx ** <span class="number">2</span>) <span class="comment"># update second momentum</span></div><div class="line">    mb = m / (<span class="number">1</span> - beta1 ** t) <span class="comment"># bias correction</span></div><div class="line">    vb = v / (<span class="number">1</span> - beta2 ** t) <span class="comment"># bias correction</span></div><div class="line">    x += - learning_rate * mb / (np.sqrt(vb) + <span class="number">1e-7</span>) <span class="comment"># RMSProp-like</span></div></pre></td></tr></table></figure><blockquote><ul><li>The bias correction compensates for the fact that m,v are initialized at zero and need<br>some time to “warm up”. Only relevant in first few iterations when t is small.</li></ul></blockquote></li></ul><h3 id="Learning-rate-decay"><a href="#Learning-rate-decay" class="headerlink" title="Learning rate decay"></a>Learning rate decay</h3><p>主要是为了让 learning rate 随着训练时间的推移慢慢变小, 防止系统动能太大, 到最后在最优点旁边跳来跳去.</p><ul><li><strong>step decay</strong>: e.g. decay learning rate by half every few epochs.</li><li><strong>exponential decay</strong>: $\alpha = \alpha_0 e^{-kt}$</li><li><strong>1/t decay</strong>: $\alpha = \alpha_0 / (1+kt)$</li></ul><h3 id="Second-order-optimization-methods"><a href="#Second-order-optimization-methods" class="headerlink" title="Second order optimization methods"></a>Second order optimization methods</h3><p>主要是一些基于牛顿法的二阶最优化方法, 包括 L-BGFS 之类的. 其优点是根本就没有 learning rate 这个超参数, 而缺点则是 Hessian 矩阵实在是太大了, 非常耗费时间与空间, 因此在 DL 和 CNN 中基本不使用.</p><h2 id="Evaluation-Model-Ensembles"><a href="#Evaluation-Model-Ensembles" class="headerlink" title="Evaluation: Model Ensembles"></a>Evaluation: Model Ensembles</h2><ul><li><p>训练多个独立的模型, 然后在测试的时候对其结果进行平均, 一般能得到 2% 的额外性能提升;</p></li><li><p>平均单个模型的多个记录点 (check point) 上的参数, 也能获得一些提升</p></li><li><p>训练的时候对参数进行平滑操作, 并用于测试集 (keep track of (and use at test time) a running average</p><p>parameter vector)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">While <span class="keyword">True</span>:</div><div class="line">    data_batch = dataset.sample_data_batch()</div><div class="line">    loss = network.forward(data_batch)</div><div class="line">    dx = network.backward()</div><div class="line">    x += - learning_rate * dx</div><div class="line">    x_test = <span class="number">0.995</span> * x_test + <span class="number">0.005</span> * x <span class="comment"># use for test set</span></div></pre></td></tr></table></figure></li></ul><h2 id="Regularization-dropout"><a href="#Regularization-dropout" class="headerlink" title="Regularization (dropout)"></a>Regularization (dropout)</h2><p>Dropout 算是很常用的一种方法了, 主要就是在前向传播的时候随机设置某些神经元为零 (“randomly set some neurons to zero in the forward pass”).</p><p><img src="/images/dropout.png" alt="dropout"></p><p>其主要想法是让网络具有一定的冗余能力 (Forces the network to have a<br>redundant representation), 或者说是训练出了一个大的集成网络 (Dropout is training a large ensemble of models (that share parameters), each binary mask is one model, gets trained on only ~one datapoint.)</p><p><img src="/images/dropout_a_good_idea.png" alt="dropout_a_good_idea"></p><p>具体实现如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">p = <span class="number">0.5</span> <span class="comment"># probability of keeping a unit active. higher = less dropout</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span> </div><div class="line">    <span class="string">""" X contains the datat """</span></div><div class="line">    </div><div class="line">    <span class="comment"># forward pass for example 3-layer neural network</span></div><div class="line">    H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) </div><div class="line">    U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># First dropout mask. Notice /p! </span></div><div class="line">    H1 *= U1 <span class="comment"># drop! </span></div><div class="line">    H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2) </div><div class="line">    U2 = (np.random.rand(*H2.shape) &lt; p) / p <span class="comment"># Second dropout mask. Notice /p!  </span></div><div class="line">    H2 *= U2 <span class="comment"># drop! </span></div><div class="line">    out = np.dot(W3, H2) + b3 </div><div class="line">    </div><div class="line">    <span class="comment"># backward pass: compute geadients ... (not shown) </span></div><div class="line">    <span class="comment"># parameter update... (not shown) </span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="comment"># ensembled forward pass </span></div><div class="line">    H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) <span class="comment"># no scaling necessary </span></div><div class="line">    H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2) </div><div class="line">    out = np.dot(W3, H2) + b3</div></pre></td></tr></table></figure><h2 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h2><p>主要就是通过数值法计算梯度, 然后和通过后向传播得到的解析梯度比较, 看看误差大不大, 防止手贱算错梯度导致后面算法全乱了.</p><ol><li>用中心化公式$\frac{df(x)}{dx} = \frac{f(x+h - f(x-h)}{2h}$计算数值梯度, $h$取 $1e-5$ 左右.</li><li>使用相对误差$\frac{|f^{‘}_a - f^{‘}_n|}{max(|f^{‘}_a|, |f^{‘}_n|)}$</li></ol><p>同时还有些注意事项, 参见 <a href="http://cs231n.github.io/neural-networks-3/#gradcheck" target="_blank" rel="external">Gradient Checks</a>.</p></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Deep-Learning/" rel="tag">#Deep Learning</a> <a href="/tags/CS231n/" rel="tag">#CS231n</a> <a href="/tags/Neural-Network/" rel="tag">#Neural Network</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2016/10/08/B-Human-Code-Brief-Analysis-2/" rel="next" title="B-Human Code 浅析 - ScanGridProvider"><i class="fa fa-chevron-left"></i> B-Human Code 浅析 - ScanGridProvider</a></div><div class="post-nav-prev post-nav-item"><a href="/2016/10/19/Notes-for-CS231n-CNN/" rel="prev" title="Notes for CS231n Convolutional Neural Network">Notes for CS231n Convolutional Neural Network <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="//schema.org/Person"><img class="site-author-image" itemprop="image" src="/uploads/avatar.png" alt="Yusu Pan"><p class="site-author-name" itemprop="name">Yusu Pan</p><p class="site-description motion-element" itemprop="description">We've been gaining one good thing through losing another.</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">21</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories"><span class="site-state-item-count">3</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">22</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/corenel" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="https://twitter.com/corenel" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i> Twitter </a></span><span class="links-of-author-item"><a href="http://www.zhihu.com/people/pan-yu-su" target="_blank" title="Zhihu"><i class="fa fa-fw fa-zhihu"></i> Zhihu</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-5"><span class="nav-number">1.</span> <span class="nav-text">Lecture 5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Activation-Functions"><span class="nav-number">1.1.</span> <span class="nav-text">Activation Functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Preprocessing"><span class="nav-number">1.2.</span> <span class="nav-text">Data Preprocessing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Weight-Initialization"><span class="nav-number">1.3.</span> <span class="nav-text">Weight Initialization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Babysitting-the-Learning-Process"><span class="nav-number">1.4.</span> <span class="nav-text">Babysitting the Learning Process</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Double-check-that-the-loss-is-reasonable"><span class="nav-number">1.4.1.</span> <span class="nav-text">Double check that the loss is reasonable</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-sanity-check-tips"><span class="nav-number">1.4.2.</span> <span class="nav-text">Other sanity check tips</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hyperparameter-Optimization"><span class="nav-number">1.5.</span> <span class="nav-text">Hyperparameter Optimization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-6"><span class="nav-number">2.</span> <span class="nav-text">Lecture 6</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Parameter-Updates"><span class="nav-number">2.1.</span> <span class="nav-text">Parameter Updates</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-rate-decay"><span class="nav-number">2.1.1.</span> <span class="nav-text">Learning rate decay</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Second-order-optimization-methods"><span class="nav-number">2.1.2.</span> <span class="nav-text">Second order optimization methods</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluation-Model-Ensembles"><span class="nav-number">2.2.</span> <span class="nav-text">Evaluation: Model Ensembles</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Regularization-dropout"><span class="nav-number">2.3.</span> <span class="nav-text">Regularization (dropout)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Checking"><span class="nav-number">2.4.</span> <span class="nav-text">Gradient Checking</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2015 - <span itemprop="copyrightYear">2016</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Yusu Pan</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }</script><script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.0.2"></script><script type="text/javascript" src="/js/src/motion.js?v=5.0.2"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.2"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.0.2"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.2"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>